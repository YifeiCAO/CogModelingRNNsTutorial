{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7x3Ff2xWTdb"
      },
      "source": [
        "# Import and preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mYuMvSuvyGuR"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "import gdown\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CPp79Z477SU8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotnine as gg\n",
        "gg.theme_set(gg.theme_classic)  # for nicer-looking plots\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "import optax\n",
        "import scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWQY_uJK7Uyx",
        "outputId": "983c0b42-ee55-48ed-e507-ccba22a8289f"
      },
      "outputs": [],
      "source": [
        "# !pip install -U dm-haiku\n",
        "import haiku as hk\n",
        "rng_seq = hk.PRNGSequence(np.random.randint(2**32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX2twY1NNT1r",
        "outputId": "c73fd2f4-31d7-470b-caf5-2d9dc712511d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'CogModelingRNNsTutorial'...\n",
            "remote: Enumerating objects: 1237, done.\u001b[K\n",
            "remote: Counting objects: 100% (313/313), done.\u001b[K\n",
            "remote: Compressing objects: 100% (128/128), done.\u001b[K\n",
            "remote: Total 1237 (delta 252), reused 229 (delta 185), pack-reused 924 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1237/1237), 6.67 MiB | 10.31 MiB/s, done.\n",
            "Resolving deltas: 100% (786/786), done.\n",
            "Obtaining file:///content/CogModelingRNNsTutorial/CogModelingRNNsTutorial\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: dm-haiku in /usr/local/lib/python3.11/dist-packages (from CogModelingRNNsTutorial==0.0.0) (0.0.14)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from CogModelingRNNsTutorial==0.0.0) (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from CogModelingRNNsTutorial==0.0.0) (3.10.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from CogModelingRNNsTutorial==0.0.0) (1.16.0)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from dm-haiku->CogModelingRNNsTutorial==0.0.0) (1.4.0)\n",
            "Requirement already satisfied: jmp>=0.0.2 in /usr/local/lib/python3.11/dist-packages (from dm-haiku->CogModelingRNNsTutorial==0.0.0) (0.0.4)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from dm-haiku->CogModelingRNNsTutorial==0.0.0) (0.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->CogModelingRNNsTutorial==0.0.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->CogModelingRNNsTutorial==0.0.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->CogModelingRNNsTutorial==0.0.0) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->CogModelingRNNsTutorial==0.0.0) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->CogModelingRNNsTutorial==0.0.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->CogModelingRNNsTutorial==0.0.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->CogModelingRNNsTutorial==0.0.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->CogModelingRNNsTutorial==0.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->CogModelingRNNsTutorial==0.0.0) (1.17.0)\n",
            "Installing collected packages: CogModelingRNNsTutorial\n",
            "  Running setup.py develop for CogModelingRNNsTutorial\n",
            "Successfully installed CogModelingRNNsTutorial-0.0.0\n"
          ]
        }
      ],
      "source": [
        "#@title Install required packages.\n",
        "try:\n",
        "    from google.colab import files\n",
        "    _ON_COLAB = True\n",
        "except:\n",
        "    _ON_COLAB = False\n",
        "\n",
        "if _ON_COLAB:\n",
        "  !rm -rf CogModelingRNNsTutorial\n",
        "  !git clone https://github.com/YifeiCAO/CogModelingRNNsTutorial\n",
        "  !pip install -e CogModelingRNNsTutorial/CogModelingRNNsTutorial\n",
        "  !cp CogModelingRNNsTutorial/CogModelingRNNsTutorial/*py CogModelingRNNsTutorial\n",
        "else:\n",
        "  !pip install CogModelingRNNsTutorial/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2jivtclNVjS"
      },
      "outputs": [],
      "source": [
        "#@title Imports + defaults settings.\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "# for reload\n",
        "# %reload_ext autoreload\n",
        "\n",
        "# import haiku as hk\n",
        "# import jax\n",
        "# import jax.numpy as jnp\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optax\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# try:\n",
        "#     from google.colab import files\n",
        "#     _ON_COLAB = True\n",
        "# except:\n",
        "#     _ON_COLAB = False\n",
        "\n",
        "from CogModelingRNNsTutorial import bandits\n",
        "from CogModelingRNNsTutorial import disrnn\n",
        "from CogModelingRNNsTutorial import hybrnn\n",
        "from CogModelingRNNsTutorial import hybconrnn\n",
        "from CogModelingRNNsTutorial import hybrnn_direct_con\n",
        "from CogModelingRNNsTutorial import plotting\n",
        "from CogModelingRNNsTutorial import rat_data\n",
        "from CogModelingRNNsTutorial import rnn_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyjWR7YmyPQB",
        "outputId": "0101e98d-62cc-42d5-a6d9-80fd8c89a044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File downloaded and read successfully!\n",
            "xs.shape = (60, 206, 2)\n",
            "ys.shape = (60, 206, 1)\n"
          ]
        }
      ],
      "source": [
        "osf_url = 'https://osf.io/download/xe6yu/'\n",
        "response = requests.get(osf_url)\n",
        "\n",
        "# Check the request\n",
        "if response.status_code == 200:\n",
        "    # Read as pandas dataframe\n",
        "    qasim_data = pd.read_csv(StringIO(response.text))\n",
        "    print('File downloaded and read successfully!')\n",
        "else:\n",
        "    print('Failed to download file. Status code:', response.status_code)\n",
        "\n",
        "qasim_data.head()\n",
        "\n",
        "# # read data\n",
        "selected_columns = ['participant', 'trials_gamble', 'gamble', 'prob', 'reward']\n",
        "\n",
        "qasim = qasim_data[selected_columns]\n",
        "qasim_filtered = qasim[qasim['trials_gamble'].notna()]\n",
        "qasim_sorted = qasim_filtered.groupby('participant', group_keys=False).apply(lambda x: x.sort_values('trials_gamble'))\n",
        "qasim_sorted = qasim_sorted.reset_index(drop=True)\n",
        "qasim_sorted['participant'] = qasim_sorted.groupby(['participant']).ngroup() + 1\n",
        "qasim_sorted['action'] = qasim_sorted['gamble']\n",
        "qasim_sorted\n",
        "\n",
        "# —— 2) 把缺失的 action 填成 -1 —— #\n",
        "qasim_sorted['action'] = qasim_sorted['action'].fillna(-1).astype(int)\n",
        "\n",
        "# —— 3) 如果需要，把 reward 映射到 0/1 —— #\n",
        "# （如果已经是 0/1 可跳过；否则取消下面注释并调整映射字典）\n",
        "# qasim_sorted['reward'] = (\n",
        "#     qasim_sorted['reward']\n",
        "#     .map({-1: 0, 1: 1})\n",
        "#     .fillna(-1)\n",
        "#     .astype(int)\n",
        "# )\n",
        "\n",
        "# —— 4) 排序，确保 trial 顺序 —— #\n",
        "qasim_sorted = qasim_sorted.sort_values(\n",
        "    ['participant', 'trials_gamble']\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# —— 5) 生成下一步动作 action_n —— #\n",
        "qasim_sorted['action_n'] = (\n",
        "    qasim_sorted\n",
        "    .groupby('participant')['action']\n",
        "    .shift(-1)\n",
        ")\n",
        "# 每个 participant 最后一 trial 的 action_n 设为 -1\n",
        "last_idxs = qasim_sorted.groupby('participant').tail(1).index\n",
        "qasim_sorted.loc[last_idxs, 'action_n'] = -1\n",
        "\n",
        "# —— 6) 按 participant 构造 xs_list, ys_list —— #\n",
        "xs_list, ys_list = [], []\n",
        "for pid, grp in qasim_sorted.groupby('participant'):\n",
        "    grp = grp.sort_values('trials_gamble')\n",
        "    x = grp[['prob', 'reward']].to_numpy().astype(float)    # 输入特征\n",
        "    y = grp[['action_n']].to_numpy().astype(int)             # 下一步动作\n",
        "    xs_list.append(x)\n",
        "    ys_list.append(y)\n",
        "\n",
        "# —— 7) 堆成三维数组 —— #\n",
        "xs_qa = np.stack(xs_list, axis=1)  # (n_sessions, n_trials, 2)\n",
        "ys_qa = np.stack(ys_list, axis=1)  # (n_sessions, n_trials, 1)\n",
        "\n",
        "print(\"xs.shape =\", xs_qa.shape)\n",
        "print(\"ys.shape =\", ys_qa.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNCQ2haXljHL"
      },
      "source": [
        "### Sidarus dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zetxo6c_lgFd",
        "outputId": "f8f0f5e1-0f46-452d-ce7e-87a314963918"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1TSV6CdyClKz831qD2ln4z3WjLLcOgG1n\n",
            "To: /content/downloaded_file.csv\n",
            "100%|██████████| 653k/653k [00:00<00:00, 8.87MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "xs.shape = (800, 40, 2)\n",
            "ys.shape = (800, 40, 1)\n"
          ]
        }
      ],
      "source": [
        "# 修改后的 file_id\n",
        "file_id = '1TSV6CdyClKz831qD2ln4z3WjLLcOgG1n'\n",
        "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "# 下载并保存为 'downloaded_file.csv'\n",
        "output_file = 'downloaded_file.csv'\n",
        "gdown.download(download_url, output_file, quiet=False)\n",
        "\n",
        "# 读取 CSV 数据\n",
        "sida_data = pd.read_csv(output_file)\n",
        "sida_data\n",
        "\n",
        "# 1) action 从 {1,2} → {0,1}，并把所有缺失值填成 -1\n",
        "sida_data['action'] = (\n",
        "    sida_data['action']\n",
        "    .map({1: 0, 2: 1})    # 映射\n",
        "    .fillna(-1)           # 缺失值设为 -1\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "# 2) outcome 从 {-1,1} → {0,1}，缺失也填成 -1\n",
        "sida_data['reward'] = (\n",
        "    sida_data['outcome']\n",
        "    .map({-1: 0, 1: 1})\n",
        "    .fillna(-1)\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "# 3) 给每个被试×session 分配一个连续的 session_id\n",
        "sida_data['session_id'] = (\n",
        "    sida_data\n",
        "    .groupby(['subj', 'session'], sort=False)\n",
        "    .ngroup()\n",
        "    + 1\n",
        ")\n",
        "\n",
        "# 4) 排序，保证 trial 顺序\n",
        "sida_data = sida_data.sort_values(\n",
        "    ['session_id', 'epN', 'epTrialN']\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# 5) 生成下一步动作 action_n；每个 session 末尾设为 -1\n",
        "sida_data['action_n'] = (\n",
        "    sida_data\n",
        "    .groupby('session_id')['action']\n",
        "    .shift(-1)\n",
        ")\n",
        "last_idx = sida_data.groupby('session_id').tail(1).index\n",
        "sida_data.loc[last_idx, 'action_n'] = -1\n",
        "\n",
        "# 6) 按 session_id 抽取序列，并堆成 xs, ys\n",
        "xs_list, ys_list = [], []\n",
        "for sid, grp in sida_data.groupby('session_id'):\n",
        "    grp = grp.sort_values(['epN', 'epTrialN'])\n",
        "    x = grp[['hiRewAct', 'reward']].to_numpy().astype(float)  # 特征\n",
        "    y = grp[['action_n']].to_numpy().astype(int)              # Label\n",
        "    xs_list.append(x)\n",
        "    ys_list.append(y)\n",
        "\n",
        "# 最终结果：xs.shape == (n_sessions, n_trials, 2)，ys.shape == (n_sessions, n_trials, 1)\n",
        "xs_sida = np.stack(xs_list, axis=1)\n",
        "ys_sida = np.stack(ys_list, axis=1)\n",
        "\n",
        "print(\"xs.shape =\", xs_sida.shape)\n",
        "print(\"ys.shape =\", ys_sida.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "147k59EcluMe"
      },
      "source": [
        "### Schaaf dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRUXDOuEltlO",
        "outputId": "4d607a65-5003-4439-f314-daa193744c6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rJFmDhCE3fdXtSHSSvtre_7V49gGsg7P\n",
            "To: /content/downloaded_file.csv\n",
            "100%|██████████| 374k/374k [00:00<00:00, 6.17MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "xs.shape = (249, 94, 2)\n",
            "ys.shape = (249, 94, 1)\n"
          ]
        }
      ],
      "source": [
        "# 修改后的 file_id\n",
        "file_id = '1rJFmDhCE3fdXtSHSSvtre_7V49gGsg7P'\n",
        "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "# 下载并保存为 'downloaded_file.csv'\n",
        "output_file = 'downloaded_file.csv'\n",
        "gdown.download(download_url, output_file, quiet=False)\n",
        "\n",
        "# 读取 CSV 数据\n",
        "schaaf_data = pd.read_csv(output_file)\n",
        "schaaf_data\n",
        "\n",
        "df1 = schaaf_data[['pp', 'trial1', 'response1', 'outcome1']].copy()\n",
        "df1.columns = ['pp', 'trial_in_session', 'response', 'reward']\n",
        "df1['session'] = 1\n",
        "\n",
        "df2 = schaaf_data[['pp', 'trial2', 'response2', 'outcome2']].copy()\n",
        "df2.columns = ['pp', 'trial_in_session', 'response', 'reward']\n",
        "df2['session'] = 2\n",
        "\n",
        "# —— 上面拆分、concat、sort 的部分保持不变 —— #\n",
        "\n",
        "# 合并成长表\n",
        "df_long = pd.concat([df1, df2], ignore_index=True)\n",
        "df_long = df_long.sort_values(['pp', 'session', 'trial_in_session']).reset_index(drop=True)\n",
        "\n",
        "# —— 映射 outcome，再填 missing response —— #\n",
        "# 把原来的 response1/response2 改名后是 `response`\n",
        "# 把 outcome1/outcome2 改名后是 `reward`\n",
        "# 把 outcome 映射成 1/0，然后把原本缺失的 reward 补成 -1\n",
        "df_long['reward'] = (\n",
        "    df_long['reward']\n",
        "    .map({1: 1, -1: 0})\n",
        "    .fillna(-1)\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "# 把缺失的 response 一样补成 -1\n",
        "df_long['response'] = (\n",
        "    df_long['response']\n",
        "    .fillna(-1)\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "\n",
        "# 接下来再做 session_id、shift action_n、以及 stack xs/ys 的流程……\n",
        "df_long['session_id'] = (df_long['pp'] - 1) * 2 + df_long['session']\n",
        "df_long['action_n'] = df_long.groupby('session_id')['response'].shift(-1)\n",
        "last_idx = df_long.groupby('session_id').tail(1).index\n",
        "df_long.loc[last_idx, 'action_n'] = -1\n",
        "\n",
        "# 生成 xs, ys\n",
        "session_ids = df_long['session_id'].unique()\n",
        "xs_list, ys_list = [], []\n",
        "for sid in session_ids:\n",
        "    sd = df_long[df_long['session_id'] == sid].sort_values('trial_in_session')\n",
        "    x = sd[['response', 'reward']].to_numpy().astype(float)\n",
        "    y = sd[['action_n']].to_numpy().astype(int)\n",
        "    xs_list.append(x)\n",
        "    ys_list.append(y)\n",
        "\n",
        "xs = np.stack(xs_list, axis=0)   # (n_sessions, n_trials, 2)\n",
        "ys = np.stack(ys_list, axis=0)   # (n_sessions, n_trials, 1)\n",
        "\n",
        "\n",
        "# 或者第 0 维是 trials，第 1 维是 sessions：\n",
        "xs_sch = np.stack(xs_list, axis=1)  # (n_trials, n_sessions, 2)\n",
        "ys_sch = np.stack(ys_list, axis=1)  # (n_trials, n_sessions, 1)\n",
        "\n",
        "print(\"xs.shape =\", xs_sch.shape)\n",
        "print(\"ys.shape =\", ys_sch.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8KQplq3tHC7"
      },
      "source": [
        "### Maria Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDBAI_AwD1PF",
        "outputId": "a54a9388-a125-4948-b27e-e97bc0ba8ab5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1N_zAy-qrbfjvF8Kbb504IH2JNhR5KI-P\n",
            "To: /content/downloaded_file.csv\n",
            "100%|██████████| 2.57M/2.57M [00:00<00:00, 20.2MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "xs_ma.shape = (120, 305, 2)\n",
            "ys_ma.shape = (120, 305, 1)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gdown\n",
        "\n",
        "# —— 1) 下载并读入原始数据 —— #\n",
        "file_id = '1N_zAy-qrbfjvF8Kbb504IH2JNhR5KI-P'\n",
        "url     = f'https://drive.google.com/uc?id={file_id}'\n",
        "gdown.download(url, 'downloaded_file.csv', quiet=False)\n",
        "eck_data = pd.read_csv('downloaded_file.csv')\n",
        "\n",
        "# —— 2) 筛选＆重命名列 —— #\n",
        "sel = ['sID','TrialID','selected_box','reward']\n",
        "eck_sorted = eck_data[sel].copy()\n",
        "eck_sorted['participant'] = eck_sorted.groupby('sID').ngroup()+1\n",
        "eck_sorted['action']      = eck_sorted['selected_box']\n",
        "\n",
        "# 只保留至少做够 120 试次的被试\n",
        "max_trial = eck_sorted.groupby('participant')['TrialID'].transform('max')\n",
        "eck_sorted = eck_sorted[max_trial>=120]\n",
        "\n",
        "# 只用前 120 试次\n",
        "eck_sorted = eck_sorted[eck_sorted['TrialID']<=120].reset_index(drop=True)\n",
        "\n",
        "# —— 3) 生成下一步动作 action_n —— #\n",
        "def generate_action_n(group):\n",
        "    group = group.sort_values('TrialID')\n",
        "    group['action_n'] = group['action'].shift(-1).fillna(-1).astype(int)\n",
        "    return group\n",
        "\n",
        "eck_sorted = (\n",
        "    eck_sorted\n",
        "    .groupby('participant', group_keys=False)\n",
        "    .apply(generate_action_n)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# —— 4) 按 participant 构造 xs_list/ys_list —— #\n",
        "xs_list, ys_list = [], []\n",
        "for pid, grp in eck_sorted.groupby('participant'):\n",
        "    grp = grp.sort_values('TrialID').iloc[:120]\n",
        "    x = grp[['action','reward']].to_numpy().astype(float)  # (120,2)\n",
        "    y = grp[['action_n']].to_numpy().astype(int)           # (120,1)\n",
        "    xs_list.append(x)\n",
        "    ys_list.append(y)\n",
        "\n",
        "# —— 5) stack 成 (n_sessions, n_trials, feat_dim) —— #\n",
        "xs_ma = np.stack(xs_list, axis=1)  # (305,120,2)\n",
        "ys_ma = np.stack(ys_list, axis=1)  # (305,120,1)\n",
        "\n",
        "print(\"xs_ma.shape =\", xs_ma.shape)\n",
        "print(\"ys_ma.shape =\", ys_ma.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbm2178XrKVd"
      },
      "source": [
        "### Generate a big human dataset with all experiments, session length is 130 trial per session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ9SycOgtbsK",
        "outputId": "9650c98e-c381-4581-eda2-0f4849543525"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total segments: 979\n",
            "First xs segment shape: (130, 2)\n",
            "First ys segment shape: (130, 1)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def segment_and_pad(x: np.ndarray,\n",
        "                    y: np.ndarray,\n",
        "                    seg_len: int = 130,\n",
        "                    pad_x: float = 0.,\n",
        "                    pad_y: int = -1):\n",
        "    T, D = x.shape\n",
        "    n_segs = int(np.ceil(T / seg_len))\n",
        "    x_segs, y_segs = [], []\n",
        "    for i in range(n_segs):\n",
        "        start = i * seg_len\n",
        "        end = start + seg_len\n",
        "        x_part = x[start : min(end, T)]\n",
        "        y_part = y[start : min(end, T)]\n",
        "        pad = end - min(end, T)\n",
        "        if pad > 0:\n",
        "            x_part = np.pad(x_part,\n",
        "                            pad_width=((0, pad), (0, 0)),\n",
        "                            constant_values=pad_x)\n",
        "            y_part = np.pad(y_part,\n",
        "                            pad_width=((0, pad), (0, 0)),\n",
        "                            constant_values=pad_y)\n",
        "        x_segs.append(x_part)\n",
        "        y_segs.append(y_part)\n",
        "    return x_segs, y_segs\n",
        "\n",
        "# —— 假设你已经有这几组 (xs, ys) —— #\n",
        "# xs_qa   (60,  206, 2), ys_qa  (60,206, 1)\n",
        "# xs_sida (800, 40,  2), ys_sida(800,40, 1)\n",
        "# xs_sch  (249, 44,  2), ys_sch (249,44, 1)\n",
        "# xs_ma   (120,305, 2), ys_ma  (120,305,1)\n",
        "\n",
        "all_xs, all_ys = [], []\n",
        "\n",
        "for xs, ys in [(xs_qa, ys_qa),\n",
        "               (xs_sida, ys_sida),\n",
        "               (xs_sch, ys_sch),\n",
        "               (xs_ma, ys_ma)]:\n",
        "\n",
        "    # 如果是 (n_trials, n_sessions, feat) 维度，就直接：\n",
        "    # T, N, D = xs.shape\n",
        "    # 否则若是 (n_sessions, n_trials, feat)，先转：\n",
        "    # xs = xs.transpose(1,0,2)\n",
        "    # ys = ys.transpose(1,0,2)\n",
        "\n",
        "    T, N, D = xs.shape\n",
        "    for sess in range(N):\n",
        "        x_seq = xs[:, sess, :]  # (T, D)\n",
        "        y_seq = ys[:, sess, :]  # (T, 1)\n",
        "        x_segs, y_segs = segment_and_pad(\n",
        "            x_seq, y_seq,\n",
        "            seg_len=130,\n",
        "            pad_x=0., pad_y=-1\n",
        "        )\n",
        "        all_xs.extend(x_segs)\n",
        "        all_ys.extend(y_segs)\n",
        "\n",
        "# —— 修改在这里：不要 np.stack，直接输出列表 —— #\n",
        "# all_xs 是一个 Python list，长度 = 总片段数，每个元素 shape=(130, D)\n",
        "# all_ys 是一个 Python list，长度 = 总片段数，每个元素 shape=(130, 1)\n",
        "\n",
        "print(f\"Total segments: {len(all_xs)}\")\n",
        "print(f\"First xs segment shape: {all_xs[0].shape}\")\n",
        "print(f\"First ys segment shape: {all_ys[0].shape}\")\n",
        "\n",
        "# 如果你需要把它们返回成变量：\n",
        "xs_segment_list = all_xs\n",
        "ys_segment_list = all_ys\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Y-JHKuQv48Ur"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def format_into_datasets_multi_source(\n",
        "    xs_list: list[np.ndarray],\n",
        "    ys_list: list[np.ndarray],\n",
        "    dataset_constructor,\n",
        "    n_train_sessions: int,\n",
        "    n_test_sessions: int,\n",
        "    n_validate_sessions: int,\n",
        "    batch_size: int = None,\n",
        "    random_seed: int = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    按照 QA、SIDA、SCH、MA 这 4 个来源的数据源比例，\n",
        "    在它们各自内部抽取 train/test/val session，\n",
        "    最后拼成全局的 DatasetRNN。\n",
        "\n",
        "    xs_list, ys_list:\n",
        "      长度 4 的 list，每个元素形状是 (timesteps, n_sessions_i, feat)\n",
        "    n_*_sessions:\n",
        "      全局希望 train/test/val 一共要多少 session\n",
        "    \"\"\"\n",
        "    if random_seed is not None:\n",
        "        rng = np.random.RandomState(random_seed)\n",
        "    else:\n",
        "        rng = np.random\n",
        "\n",
        "    # 1) 计算每个来源各有多少 session\n",
        "    sess_counts = np.array([xs.shape[1] for xs in xs_list])  # e.g. [206, 40, 44, 305]\n",
        "    total_sessions = sess_counts.sum()\n",
        "\n",
        "    # 2) 按比例分配到每个来源的 train/test/val 数目\n",
        "    def proportional_alloc(total, counts):\n",
        "        floats = counts / counts.sum() * total\n",
        "        floors = np.floor(floats).astype(int)\n",
        "        rem = total - floors.sum()\n",
        "        # 剩余的按余数最大的那些来源补齐\n",
        "        remainders = floats - floors\n",
        "        for idx in np.argsort(remainders)[-rem:]:\n",
        "            floors[idx] += 1\n",
        "        return floors\n",
        "\n",
        "    n_train_per = proportional_alloc(n_train_sessions,    sess_counts)\n",
        "    n_test_per  = proportional_alloc(n_test_sessions,     sess_counts)\n",
        "    n_val_per   = proportional_alloc(n_validate_sessions, sess_counts)\n",
        "\n",
        "    # 3) 在每个来源内部随机打乱并切分\n",
        "    train_idx_list, test_idx_list, val_idx_list = [], [], []\n",
        "    for cnt, n_tr, n_te, n_va in zip(sess_counts,\n",
        "                                     n_train_per,\n",
        "                                     n_test_per,\n",
        "                                     n_val_per):\n",
        "        all_idx = np.arange(cnt)\n",
        "        rng.shuffle(all_idx)\n",
        "        train_idx_list.append(all_idx[:n_tr])\n",
        "        test_idx_list.append( all_idx[n_tr:n_tr+n_te] )\n",
        "        val_idx_list.append(  all_idx[n_tr+n_te:n_tr+n_te+n_va] )\n",
        "\n",
        "    # 4) 汇总抽到的 sessions：concat 出全局 xs/ys\n",
        "    def gather(xs_list, ys_list, idx_lists):\n",
        "        parts_x, parts_y = [], []\n",
        "        for xs, ys, idx in zip(xs_list, ys_list, idx_lists):\n",
        "            # xs: (timesteps, n_sessions_i, feat)\n",
        "            parts_x.append(xs[:, idx, :])\n",
        "            parts_y.append(ys[:, idx, :])\n",
        "        return np.concatenate(parts_x, axis=1), np.concatenate(parts_y, axis=1)\n",
        "\n",
        "    xs_train, ys_train = gather(xs_list, ys_list, train_idx_list)\n",
        "    xs_test,  ys_test  = gather(xs_list, ys_list, test_idx_list)\n",
        "    xs_val,   ys_val   = gather(xs_list, ys_list, val_idx_list)\n",
        "\n",
        "    # 5) 构造 DatasetRNN\n",
        "    ds_train = dataset_constructor(xs_train, ys_train, batch_size=batch_size)\n",
        "    ds_test  = dataset_constructor(xs_test,  ys_test,  batch_size=batch_size)\n",
        "    ds_val   = dataset_constructor(xs_val,   ys_val,   batch_size=batch_size)\n",
        "\n",
        "    return ds_train, ds_test, ds_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VnupWYiP5vW6"
      },
      "outputs": [],
      "source": [
        "# 假设你已经有原始的：\n",
        "#   xs_qa   (T_qa,   N_qa,   D),   ys_qa   (T_qa,   N_qa,   1)\n",
        "#   xs_sida (T_sida, N_sida, D),   ys_sida (T_sida, N_sida, 1)\n",
        "#   xs_sch  (T_sch,  N_sch,  D),   ys_sch  (T_sch,  N_sch,  1)\n",
        "#   xs_ma   (T_ma,   N_ma,   D),   ys_ma   (T_ma,   N_ma,   1)\n",
        "\n",
        "def make_segmented_array(xs, ys, seg_len=130, pad_x=0., pad_y=-1):\n",
        "    all_xs, all_ys = [], []\n",
        "    T, N, D = xs.shape\n",
        "    for sess in range(N):\n",
        "        x_seq = xs[:, sess, :]    # (T, D)\n",
        "        y_seq = ys[:, sess, :]    # (T, 1)\n",
        "        x_segs, y_segs = segment_and_pad(x_seq, y_seq, seg_len, pad_x, pad_y)\n",
        "        all_xs.extend(x_segs)     # list of (130, D)\n",
        "        all_ys.extend(y_segs)     # list of (130, 1)\n",
        "    # 把 list 再拼成一个三维 array (130, n_segments, D)\n",
        "    xs_seg = np.stack(all_xs, axis=1)\n",
        "    ys_seg = np.stack(all_ys, axis=1)\n",
        "    return xs_seg, ys_seg\n",
        "\n",
        "# 针对四个源分别做一次\n",
        "xs_qa_seg,   ys_qa_seg   = make_segmented_array(xs_qa,   ys_qa)\n",
        "xs_sida_seg,ys_sida_seg = make_segmented_array(xs_sida, ys_sida)\n",
        "xs_sch_seg, ys_sch_seg  = make_segmented_array(xs_sch,  ys_sch)\n",
        "xs_ma_seg,  ys_ma_seg   = make_segmented_array(xs_ma,   ys_ma)\n",
        "\n",
        "# 然后再把它们送入多源拼分函数\n",
        "dataset_train, dataset_test, dataset_validate = format_into_datasets_multi_source(\n",
        "    xs_list   = [xs_qa_seg,   xs_sida_seg,   xs_sch_seg,   xs_ma_seg],\n",
        "    ys_list   = [ys_qa_seg,   ys_sida_seg,   ys_sch_seg,   ys_ma_seg],\n",
        "    dataset_constructor = rnn_utils.DatasetRNN,\n",
        "    n_train_sessions   = 783,\n",
        "    n_test_sessions    = 98,\n",
        "    n_validate_sessions= 98,\n",
        "    batch_size=64,\n",
        "    random_seed=42,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "otkhuOQyx7hB"
      },
      "outputs": [],
      "source": [
        "def compute_log_likelihood(dataset, model_fun, params):\n",
        "    xs, actual_choices = next(dataset)\n",
        "    n_trials_per_session, n_sessions = actual_choices.shape[:2]\n",
        "    model_outputs, model_states = rnn_utils.eval_model(model_fun, params, xs)\n",
        "\n",
        "    # predicted log-probs for the first two actions\n",
        "    predicted_log_choice_probabilities = np.array(\n",
        "        jax.nn.log_softmax(model_outputs[:, :, :2], axis=-1)\n",
        "    )\n",
        "\n",
        "    n_actions = predicted_log_choice_probabilities.shape[2]\n",
        "    log_likelihoods = []\n",
        "\n",
        "    for sess_i in range(n_sessions):\n",
        "        log_likelihood = 0.0\n",
        "        n = 0\n",
        "        for trial_i in range(n_trials_per_session):\n",
        "            actual_choice = int(actual_choices[trial_i, sess_i])\n",
        "            # ignore invalid trials (<0 or ≥n_actions)\n",
        "            if 0 <= actual_choice < n_actions:\n",
        "                log_likelihood += predicted_log_choice_probabilities[\n",
        "                    trial_i, sess_i, actual_choice\n",
        "                ]\n",
        "                n += 1\n",
        "\n",
        "        if n > 0:\n",
        "            normalized_likelihood = np.exp(log_likelihood / n)\n",
        "            log_likelihoods.append(normalized_likelihood)\n",
        "\n",
        "    mean_likelihood = np.mean(log_likelihoods)\n",
        "    std_likelihood  = np.std(log_likelihoods)\n",
        "\n",
        "    print(f'Average Normalized Likelihood: {100 * mean_likelihood:.1f}%')\n",
        "    return mean_likelihood, std_likelihood\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCzihu0NJO6R",
        "outputId": "94f1db8b-0a3f-436a-d233-470f384a6c11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pickle\n",
        "\n",
        "# 挂载 Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8sai1OhWYy3"
      },
      "source": [
        "# Fitting different models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmgO7HywNl5-"
      },
      "source": [
        "# Fit Vanilla RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZQyXrnBlvJaZ",
        "outputId": "527bb074-2a42-4519-bac2-c88a6f866cee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 200 of 200; Loss: 3.0358e+03; Test Loss: 1.9863e+03. (Time: 3.8s)\n",
            "first loss: inf test loss new: 3113.832800292969\n",
            "updating best model ..\n",
            "Step 200 of 100000; Loss: 3.1138e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3901e+02; Test Loss: 1.9848e+03. (Time: 2.3s)\n",
            "first loss: 3113.832800292969 test loss new: 3110.567392578125\n",
            "updating best model ..\n",
            "Step 400 of 100000; Loss: 3.1106e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2713e+03; Test Loss: 1.9834e+03. (Time: 2.3s)\n",
            "first loss: 3110.567392578125 test loss new: 3107.3826098632812\n",
            "updating best model ..\n",
            "Step 600 of 100000; Loss: 3.1074e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9798e+03; Test Loss: 1.9821e+03. (Time: 3.3s)\n",
            "first loss: 3107.3826098632812 test loss new: 3104.287646484375\n",
            "updating best model ..\n",
            "Step 800 of 100000; Loss: 3.1043e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5663e+03; Test Loss: 1.9809e+03. (Time: 2.5s)\n",
            "first loss: 3104.287646484375 test loss new: 3101.2500610351562\n",
            "updating best model ..\n",
            "Step 1000 of 100000; Loss: 3.1013e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9342e+03; Test Loss: 1.9797e+03. (Time: 2.2s)\n",
            "first loss: 3101.2500610351562 test loss new: 3098.290905761719\n",
            "updating best model ..\n",
            "Step 1200 of 100000; Loss: 3.0983e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3419e+03; Test Loss: 1.9787e+03. (Time: 2.3s)\n",
            "first loss: 3098.290905761719 test loss new: 3095.418571777344\n",
            "updating best model ..\n",
            "Step 1400 of 100000; Loss: 3.0954e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9965e+03; Test Loss: 1.9777e+03. (Time: 2.3s)\n",
            "first loss: 3095.418571777344 test loss new: 3092.613391113281\n",
            "updating best model ..\n",
            "Step 1600 of 100000; Loss: 3.0926e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.3678e+03; Test Loss: 1.9768e+03. (Time: 2.9s)\n",
            "first loss: 3092.613391113281 test loss new: 3089.8847509765624\n",
            "updating best model ..\n",
            "Step 1800 of 100000; Loss: 3.0899e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3068e+03; Test Loss: 1.9760e+03. (Time: 2.9s)\n",
            "first loss: 3089.8847509765624 test loss new: 3087.222744140625\n",
            "updating best model ..\n",
            "Step 2000 of 100000; Loss: 3.0872e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6295e+03; Test Loss: 1.9752e+03. (Time: 2.4s)\n",
            "first loss: 3087.222744140625 test loss new: 3084.634431152344\n",
            "updating best model ..\n",
            "Step 2200 of 100000; Loss: 3.0846e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6785e+03; Test Loss: 1.9744e+03. (Time: 2.3s)\n",
            "first loss: 3084.634431152344 test loss new: 3082.121335449219\n",
            "updating best model ..\n",
            "Step 2400 of 100000; Loss: 3.0821e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6305e+03; Test Loss: 1.9737e+03. (Time: 2.3s)\n",
            "first loss: 3082.121335449219 test loss new: 3079.6720068359373\n",
            "updating best model ..\n",
            "Step 2600 of 100000; Loss: 3.0797e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9177e+03; Test Loss: 1.9731e+03. (Time: 2.7s)\n",
            "first loss: 3079.6720068359373 test loss new: 3077.2934521484376\n",
            "updating best model ..\n",
            "Step 2800 of 100000; Loss: 3.0773e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3218e+02; Test Loss: 1.9725e+03. (Time: 3.0s)\n",
            "first loss: 3077.2934521484376 test loss new: 3074.9809301757814\n",
            "updating best model ..\n",
            "Step 3000 of 100000; Loss: 3.0750e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2740e+03; Test Loss: 1.9720e+03. (Time: 2.3s)\n",
            "first loss: 3074.9809301757814 test loss new: 3072.7237646484373\n",
            "updating best model ..\n",
            "Step 3200 of 100000; Loss: 3.0727e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9373e+03; Test Loss: 1.9716e+03. (Time: 2.3s)\n",
            "first loss: 3072.7237646484373 test loss new: 3070.522482910156\n",
            "updating best model ..\n",
            "Step 3400 of 100000; Loss: 3.0705e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5732e+03; Test Loss: 1.9712e+03. (Time: 2.3s)\n",
            "first loss: 3070.522482910156 test loss new: 3068.354436035156\n",
            "updating best model ..\n",
            "Step 3600 of 100000; Loss: 3.0684e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9082e+03; Test Loss: 1.9708e+03. (Time: 2.4s)\n",
            "first loss: 3068.354436035156 test loss new: 3066.239482421875\n",
            "updating best model ..\n",
            "Step 3800 of 100000; Loss: 3.0662e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3497e+03; Test Loss: 1.9705e+03. (Time: 4.9s)\n",
            "first loss: 3066.239482421875 test loss new: 3064.1784790039064\n",
            "updating best model ..\n",
            "Step 4000 of 100000; Loss: 3.0642e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9666e+03; Test Loss: 1.9703e+03. (Time: 2.3s)\n",
            "first loss: 3064.1784790039064 test loss new: 3062.1666479492187\n",
            "updating best model ..\n",
            "Step 4200 of 100000; Loss: 3.0622e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.3313e+03; Test Loss: 1.9701e+03. (Time: 2.3s)\n",
            "first loss: 3062.1666479492187 test loss new: 3060.203054199219\n",
            "updating best model ..\n",
            "Step 4400 of 100000; Loss: 3.0602e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2547e+03; Test Loss: 1.9699e+03. (Time: 2.3s)\n",
            "first loss: 3060.203054199219 test loss new: 3058.287763671875\n",
            "updating best model ..\n",
            "Step 4600 of 100000; Loss: 3.0583e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5723e+03; Test Loss: 1.9698e+03. (Time: 2.5s)\n",
            "first loss: 3058.287763671875 test loss new: 3056.419768066406\n",
            "updating best model ..\n",
            "Step 4800 of 100000; Loss: 3.0564e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5745e+03; Test Loss: 1.9697e+03. (Time: 3.3s)\n",
            "first loss: 3056.419768066406 test loss new: 3054.60966796875\n",
            "updating best model ..\n",
            "Step 5000 of 100000; Loss: 3.0546e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5717e+03; Test Loss: 1.9696e+03. (Time: 2.3s)\n",
            "first loss: 3054.60966796875 test loss new: 3052.8383203125\n",
            "updating best model ..\n",
            "Step 5200 of 100000; Loss: 3.0528e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.8281e+03; Test Loss: 1.9696e+03. (Time: 2.3s)\n",
            "first loss: 3052.8383203125 test loss new: 3051.1217431640625\n",
            "updating best model ..\n",
            "Step 5400 of 100000; Loss: 3.0511e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2997e+02; Test Loss: 1.9695e+03. (Time: 2.2s)\n",
            "first loss: 3051.1217431640625 test loss new: 3049.4471069335937\n",
            "updating best model ..\n",
            "Step 5600 of 100000; Loss: 3.0494e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2792e+03; Test Loss: 1.9695e+03. (Time: 2.3s)\n",
            "first loss: 3049.4471069335937 test loss new: 3047.813837890625\n",
            "updating best model ..\n",
            "Step 5800 of 100000; Loss: 3.0478e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9165e+03; Test Loss: 1.9696e+03. (Time: 3.4s)\n",
            "first loss: 3047.813837890625 test loss new: 3046.2145703125\n",
            "updating best model ..\n",
            "Step 6000 of 100000; Loss: 3.0462e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5807e+03; Test Loss: 1.9698e+03. (Time: 2.3s)\n",
            "first loss: 3046.2145703125 test loss new: 3044.6345849609374\n",
            "updating best model ..\n",
            "Step 6200 of 100000; Loss: 3.0446e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9003e+03; Test Loss: 1.9699e+03. (Time: 2.5s)\n",
            "first loss: 3044.6345849609374 test loss new: 3043.093203125\n",
            "updating best model ..\n",
            "Step 6400 of 100000; Loss: 3.0431e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3584e+03; Test Loss: 1.9701e+03. (Time: 2.3s)\n",
            "first loss: 3043.093203125 test loss new: 3041.584055175781\n",
            "updating best model ..\n",
            "Step 6600 of 100000; Loss: 3.0416e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9552e+03; Test Loss: 1.9702e+03. (Time: 2.3s)\n",
            "first loss: 3041.584055175781 test loss new: 3040.112907714844\n",
            "updating best model ..\n",
            "Step 6800 of 100000; Loss: 3.0401e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.2984e+03; Test Loss: 1.9705e+03. (Time: 3.2s)\n",
            "first loss: 3040.112907714844 test loss new: 3038.670625\n",
            "updating best model ..\n",
            "Step 7000 of 100000; Loss: 3.0387e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2222e+03; Test Loss: 1.9707e+03. (Time: 2.6s)\n",
            "first loss: 3038.670625 test loss new: 3037.2649853515627\n",
            "updating best model ..\n",
            "Step 7200 of 100000; Loss: 3.0373e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5186e+03; Test Loss: 1.9710e+03. (Time: 2.3s)\n",
            "first loss: 3037.2649853515627 test loss new: 3035.8884594726565\n",
            "updating best model ..\n",
            "Step 7400 of 100000; Loss: 3.0359e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4953e+03; Test Loss: 1.9713e+03. (Time: 2.3s)\n",
            "first loss: 3035.8884594726565 test loss new: 3034.557614746094\n",
            "updating best model ..\n",
            "Step 7600 of 100000; Loss: 3.0346e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5169e+03; Test Loss: 1.9716e+03. (Time: 2.3s)\n",
            "first loss: 3034.557614746094 test loss new: 3033.248701171875\n",
            "updating best model ..\n",
            "Step 7800 of 100000; Loss: 3.0332e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7605e+03; Test Loss: 1.9719e+03. (Time: 2.8s)\n",
            "first loss: 3033.248701171875 test loss new: 3031.983955078125\n",
            "updating best model ..\n",
            "Step 8000 of 100000; Loss: 3.0320e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3060e+02; Test Loss: 1.9722e+03. (Time: 2.9s)\n",
            "first loss: 3031.983955078125 test loss new: 3030.7441796875\n",
            "updating best model ..\n",
            "Step 8200 of 100000; Loss: 3.0307e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2846e+03; Test Loss: 1.9725e+03. (Time: 2.3s)\n",
            "first loss: 3030.7441796875 test loss new: 3029.5367431640625\n",
            "updating best model ..\n",
            "Step 8400 of 100000; Loss: 3.0295e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9099e+03; Test Loss: 1.9729e+03. (Time: 2.3s)\n",
            "first loss: 3029.5367431640625 test loss new: 3028.3482690429687\n",
            "updating best model ..\n",
            "Step 8600 of 100000; Loss: 3.0283e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5869e+03; Test Loss: 1.9733e+03. (Time: 2.3s)\n",
            "first loss: 3028.3482690429687 test loss new: 3027.1704052734376\n",
            "updating best model ..\n",
            "Step 8800 of 100000; Loss: 3.0272e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9037e+03; Test Loss: 1.9737e+03. (Time: 2.8s)\n",
            "first loss: 3027.1704052734376 test loss new: 3026.021706542969\n",
            "updating best model ..\n",
            "Step 9000 of 100000; Loss: 3.0260e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3660e+03; Test Loss: 1.9742e+03. (Time: 3.1s)\n",
            "first loss: 3026.021706542969 test loss new: 3024.890439453125\n",
            "updating best model ..\n",
            "Step 9200 of 100000; Loss: 3.0249e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9554e+03; Test Loss: 1.9746e+03. (Time: 2.2s)\n",
            "first loss: 3024.890439453125 test loss new: 3023.7896557617187\n",
            "updating best model ..\n",
            "Step 9400 of 100000; Loss: 3.0238e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.2682e+03; Test Loss: 1.9751e+03. (Time: 2.3s)\n",
            "first loss: 3023.7896557617187 test loss new: 3022.7046826171877\n",
            "updating best model ..\n",
            "Step 9600 of 100000; Loss: 3.0227e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2034e+03; Test Loss: 1.9756e+03. (Time: 2.3s)\n",
            "first loss: 3022.7046826171877 test loss new: 3021.648513183594\n",
            "updating best model ..\n",
            "Step 9800 of 100000; Loss: 3.0216e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.4686e+03; Test Loss: 1.9760e+03. (Time: 2.3s)\n",
            "first loss: 3021.648513183594 test loss new: 3020.608935546875\n",
            "updating best model ..\n",
            "Step 10000 of 100000; Loss: 3.0206e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4357e+03; Test Loss: 1.9765e+03. (Time: 3.5s)\n",
            "first loss: 3020.608935546875 test loss new: 3019.606923828125\n",
            "updating best model ..\n",
            "Step 10200 of 100000; Loss: 3.0196e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.4661e+03; Test Loss: 1.9770e+03. (Time: 2.3s)\n",
            "first loss: 3019.606923828125 test loss new: 3018.6155908203127\n",
            "updating best model ..\n",
            "Step 10400 of 100000; Loss: 3.0186e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7100e+03; Test Loss: 1.9775e+03. (Time: 2.2s)\n",
            "first loss: 3018.6155908203127 test loss new: 3017.6608544921874\n",
            "updating best model ..\n",
            "Step 10600 of 100000; Loss: 3.0177e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3287e+02; Test Loss: 1.9780e+03. (Time: 2.3s)\n",
            "first loss: 3017.6608544921874 test loss new: 3016.7194360351564\n",
            "updating best model ..\n",
            "Step 10800 of 100000; Loss: 3.0167e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2890e+03; Test Loss: 1.9785e+03. (Time: 2.3s)\n",
            "first loss: 3016.7194360351564 test loss new: 3015.8047485351562\n",
            "updating best model ..\n",
            "Step 11000 of 100000; Loss: 3.0158e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9121e+03; Test Loss: 1.9790e+03. (Time: 3.3s)\n",
            "first loss: 3015.8047485351562 test loss new: 3014.898981933594\n",
            "updating best model ..\n",
            "Step 11200 of 100000; Loss: 3.0149e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5913e+03; Test Loss: 1.9796e+03. (Time: 2.5s)\n",
            "first loss: 3014.898981933594 test loss new: 3013.998684082031\n",
            "updating best model ..\n",
            "Step 11400 of 100000; Loss: 3.0140e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9134e+03; Test Loss: 1.9801e+03. (Time: 2.2s)\n",
            "first loss: 3013.998684082031 test loss new: 3013.1216674804687\n",
            "updating best model ..\n",
            "Step 11600 of 100000; Loss: 3.0131e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3720e+03; Test Loss: 1.9807e+03. (Time: 2.3s)\n",
            "first loss: 3013.1216674804687 test loss new: 3012.2525268554687\n",
            "updating best model ..\n",
            "Step 11800 of 100000; Loss: 3.0123e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9625e+03; Test Loss: 1.9813e+03. (Time: 2.5s)\n",
            "first loss: 3012.2525268554687 test loss new: 3011.4087768554687\n",
            "updating best model ..\n",
            "Step 12000 of 100000; Loss: 3.0114e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.2405e+03; Test Loss: 1.9818e+03. (Time: 3.0s)\n",
            "first loss: 3011.4087768554687 test loss new: 3010.5723193359377\n",
            "updating best model ..\n",
            "Step 12200 of 100000; Loss: 3.0106e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1937e+03; Test Loss: 1.9824e+03. (Time: 2.8s)\n",
            "first loss: 3010.5723193359377 test loss new: 3009.7595654296874\n",
            "updating best model ..\n",
            "Step 12400 of 100000; Loss: 3.0098e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.4226e+03; Test Loss: 1.9830e+03. (Time: 2.3s)\n",
            "first loss: 3009.7595654296874 test loss new: 3008.955344238281\n",
            "updating best model ..\n",
            "Step 12600 of 100000; Loss: 3.0090e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3910e+03; Test Loss: 1.9836e+03. (Time: 2.3s)\n",
            "first loss: 3008.955344238281 test loss new: 3008.1827807617187\n",
            "updating best model ..\n",
            "Step 12800 of 100000; Loss: 3.0082e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.4197e+03; Test Loss: 1.9841e+03. (Time: 2.3s)\n",
            "first loss: 3008.1827807617187 test loss new: 3007.4137768554688\n",
            "updating best model ..\n",
            "Step 13000 of 100000; Loss: 3.0074e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6724e+03; Test Loss: 1.9847e+03. (Time: 2.7s)\n",
            "first loss: 3007.4137768554688 test loss new: 3006.676193847656\n",
            "updating best model ..\n",
            "Step 13200 of 100000; Loss: 3.0067e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3594e+02; Test Loss: 1.9853e+03. (Time: 3.0s)\n",
            "first loss: 3006.676193847656 test loss new: 3005.9444946289063\n",
            "updating best model ..\n",
            "Step 13400 of 100000; Loss: 3.0059e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2923e+03; Test Loss: 1.9858e+03. (Time: 2.3s)\n",
            "first loss: 3005.9444946289063 test loss new: 3005.2357666015623\n",
            "updating best model ..\n",
            "Step 13600 of 100000; Loss: 3.0052e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9192e+03; Test Loss: 1.9864e+03. (Time: 2.3s)\n",
            "first loss: 3005.2357666015623 test loss new: 3004.5298461914062\n",
            "updating best model ..\n",
            "Step 13800 of 100000; Loss: 3.0045e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5939e+03; Test Loss: 1.9870e+03. (Time: 2.3s)\n",
            "first loss: 3004.5298461914062 test loss new: 3003.8263110351563\n",
            "updating best model ..\n",
            "Step 14000 of 100000; Loss: 3.0038e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9262e+03; Test Loss: 1.9876e+03. (Time: 2.3s)\n",
            "first loss: 3003.8263110351563 test loss new: 3003.1423266601564\n",
            "updating best model ..\n",
            "Step 14200 of 100000; Loss: 3.0031e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3764e+03; Test Loss: 1.9882e+03. (Time: 4.0s)\n",
            "first loss: 3003.1423266601564 test loss new: 3002.459997558594\n",
            "updating best model ..\n",
            "Step 14400 of 100000; Loss: 3.0025e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9730e+03; Test Loss: 1.9888e+03. (Time: 2.9s)\n",
            "first loss: 3002.459997558594 test loss new: 3001.7996923828123\n",
            "updating best model ..\n",
            "Step 14600 of 100000; Loss: 3.0018e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.2155e+03; Test Loss: 1.9894e+03. (Time: 2.4s)\n",
            "first loss: 3001.7996923828123 test loss new: 3001.141396484375\n",
            "updating best model ..\n",
            "Step 14800 of 100000; Loss: 3.0011e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1900e+03; Test Loss: 1.9900e+03. (Time: 2.3s)\n",
            "first loss: 3001.141396484375 test loss new: 3000.5033837890624\n",
            "updating best model ..\n",
            "Step 15000 of 100000; Loss: 3.0005e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3812e+03; Test Loss: 1.9906e+03. (Time: 2.3s)\n",
            "first loss: 3000.5033837890624 test loss new: 2999.8688110351563\n",
            "updating best model ..\n",
            "Step 15200 of 100000; Loss: 2.9999e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3574e+03; Test Loss: 1.9912e+03. (Time: 3.8s)\n",
            "first loss: 2999.8688110351563 test loss new: 2999.261530761719\n",
            "updating best model ..\n",
            "Step 15400 of 100000; Loss: 2.9993e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3782e+03; Test Loss: 1.9918e+03. (Time: 2.3s)\n",
            "first loss: 2999.261530761719 test loss new: 2998.653410644531\n",
            "updating best model ..\n",
            "Step 15600 of 100000; Loss: 2.9987e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6443e+03; Test Loss: 1.9923e+03. (Time: 2.3s)\n",
            "first loss: 2998.653410644531 test loss new: 2998.0728588867187\n",
            "updating best model ..\n",
            "Step 15800 of 100000; Loss: 2.9981e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3927e+02; Test Loss: 1.9929e+03. (Time: 2.3s)\n",
            "first loss: 2998.0728588867187 test loss new: 2997.493352050781\n",
            "updating best model ..\n",
            "Step 16000 of 100000; Loss: 2.9975e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2946e+03; Test Loss: 1.9935e+03. (Time: 2.3s)\n",
            "first loss: 2997.493352050781 test loss new: 2996.934216308594\n",
            "updating best model ..\n",
            "Step 16200 of 100000; Loss: 2.9969e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9286e+03; Test Loss: 1.9940e+03. (Time: 3.3s)\n",
            "first loss: 2996.934216308594 test loss new: 2996.373986816406\n",
            "updating best model ..\n",
            "Step 16400 of 100000; Loss: 2.9964e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5954e+03; Test Loss: 1.9946e+03. (Time: 2.4s)\n",
            "first loss: 2996.373986816406 test loss new: 2995.8146508789064\n",
            "updating best model ..\n",
            "Step 16600 of 100000; Loss: 2.9958e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9399e+03; Test Loss: 1.9952e+03. (Time: 2.3s)\n",
            "first loss: 2995.8146508789064 test loss new: 2995.272294921875\n",
            "updating best model ..\n",
            "Step 16800 of 100000; Loss: 2.9953e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3796e+03; Test Loss: 1.9958e+03. (Time: 2.3s)\n",
            "first loss: 2995.272294921875 test loss new: 2994.727727050781\n",
            "updating best model ..\n",
            "Step 17000 of 100000; Loss: 2.9947e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9849e+03; Test Loss: 1.9964e+03. (Time: 2.3s)\n",
            "first loss: 2994.727727050781 test loss new: 2994.2027416992187\n",
            "updating best model ..\n",
            "Step 17200 of 100000; Loss: 2.9942e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1933e+03; Test Loss: 1.9970e+03. (Time: 2.9s)\n",
            "first loss: 2994.2027416992187 test loss new: 2993.676530761719\n",
            "updating best model ..\n",
            "Step 17400 of 100000; Loss: 2.9937e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1897e+03; Test Loss: 1.9976e+03. (Time: 2.9s)\n",
            "first loss: 2993.676530761719 test loss new: 2993.167939453125\n",
            "updating best model ..\n",
            "Step 17600 of 100000; Loss: 2.9932e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3444e+03; Test Loss: 1.9981e+03. (Time: 2.3s)\n",
            "first loss: 2993.167939453125 test loss new: 2992.659533691406\n",
            "updating best model ..\n",
            "Step 17800 of 100000; Loss: 2.9927e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3321e+03; Test Loss: 1.9987e+03. (Time: 2.3s)\n",
            "first loss: 2992.659533691406 test loss new: 2992.1751318359375\n",
            "updating best model ..\n",
            "Step 18000 of 100000; Loss: 2.9922e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3414e+03; Test Loss: 1.9992e+03. (Time: 2.3s)\n",
            "first loss: 2992.1751318359375 test loss new: 2991.6871142578125\n",
            "updating best model ..\n",
            "Step 18200 of 100000; Loss: 2.9917e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6232e+03; Test Loss: 1.9998e+03. (Time: 2.7s)\n",
            "first loss: 2991.6871142578125 test loss new: 2991.2235815429685\n",
            "updating best model ..\n",
            "Step 18400 of 100000; Loss: 2.9912e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.4254e+02; Test Loss: 2.0003e+03. (Time: 3.1s)\n",
            "first loss: 2991.2235815429685 test loss new: 2990.7581005859374\n",
            "updating best model ..\n",
            "Step 18600 of 100000; Loss: 2.9908e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2963e+03; Test Loss: 2.0008e+03. (Time: 2.3s)\n",
            "first loss: 2990.7581005859374 test loss new: 2990.3108911132813\n",
            "updating best model ..\n",
            "Step 18800 of 100000; Loss: 2.9903e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9386e+03; Test Loss: 2.0013e+03. (Time: 2.3s)\n",
            "first loss: 2990.3108911132813 test loss new: 2989.86021484375\n",
            "updating best model ..\n",
            "Step 19000 of 100000; Loss: 2.9899e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5960e+03; Test Loss: 2.0019e+03. (Time: 2.7s)\n",
            "first loss: 2989.86021484375 test loss new: 2989.409558105469\n",
            "updating best model ..\n",
            "Step 19200 of 100000; Loss: 2.9894e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9532e+03; Test Loss: 2.0024e+03. (Time: 2.6s)\n",
            "first loss: 2989.409558105469 test loss new: 2988.974091796875\n",
            "updating best model ..\n",
            "Step 19400 of 100000; Loss: 2.9890e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3821e+03; Test Loss: 2.0030e+03. (Time: 3.2s)\n",
            "first loss: 2988.974091796875 test loss new: 2988.5338549804687\n",
            "updating best model ..\n",
            "Step 19600 of 100000; Loss: 2.9885e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9968e+03; Test Loss: 2.0035e+03. (Time: 2.3s)\n",
            "first loss: 2988.5338549804687 test loss new: 2988.111325683594\n",
            "updating best model ..\n",
            "Step 19800 of 100000; Loss: 2.9881e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1738e+03; Test Loss: 2.0040e+03. (Time: 2.3s)\n",
            "first loss: 2988.111325683594 test loss new: 2987.685393066406\n",
            "updating best model ..\n",
            "Step 20000 of 100000; Loss: 2.9877e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1914e+03; Test Loss: 2.0046e+03. (Time: 2.3s)\n",
            "first loss: 2987.685393066406 test loss new: 2987.2751220703126\n",
            "updating best model ..\n",
            "Step 20200 of 100000; Loss: 2.9873e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3121e+03; Test Loss: 2.0051e+03. (Time: 2.3s)\n",
            "first loss: 2987.2751220703126 test loss new: 2986.862880859375\n",
            "updating best model ..\n",
            "Step 20400 of 100000; Loss: 2.9869e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3128e+03; Test Loss: 2.0056e+03. (Time: 3.5s)\n",
            "first loss: 2986.862880859375 test loss new: 2986.471923828125\n",
            "updating best model ..\n",
            "Step 20600 of 100000; Loss: 2.9865e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3093e+03; Test Loss: 2.0061e+03. (Time: 2.3s)\n",
            "first loss: 2986.471923828125 test loss new: 2986.0757275390624\n",
            "updating best model ..\n",
            "Step 20800 of 100000; Loss: 2.9861e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6073e+03; Test Loss: 2.0066e+03. (Time: 2.3s)\n",
            "first loss: 2986.0757275390624 test loss new: 2985.7014599609374\n",
            "updating best model ..\n",
            "Step 21000 of 100000; Loss: 2.9857e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.4556e+02; Test Loss: 2.0070e+03. (Time: 2.3s)\n",
            "first loss: 2985.7014599609374 test loss new: 2985.323186035156\n",
            "updating best model ..\n",
            "Step 21200 of 100000; Loss: 2.9853e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2977e+03; Test Loss: 2.0075e+03. (Time: 2.3s)\n",
            "first loss: 2985.323186035156 test loss new: 2984.961667480469\n",
            "updating best model ..\n",
            "Step 21400 of 100000; Loss: 2.9850e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9483e+03; Test Loss: 2.0080e+03. (Time: 3.2s)\n",
            "first loss: 2984.961667480469 test loss new: 2984.5949365234374\n",
            "updating best model ..\n",
            "Step 21600 of 100000; Loss: 2.9846e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5964e+03; Test Loss: 2.0085e+03. (Time: 2.5s)\n",
            "first loss: 2984.5949365234374 test loss new: 2984.2278295898436\n",
            "updating best model ..\n",
            "Step 21800 of 100000; Loss: 2.9842e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9654e+03; Test Loss: 2.0089e+03. (Time: 2.3s)\n",
            "first loss: 2984.2278295898436 test loss new: 2983.8744970703124\n",
            "updating best model ..\n",
            "Step 22000 of 100000; Loss: 2.9839e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3841e+03; Test Loss: 2.0094e+03. (Time: 2.2s)\n",
            "first loss: 2983.8744970703124 test loss new: 2983.514816894531\n",
            "updating best model ..\n",
            "Step 22200 of 100000; Loss: 2.9835e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.0080e+03; Test Loss: 2.0099e+03. (Time: 2.3s)\n",
            "first loss: 2983.514816894531 test loss new: 2983.171159667969\n",
            "updating best model ..\n",
            "Step 22400 of 100000; Loss: 2.9832e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1571e+03; Test Loss: 2.0104e+03. (Time: 2.8s)\n",
            "first loss: 2983.171159667969 test loss new: 2982.822770996094\n",
            "updating best model ..\n",
            "Step 22600 of 100000; Loss: 2.9828e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1940e+03; Test Loss: 2.0108e+03. (Time: 3.0s)\n",
            "first loss: 2982.822770996094 test loss new: 2982.4884594726564\n",
            "updating best model ..\n",
            "Step 22800 of 100000; Loss: 2.9825e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2842e+03; Test Loss: 2.0113e+03. (Time: 2.3s)\n",
            "first loss: 2982.4884594726564 test loss new: 2982.150751953125\n",
            "updating best model ..\n",
            "Step 23000 of 100000; Loss: 2.9822e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2980e+03; Test Loss: 2.0117e+03. (Time: 2.3s)\n",
            "first loss: 2982.150751953125 test loss new: 2981.831955566406\n",
            "updating best model ..\n",
            "Step 23200 of 100000; Loss: 2.9818e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2815e+03; Test Loss: 2.0122e+03. (Time: 2.3s)\n",
            "first loss: 2981.831955566406 test loss new: 2981.506884765625\n",
            "updating best model ..\n",
            "Step 23400 of 100000; Loss: 2.9815e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5952e+03; Test Loss: 2.0126e+03. (Time: 3.1s)\n",
            "first loss: 2981.506884765625 test loss new: 2981.201604003906\n",
            "updating best model ..\n",
            "Step 23600 of 100000; Loss: 2.9812e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.4824e+02; Test Loss: 2.0130e+03. (Time: 3.0s)\n",
            "first loss: 2981.201604003906 test loss new: 2980.890959472656\n",
            "updating best model ..\n",
            "Step 23800 of 100000; Loss: 2.9809e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2989e+03; Test Loss: 2.0134e+03. (Time: 2.3s)\n",
            "first loss: 2980.890959472656 test loss new: 2980.595725097656\n",
            "updating best model ..\n",
            "Step 24000 of 100000; Loss: 2.9806e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9571e+03; Test Loss: 2.0138e+03. (Time: 2.3s)\n",
            "first loss: 2980.595725097656 test loss new: 2980.2943994140624\n",
            "updating best model ..\n",
            "Step 24200 of 100000; Loss: 2.9803e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5967e+03; Test Loss: 2.0142e+03. (Time: 2.3s)\n",
            "first loss: 2980.2943994140624 test loss new: 2979.9922412109377\n",
            "updating best model ..\n",
            "Step 24400 of 100000; Loss: 2.9800e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9762e+03; Test Loss: 2.0146e+03. (Time: 2.5s)\n",
            "first loss: 2979.9922412109377 test loss new: 2979.7027734375\n",
            "updating best model ..\n",
            "Step 24600 of 100000; Loss: 2.9797e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3859e+03; Test Loss: 2.0151e+03. (Time: 3.2s)\n",
            "first loss: 2979.7027734375 test loss new: 2979.4058984375\n",
            "updating best model ..\n",
            "Step 24800 of 100000; Loss: 2.9794e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.0179e+03; Test Loss: 2.0155e+03. (Time: 2.3s)\n",
            "first loss: 2979.4058984375 test loss new: 2979.1237451171874\n",
            "updating best model ..\n",
            "Step 25000 of 100000; Loss: 2.9791e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1429e+03; Test Loss: 2.0159e+03. (Time: 2.3s)\n",
            "first loss: 2979.1237451171874 test loss new: 2978.83587890625\n",
            "updating best model ..\n",
            "Step 25200 of 100000; Loss: 2.9788e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1969e+03; Test Loss: 2.0163e+03. (Time: 2.3s)\n",
            "first loss: 2978.83587890625 test loss new: 2978.5607983398436\n",
            "updating best model ..\n",
            "Step 25400 of 100000; Loss: 2.9786e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2602e+03; Test Loss: 2.0167e+03. (Time: 2.3s)\n",
            "first loss: 2978.5607983398436 test loss new: 2978.281259765625\n",
            "updating best model ..\n",
            "Step 25600 of 100000; Loss: 2.9783e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2865e+03; Test Loss: 2.0170e+03. (Time: 3.4s)\n",
            "first loss: 2978.281259765625 test loss new: 2978.0188427734374\n",
            "updating best model ..\n",
            "Step 25800 of 100000; Loss: 2.9780e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2577e+03; Test Loss: 2.0174e+03. (Time: 2.3s)\n",
            "first loss: 2978.0188427734374 test loss new: 2977.7493798828127\n",
            "updating best model ..\n",
            "Step 26000 of 100000; Loss: 2.9777e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5859e+03; Test Loss: 2.0178e+03. (Time: 2.3s)\n",
            "first loss: 2977.7493798828127 test loss new: 2977.4979833984376\n",
            "updating best model ..\n",
            "Step 26200 of 100000; Loss: 2.9775e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5055e+02; Test Loss: 2.0181e+03. (Time: 2.3s)\n",
            "first loss: 2977.4979833984376 test loss new: 2977.2402587890624\n",
            "updating best model ..\n",
            "Step 26400 of 100000; Loss: 2.9772e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3002e+03; Test Loss: 2.0184e+03. (Time: 2.3s)\n",
            "first loss: 2977.2402587890624 test loss new: 2976.9967944335935\n",
            "updating best model ..\n",
            "Step 26600 of 100000; Loss: 2.9770e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9647e+03; Test Loss: 2.0188e+03. (Time: 3.2s)\n",
            "first loss: 2976.9967944335935 test loss new: 2976.7465405273438\n",
            "updating best model ..\n",
            "Step 26800 of 100000; Loss: 2.9767e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5970e+03; Test Loss: 2.0191e+03. (Time: 2.5s)\n",
            "first loss: 2976.7465405273438 test loss new: 2976.4953369140626\n",
            "updating best model ..\n",
            "Step 27000 of 100000; Loss: 2.9765e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9853e+03; Test Loss: 2.0194e+03. (Time: 2.3s)\n",
            "first loss: 2976.4953369140626 test loss new: 2976.2559423828125\n",
            "updating best model ..\n",
            "Step 27200 of 100000; Loss: 2.9763e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3876e+03; Test Loss: 2.0198e+03. (Time: 2.3s)\n",
            "first loss: 2976.2559423828125 test loss new: 2976.008408203125\n",
            "updating best model ..\n",
            "Step 27400 of 100000; Loss: 2.9760e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.0265e+03; Test Loss: 2.0202e+03. (Time: 2.3s)\n",
            "first loss: 2976.008408203125 test loss new: 2975.7744262695314\n",
            "updating best model ..\n",
            "Step 27600 of 100000; Loss: 2.9758e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1309e+03; Test Loss: 2.0205e+03. (Time: 2.9s)\n",
            "first loss: 2975.7744262695314 test loss new: 2975.5342846679687\n",
            "updating best model ..\n",
            "Step 27800 of 100000; Loss: 2.9755e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1998e+03; Test Loss: 2.0208e+03. (Time: 2.9s)\n",
            "first loss: 2975.5342846679687 test loss new: 2975.305642089844\n",
            "updating best model ..\n",
            "Step 28000 of 100000; Loss: 2.9753e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2396e+03; Test Loss: 2.0212e+03. (Time: 2.3s)\n",
            "first loss: 2975.305642089844 test loss new: 2975.0719946289064\n",
            "updating best model ..\n",
            "Step 28200 of 100000; Loss: 2.9751e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2776e+03; Test Loss: 2.0215e+03. (Time: 2.3s)\n",
            "first loss: 2975.0719946289064 test loss new: 2974.853796386719\n",
            "updating best model ..\n",
            "Step 28400 of 100000; Loss: 2.9749e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2374e+03; Test Loss: 2.0218e+03. (Time: 2.7s)\n",
            "first loss: 2974.853796386719 test loss new: 2974.628212890625\n",
            "updating best model ..\n",
            "Step 28600 of 100000; Loss: 2.9746e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5787e+03; Test Loss: 2.0221e+03. (Time: 2.9s)\n",
            "first loss: 2974.628212890625 test loss new: 2974.4190258789063\n",
            "updating best model ..\n",
            "Step 28800 of 100000; Loss: 2.9744e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5248e+02; Test Loss: 2.0223e+03. (Time: 2.8s)\n",
            "first loss: 2974.4190258789063 test loss new: 2974.2029223632812\n",
            "updating best model ..\n",
            "Step 29000 of 100000; Loss: 2.9742e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3015e+03; Test Loss: 2.0226e+03. (Time: 2.3s)\n",
            "first loss: 2974.2029223632812 test loss new: 2974.0000634765624\n",
            "updating best model ..\n",
            "Step 29200 of 100000; Loss: 2.9740e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9710e+03; Test Loss: 2.0229e+03. (Time: 2.3s)\n",
            "first loss: 2974.0000634765624 test loss new: 2973.7900146484376\n",
            "updating best model ..\n",
            "Step 29400 of 100000; Loss: 2.9738e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5976e+03; Test Loss: 2.0232e+03. (Time: 2.2s)\n",
            "first loss: 2973.7900146484376 test loss new: 2973.5790185546875\n",
            "updating best model ..\n",
            "Step 29600 of 100000; Loss: 2.9736e+03. (Time: 0.0s)\n",
            "Step 30 of 200; Loss: 4.0323e+03; Test Loss: 2.0232e+03. (Time: 1.2s)"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1060343794.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 这个相当于是最好的认知模型，在rescolar里面加了一些参数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m rl_pc_params, _ = rnn_utils.fit_model(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_fun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbandits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHk_PreserveConAgentQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdataset_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_validate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CogModelingRNNsTutorial/rnn_utils.py\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model_fun, dataset_train, dataset_test, optimizer, loss_fun, random_key, n_steps_per_call, n_steps_max, return_all_losses, early_stop_step, if_early_stop)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m     params, opt_state, losses = train_model(\n\u001b[0m\u001b[1;32m    549\u001b[0m         \u001b[0mmodel_fun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CogModelingRNNsTutorial/rnn_utils.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_fun, dataset_train, dataset_test, optimizer, random_key, opt_state, params, n_steps, penalty_scale, loss_fun, do_plot, truncate_seq_length)\u001b[0m\n\u001b[1;32m    265\u001b[0m       \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtruncate_seq_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m     \u001b[0mtraining_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(_cls)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 这个相当于是最好的认知模型，在rescolar里面加了一些参数\n",
        "rl_pc_params, _ = rnn_utils.fit_model(\n",
        "    model_fun=bandits.Hk_PreserveConAgentQ,\n",
        "    dataset_train = dataset_train,\n",
        "    dataset_test = dataset_validate,\n",
        "    loss_fun='categorical',\n",
        "    optimizer= optax.chain(\n",
        "        optax.add_decayed_weights(1e-5),  # L2 正则化\n",
        "        optax.adam(learning_rate=1e-4)  # Adam 优化器\n",
        "    ),\n",
        "    n_steps_per_call=200,\n",
        "    n_steps_max=100000,\n",
        "    #return_all_losses=True,\n",
        "    early_stop_step=200,\n",
        "    if_early_stop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJIf1uLXwYLM",
        "outputId": "613e2379-28cc-4a12-8402-5b82183e9405"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Normalized Likelihood: 48.5%\n"
          ]
        }
      ],
      "source": [
        "mean,std = compute_log_likelihood(dataset_test, bandits.Hk_PreserveConAgentQ, rl_pc_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKUz7-bxNvub"
      },
      "outputs": [],
      "source": [
        "#@title Set up the RNN (Vanilla RNN) Model\n",
        "n_hidden = 8\n",
        "def make_vanilla_rnn():\n",
        "    model = hk.DeepRNN(\n",
        "        [hk.VanillaRNN(n_hidden), hk.Linear(output_size=2)]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "#@title Set up the RNN (Vanilla RNN) Model\n",
        "n_hidden = 8\n",
        "def make_lstm():\n",
        "    model = hk.DeepRNN(\n",
        "        [hk.LSTM(n_hidden), hk.Linear(output_size=2)]\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "khylH5F1OKHK",
        "outputId": "4eaafe01-db93-4ae3-bdc1-06e810217d6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 200 of 200; Loss: 2.6275e+03; Test Loss: 2.5221e+03. (Time: 4.7s)updating best model ..\n",
            "Step 200 of 100000; Loss: 3.0262e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1956e+03; Test Loss: 2.5076e+03. (Time: 3.9s)updating best model ..\n",
            "Step 400 of 100000; Loss: 3.0114e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.0060e+03; Test Loss: 2.4942e+03. (Time: 4.3s)updating best model ..\n",
            "Step 600 of 100000; Loss: 2.9995e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.0375e+03; Test Loss: 2.4818e+03. (Time: 4.8s)updating best model ..\n",
            "Step 800 of 100000; Loss: 2.9901e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2896e+03; Test Loss: 2.4699e+03. (Time: 3.8s)updating best model ..\n",
            "Step 1000 of 100000; Loss: 2.9821e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.0236e+03; Test Loss: 2.4578e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1200 of 100000; Loss: 2.9748e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.1469e+03; Test Loss: 2.4449e+03. (Time: 4.8s)updating best model ..\n",
            "Step 1400 of 100000; Loss: 2.9673e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.9505e+03; Test Loss: 2.4300e+03. (Time: 3.8s)updating best model ..\n",
            "Step 1600 of 100000; Loss: 2.9588e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6482e+03; Test Loss: 2.4133e+03. (Time: 3.8s)updating best model ..\n",
            "Step 1800 of 100000; Loss: 2.9494e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6579e+03; Test Loss: 2.3955e+03. (Time: 4.6s)updating best model ..\n",
            "Step 2000 of 100000; Loss: 2.9394e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6355e+03; Test Loss: 2.3780e+03. (Time: 3.8s)updating best model ..\n",
            "Step 2200 of 100000; Loss: 2.9297e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9643e+03; Test Loss: 2.3620e+03. (Time: 3.6s)updating best model ..\n",
            "Step 2400 of 100000; Loss: 2.9206e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6343e+03; Test Loss: 2.3475e+03. (Time: 4.5s)updating best model ..\n",
            "Step 2600 of 100000; Loss: 2.9125e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2995e+03; Test Loss: 2.3353e+03. (Time: 3.6s)updating best model ..\n",
            "Step 2800 of 100000; Loss: 2.9054e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1132e+03; Test Loss: 2.3244e+03. (Time: 3.6s)updating best model ..\n",
            "Step 3000 of 100000; Loss: 2.8993e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.0844e+03; Test Loss: 2.3150e+03. (Time: 4.1s)updating best model ..\n",
            "Step 3200 of 100000; Loss: 2.8938e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.6416e+03; Test Loss: 2.3065e+03. (Time: 3.6s)updating best model ..\n",
            "Step 3400 of 100000; Loss: 2.8890e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3279e+03; Test Loss: 2.2987e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3600 of 100000; Loss: 2.8845e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.6887e+03; Test Loss: 2.2914e+03. (Time: 4.0s)updating best model ..\n",
            "Step 3800 of 100000; Loss: 2.8803e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.1835e+03; Test Loss: 2.2847e+03. (Time: 3.6s)updating best model ..\n",
            "Step 4000 of 100000; Loss: 2.8765e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.6189e+03; Test Loss: 2.2784e+03. (Time: 3.6s)updating best model ..\n",
            "Step 4200 of 100000; Loss: 2.8728e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6547e+03; Test Loss: 2.2721e+03. (Time: 4.0s)updating best model ..\n",
            "Step 4400 of 100000; Loss: 2.8693e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.4394e+03; Test Loss: 2.2666e+03. (Time: 3.6s)updating best model ..\n",
            "Step 4600 of 100000; Loss: 2.8660e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6320e+03; Test Loss: 2.2607e+03. (Time: 3.6s)updating best model ..\n",
            "Step 4800 of 100000; Loss: 2.8629e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8360e+03; Test Loss: 2.2559e+03. (Time: 3.8s)updating best model ..\n",
            "Step 5000 of 100000; Loss: 2.8599e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6120e+03; Test Loss: 2.2506e+03. (Time: 4.0s)updating best model ..\n",
            "Step 5200 of 100000; Loss: 2.8572e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2125e+03; Test Loss: 2.2464e+03. (Time: 3.7s)updating best model ..\n",
            "Step 5400 of 100000; Loss: 2.8546e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0758e+03; Test Loss: 2.2418e+03. (Time: 3.7s)updating best model ..\n",
            "Step 5600 of 100000; Loss: 2.8522e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.1448e+03; Test Loss: 2.2379e+03. (Time: 4.5s)updating best model ..\n",
            "Step 5800 of 100000; Loss: 2.8498e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4851e+03; Test Loss: 2.2340e+03. (Time: 3.6s)updating best model ..\n",
            "Step 6000 of 100000; Loss: 2.8477e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3665e+03; Test Loss: 2.2303e+03. (Time: 3.6s)updating best model ..\n",
            "Step 6200 of 100000; Loss: 2.8457e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5532e+03; Test Loss: 2.2268e+03. (Time: 4.5s)updating best model ..\n",
            "Step 6400 of 100000; Loss: 2.8438e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2168e+03; Test Loss: 2.2234e+03. (Time: 3.6s)updating best model ..\n",
            "Step 6600 of 100000; Loss: 2.8421e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5063e+03; Test Loss: 2.2204e+03. (Time: 3.7s)updating best model ..\n",
            "Step 6800 of 100000; Loss: 2.8404e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6546e+03; Test Loss: 2.2172e+03. (Time: 4.2s)updating best model ..\n",
            "Step 7000 of 100000; Loss: 2.8389e+03. (Time: 0.0s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _xla_gc_callback at 0x7d103f43cfe0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "    \n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 200 of 200; Loss: 3.3487e+03; Test Loss: 2.2146e+03. (Time: 4.0s)updating best model ..\n",
            "Step 7200 of 100000; Loss: 2.8375e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6303e+03; Test Loss: 2.2115e+03. (Time: 3.7s)updating best model ..\n",
            "Step 7400 of 100000; Loss: 2.8362e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7915e+03; Test Loss: 2.2091e+03. (Time: 4.1s)updating best model ..\n",
            "Step 7600 of 100000; Loss: 2.8350e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6021e+03; Test Loss: 2.2060e+03. (Time: 3.7s)updating best model ..\n",
            "Step 7800 of 100000; Loss: 2.8340e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1777e+03; Test Loss: 2.2033e+03. (Time: 3.7s)updating best model ..\n",
            "Step 8000 of 100000; Loss: 2.8327e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0539e+03; Test Loss: 2.1991e+03. (Time: 3.9s)updating best model ..\n",
            "Step 8200 of 100000; Loss: 2.8297e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.1424e+03; Test Loss: 2.1942e+03. (Time: 3.6s)updating best model ..\n",
            "Step 8400 of 100000; Loss: 2.8250e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4035e+03; Test Loss: 2.1899e+03. (Time: 3.6s)updating best model ..\n",
            "Step 8600 of 100000; Loss: 2.8194e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3696e+03; Test Loss: 2.1861e+03. (Time: 3.9s)updating best model ..\n",
            "Step 8800 of 100000; Loss: 2.8164e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4715e+03; Test Loss: 2.1830e+03. (Time: 3.8s)updating best model ..\n",
            "Step 9000 of 100000; Loss: 2.8092e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2051e+03; Test Loss: 2.1794e+03. (Time: 3.7s)updating best model ..\n",
            "Step 9200 of 100000; Loss: 2.8057e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4275e+03; Test Loss: 2.1765e+03. (Time: 3.7s)updating best model ..\n",
            "Step 9400 of 100000; Loss: 2.8032e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6944e+03; Test Loss: 2.1738e+03. (Time: 4.3s)updating best model ..\n",
            "Step 9600 of 100000; Loss: 2.8014e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2116e+03; Test Loss: 2.1717e+03. (Time: 3.6s)updating best model ..\n",
            "Step 9800 of 100000; Loss: 2.7998e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6479e+03; Test Loss: 2.1694e+03. (Time: 3.7s)updating best model ..\n",
            "Step 10000 of 100000; Loss: 2.7984e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6075e+03; Test Loss: 2.1676e+03. (Time: 4.5s)updating best model ..\n",
            "Step 10200 of 100000; Loss: 2.7970e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6094e+03; Test Loss: 2.1656e+03. (Time: 3.7s)updating best model ..\n",
            "Step 10400 of 100000; Loss: 2.7958e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9776e+03; Test Loss: 2.1641e+03. (Time: 3.7s)updating best model ..\n",
            "Step 10600 of 100000; Loss: 2.7945e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0370e+03; Test Loss: 2.1623e+03. (Time: 4.5s)updating best model ..\n",
            "Step 10800 of 100000; Loss: 2.7933e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2220e+03; Test Loss: 2.1603e+03. (Time: 4.1s)updating best model ..\n",
            "Step 11000 of 100000; Loss: 2.7919e+03. (Time: 0.0s)\n",
            "Step 30 of 200; Loss: 4.4276e+03; Test Loss: 2.1604e+03. (Time: 1.6s)"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3664668.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#n_steps_max = 1000000 #@param\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m gru_params, _, all_losses = rnn_utils.fit_model(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_fun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_lstm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CogModelingRNNsTutorial/rnn_utils.py\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model_fun, dataset_train, dataset_test, optimizer, loss_fun, random_key, n_steps_per_call, n_steps_max, return_all_losses, early_stop_step, if_early_stop)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m     params, opt_state, losses = train_model(\n\u001b[0m\u001b[1;32m    549\u001b[0m         \u001b[0mmodel_fun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CogModelingRNNsTutorial/rnn_utils.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_fun, dataset_train, dataset_test, optimizer, random_key, opt_state, params, n_steps, penalty_scale, loss_fun, do_plot, truncate_seq_length)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;31m# Log every 10th step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m       print((f'\\rStep {step + 1} of {n_steps}; '\n\u001b[1;32m    282\u001b[0m              \u001b[0;34mf'Loss: {loss:.4e}; Test Loss: {test_loss:.4e}. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/numpy/array_methods.py\u001b[0m in \u001b[0;36mdeferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mswap\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_accepted_binop_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m     \u001b[0;31m# Note: don't use isinstance here, because we don't want to raise for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;31m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title Fit the RNN (GRU) model\n",
        "#n_steps_max = 1000000 #@param\n",
        "\n",
        "gru_params, _, all_losses = rnn_utils.fit_model(\n",
        "    model_fun=make_lstm,\n",
        "    dataset_train = dataset_train,\n",
        "    dataset_test = dataset_validate,\n",
        "    loss_fun='categorical',\n",
        "    optimizer= optax.chain(\n",
        "        optax.add_decayed_weights(1e-5),  # L2 正则化\n",
        "        optax.adam(learning_rate=1e-4)  # Adam 优化器\n",
        "    ),\n",
        "    n_steps_per_call=200,\n",
        "    n_steps_max=100000,\n",
        "    return_all_losses=True,\n",
        "    early_stop_step=200,\n",
        "    if_early_stop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOpDYABeOaEM"
      },
      "outputs": [],
      "source": [
        "# #@title Compute log-likelihood\n",
        "# def compute_log_likelihood(dataset, model_fun, params):\n",
        "\n",
        "#   xs, actual_choices = next(dataset)\n",
        "#   n_trials_per_session, n_sessions = actual_choices.shape[:2]\n",
        "#   model_outputs, model_states = rnn_utils.eval_model(model_fun, params, xs)\n",
        "\n",
        "#   predicted_log_choice_probabilities = np.array(jax.nn.log_softmax(model_outputs[:, :, :2]))\n",
        "\n",
        "#   log_likelihood = 0\n",
        "#   n = 0  # Total number of trials across sessions.\n",
        "#   for sess_i in range(n_sessions):\n",
        "#     for trial_i in range(n_trials_per_session):\n",
        "#       actual_choice = int(actual_choices[trial_i, sess_i])\n",
        "#       if actual_choice >= 0:  # values < 0 are invalid trials which we ignore.\n",
        "#         log_likelihood += predicted_log_choice_probabilities[trial_i, sess_i, actual_choice]\n",
        "#         n += 1\n",
        "\n",
        "#   normalized_likelihood = np.exp(log_likelihood / n)\n",
        "\n",
        "#   print(f'Normalized Likelihood: {100 * normalized_likelihood:.1f}%')\n",
        "\n",
        "#   return normalized_likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ute8Yj_xOMd8",
        "outputId": "ca85be09-e42c-41a5-dd3e-13c9a6bc0d28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalized Likelihoods for GRU\n",
            "Training Dataset\n",
            "Average Normalized Likelihood: 61.3%\n",
            "Held-Out Dataset\n",
            "Average Normalized Likelihood: 62.0%\n"
          ]
        }
      ],
      "source": [
        "#@title Compute quality-of-fit: Held-out Normalized Likelihood\n",
        "# Compute log-likelihood\n",
        "print('Normalized Likelihoods for GRU')\n",
        "print('Training Dataset')\n",
        "training_likelihood = compute_log_likelihood(dataset_train, make_lstm, gru_params)\n",
        "print('Held-Out Dataset')\n",
        "testing_likelihood = compute_log_likelihood(dataset_validate, make_lstm, gru_params)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "i7x3Ff2xWTdb",
        "YOGiNjm4FUAx",
        "WmgO7HywNl5-",
        "GvGdnab8CdCN",
        "mrmkwDvr500Q",
        "9WygdkV_tSUA",
        "EYkrrFeXOqG2"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (CogModel)",
      "language": "python",
      "name": "cogmodel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
