{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7x3Ff2xWTdb"
      },
      "source": [
        "# Import and preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYuMvSuvyGuR",
        "outputId": "1991ace2-58e1-4bab-8216-ba8844c70938"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "import gdown\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# !pip install --upgrade pip\n",
        "# !pip install --upgrade \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "\n",
        "\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "import optax\n",
        "import scipy\n",
        "\n",
        "import matplotlib as mpl\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "\n",
        "\n",
        "# !pip install -U dm-haiku\n",
        "import haiku as hk\n",
        "rng_seq = hk.PRNGSequence(np.random.randint(2**32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbWhtXrJAns0",
        "outputId": "bd2dccfa-11be-4d61-9675-e9d177abaaf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available JAX devices: [CudaDevice(id=0)]\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "# 这行会打印出可用的设备，应该至少能看到一个 GpuDevice\n",
        "print(\"Available JAX devices:\", jax.devices())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX2twY1NNT1r",
        "outputId": "c5319c67-b25e-4433-c01c-9fc763af91e9"
      },
      "outputs": [],
      "source": [
        "# try:\n",
        "#     from google.colab import files\n",
        "#     _ON_COLAB = True\n",
        "# except:\n",
        "#     _ON_COLAB = False\n",
        "\n",
        "# if _ON_COLAB:\n",
        "#   !rm -rf CogModelingRNNsTutorial\n",
        "#   !git clone https://github.com/YifeiCAO/CogModelingRNNsTutorial\n",
        "#   !pip install -e CogModelingRNNsTutorial/CogModelingRNNsTutorial\n",
        "#   !cp CogModelingRNNsTutorial/CogModelingRNNsTutorial/*py CogModelingRNNsTutorial\n",
        "# else:\n",
        "#   !pip install CogModelingRNNsTutorial/requirements.txt\n",
        "\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "\n",
        "# import haiku as hk\n",
        "# import jax\n",
        "# import jax.numpy as jnp\n",
        "\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# try:\n",
        "#     from google.colab import files\n",
        "#     _ON_COLAB = True\n",
        "# except:\n",
        "#     _ON_COLAB = False\n",
        "\n",
        "from CogModelingRNNsTutorial.CogModelingRNNsTutorial import bandits\n",
        "from CogModelingRNNsTutorial.CogModelingRNNsTutorial import disrnn\n",
        "from CogModelingRNNsTutorial.CogModelingRNNsTutorial import hybrnn\n",
        "from CogModelingRNNsTutorial.CogModelingRNNsTutorial import hybconrnn\n",
        "from CogModelingRNNsTutorial.CogModelingRNNsTutorial import hybrnn_direct_con\n",
        "from CogModelingRNNsTutorial.CogModelingRNNsTutorial import plotting\n",
        "from CogModelingRNNsTutorial.CogModelingRNNsTutorial import rat_data\n",
        "from CogModelingRNNsTutorial.CogModelingRNNsTutorial import rnn_utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTr066RBtXvr"
      },
      "source": [
        "## Downloading Data for different species"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiTOCRKDqY0Y"
      },
      "source": [
        "### Monkey Data Download and tidy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfNF-4GgiDVW",
        "outputId": "4e6bea18-73a3-4398-e0c5-a43349b0a65e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1HqQxA2uIz6C2Eq-51rhrkFj0DMoS25RW\n",
            "To: /teamspace/studios/this_studio/src/hua_tang_data.csv\n",
            "100%|██████████| 363k/363k [00:00<00:00, 112MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Monkey dataset from Hua Tang, Bartolo\n",
        "import gdown\n",
        "\n",
        "# downlowd link for Google Drive\n",
        "file_id_tang = '1HqQxA2uIz6C2Eq-51rhrkFj0DMoS25RW'\n",
        "url_tang = f'https://drive.google.com/uc?id={file_id_tang}'\n",
        "output_tang = 'hua_tang_data.csv'\n",
        "gdown.download(url_tang, output_tang, quiet=False)\n",
        "monkey_data = pd.read_csv(output_tang)\n",
        "\n",
        "# 为每个 (monkey, date, blockIndex) 内生成 trial_id，从0递增\n",
        "monkey_data['trial_id'] = monkey_data.groupby(['monkey', 'date', 'blockIndex']).cumcount()\n",
        "\n",
        "# 操作1：每只猴子按照日期从早到晚编码\n",
        "monkey_data['date'] = pd.to_datetime(monkey_data['date'], format='%Y%m%d')\n",
        "monkey_data.sort_values(by=['monkey', 'date'], inplace=True)\n",
        "\n",
        "monkey_data['date'] = monkey_data.groupby('monkey')['date'].rank(method='dense').astype(int)\n",
        "\n",
        "# 操作2：根据blockType创建新变量action\n",
        "monkey_data['action'] = monkey_data.apply(lambda x: x['trialObject'] if x['blockType'] == 1 else x['trialDirection'], axis=1)\n",
        "\n",
        "# 创建 session key\n",
        "monkey_data['session_key'] = monkey_data[['monkey', 'date', 'blockIndex']].astype(str).agg('-'.join, axis=1)\n",
        "\n",
        "# 编号 session，从1开始\n",
        "monkey_data['session'] = monkey_data['session_key'].astype('category').cat.codes + 1\n",
        "\n",
        "# 可选：删除临时变量\n",
        "monkey_data = monkey_data.drop(columns='session_key')\n",
        "\n",
        "# Dataset Shape information\n",
        "n_participants_monkey = 2\n",
        "n_trials_monkey = 80\n",
        "n_bandits_monkey = 2\n",
        "min_points_monkey = 1\n",
        "max_points_monkey = 0\n",
        "\n",
        "#Creates one hot code for actions\n",
        "action_cols = ['action_{}'.format(i) for i in range(2)]  # this creates the following vector: 'action_0', 'action_1', 'action_2', 'action_3']\n",
        "monkey_data[action_cols] = jax.nn.one_hot(jnp.array(monkey_data['action']), 2)\n",
        "\n",
        "#Generate dataset for training, validation\n",
        "def generate_action_n(group):\n",
        "    group['action_n'] = group['action'].shift(-1)  # 将 action 列向上移动一行\n",
        "    group['action_n'].iloc[-1] = -1  # 将每个组的最后一个值设为 -1\n",
        "    return group\n",
        "\n",
        "# 按 participant 分组并应用函数\n",
        "monkey_data = monkey_data.groupby(['session']).apply(generate_action_n)\n",
        "monkey_data = monkey_data.reset_index(drop=True)\n",
        "\n",
        "# ✅ 删除 trial 数不足 80 的 session\n",
        "session_lengths = monkey_data['session'].value_counts()\n",
        "valid_sessions = session_lengths[session_lengths >= 80].index\n",
        "monkey_data = monkey_data[monkey_data['session'].isin(valid_sessions)]\n",
        "\n",
        "monkey_rnn = monkey_data.sort_values(by=['session','trial_id'])\n",
        "\n",
        "# 提取action和reward列并转换为numpy数组\n",
        "seq_rnn = monkey_rnn[['action', 'reward','action_n']]\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# 确保 monkey_rnn 是按 session 和 trial_id 排好序的\n",
        "monkey_rnn = monkey_data.sort_values(by=['session', 'trial_id'])\n",
        "\n",
        "# 获取所有 session 的 id（已经筛选为 trial 数 >= 80）\n",
        "session_ids = monkey_rnn['session'].unique()\n",
        "\n",
        "# 初始化空列表\n",
        "xs_list = []\n",
        "ys_list = []\n",
        "\n",
        "# 遍历每个 session\n",
        "for session_id in session_ids:\n",
        "    session_data = monkey_rnn[monkey_rnn['session'] == session_id]\n",
        "\n",
        "    # 只保留前 80 个 trial（确保一致）\n",
        "    session_data = session_data.iloc[:80]\n",
        "\n",
        "    x = session_data[['action', 'reward']].to_numpy().astype(float)  # shape (80, 2)\n",
        "    y = session_data[['action_n']].to_numpy().astype(int)            # shape (80, 1)\n",
        "\n",
        "    xs_list.append(x)\n",
        "    ys_list.append(y)\n",
        "\n",
        "# 堆叠为 numpy array，并转置成 (80, 190, 2) 以及 (80, 190, 1)\n",
        "xs = np.stack(xs_list, axis=0)  # shape (80, n_sessions, 2)\n",
        "ys = np.stack(ys_list, axis=0)  # shape (80, n_sessions, 1)\n",
        "\n",
        "xs_t = np.stack(xs_list, axis=1)  # shape (80, n_sessions, 2)\n",
        "ys_t = np.stack(ys_list, axis=1)  # shape (80, n_sessions, 1\n",
        "#dataset_m_train, dataset_m_test, dataset_m_validate = rat_data.format_into_datasets(xs,ys, rnn_utils.DatasetRNN, 160, 15, 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "76ezppI5qWMx",
        "outputId": "6a57072a-d854-4c96-bc42-10670ea1e2a5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>trialObject</th>\n",
              "      <th>trialDirection</th>\n",
              "      <th>blockType</th>\n",
              "      <th>blockIndex</th>\n",
              "      <th>reversal</th>\n",
              "      <th>reward</th>\n",
              "      <th>monkey</th>\n",
              "      <th>date</th>\n",
              "      <th>trial_id</th>\n",
              "      <th>action</th>\n",
              "      <th>session</th>\n",
              "      <th>action_0</th>\n",
              "      <th>action_1</th>\n",
              "      <th>action_n</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>V</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>V</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>V</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>V</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>V</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15370</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>w</td>\n",
              "      <td>4</td>\n",
              "      <td>75</td>\n",
              "      <td>1</td>\n",
              "      <td>195</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15371</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>w</td>\n",
              "      <td>4</td>\n",
              "      <td>76</td>\n",
              "      <td>1</td>\n",
              "      <td>195</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15372</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>w</td>\n",
              "      <td>4</td>\n",
              "      <td>77</td>\n",
              "      <td>1</td>\n",
              "      <td>195</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15373</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>w</td>\n",
              "      <td>4</td>\n",
              "      <td>78</td>\n",
              "      <td>1</td>\n",
              "      <td>195</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15374</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>w</td>\n",
              "      <td>4</td>\n",
              "      <td>79</td>\n",
              "      <td>0</td>\n",
              "      <td>195</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15200 rows × 14 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       trialObject  trialDirection  blockType  blockIndex  reversal  reward  \\\n",
              "0                0               0          2           1         0       0   \n",
              "1                1               1          2           1         0       0   \n",
              "2                1               1          2           1         0       0   \n",
              "3                0               1          2           1         0       0   \n",
              "4                1               0          2           1         0       1   \n",
              "...            ...             ...        ...         ...       ...     ...   \n",
              "15370            1               1          1           9         1       1   \n",
              "15371            1               1          1           9         1       1   \n",
              "15372            1               1          1           9         1       1   \n",
              "15373            1               1          1           9         1       1   \n",
              "15374            0               1          1           9         1       0   \n",
              "\n",
              "      monkey  date  trial_id  action  session  action_0  action_1  action_n  \n",
              "0          V     1         0       0        1       1.0       0.0       1.0  \n",
              "1          V     1         1       1        1       0.0       1.0       1.0  \n",
              "2          V     1         2       1        1       0.0       1.0       1.0  \n",
              "3          V     1         3       1        1       0.0       1.0       0.0  \n",
              "4          V     1         4       0        1       1.0       0.0       0.0  \n",
              "...      ...   ...       ...     ...      ...       ...       ...       ...  \n",
              "15370      w     4        75       1      195       0.0       1.0       1.0  \n",
              "15371      w     4        76       1      195       0.0       1.0       1.0  \n",
              "15372      w     4        77       1      195       0.0       1.0       1.0  \n",
              "15373      w     4        78       1      195       0.0       1.0       0.0  \n",
              "15374      w     4        79       0      195       1.0       0.0      -1.0  \n",
              "\n",
              "[15200 rows x 14 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "monkey_rnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNOyrMVUpsqu",
        "outputId": "c5db5ed4-d8f8-4ff9-c929-c437d293aa38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "V: (80, 96, 2) (80, 96, 1)\n",
            "w: (80, 94, 2) (80, 94, 1)\n"
          ]
        }
      ],
      "source": [
        "# 1. 取出所有 monkey_id\n",
        "monkey_ids = monkey_rnn['monkey'].unique()  # -> array(['V','w'], dtype=object)\n",
        "\n",
        "# 2. 为每只猴子分别构造 xs, ys\n",
        "xs_by_monkey = {}\n",
        "ys_by_monkey = {}\n",
        "\n",
        "for m in monkey_ids:\n",
        "    df_m = monkey_rnn[monkey_rnn['monkey'] == m]\n",
        "    sess_ids = df_m['session'].unique()\n",
        "    xs_list = []\n",
        "    ys_list = []\n",
        "    for sid in sess_ids:\n",
        "        sub = (\n",
        "            df_m[df_m['session'] == sid]\n",
        "            .sort_values('trial_id')\n",
        "            .iloc[:80]       # 保证每 session 最多 80 trials\n",
        "        )\n",
        "        x = sub[['action', 'reward']].to_numpy().astype(float)  # (80, 2)\n",
        "        y = sub[['action_n']].to_numpy().astype(int)            # (80, 1)\n",
        "        xs_list.append(x)\n",
        "        ys_list.append(y)\n",
        "\n",
        "    # 把 list 堆成 array，然后转置成 (timesteps, n_sessions, feat)\n",
        "    arr_x = np.stack(xs_list, axis=0)  # (n_sessions_m, 80, 2)\n",
        "    arr_y = np.stack(ys_list, axis=0)  # (n_sessions_m, 80, 1)\n",
        "    xs_by_monkey[m] = arr_x.transpose(1, 0, 2)  # (80, n_sessions_m, 2)\n",
        "    ys_by_monkey[m] = arr_y.transpose(1, 0, 2)  # (80, n_sessions_m, 1)\n",
        "\n",
        "# 3. 拿到 V 和 w 两只猴子的 xs、ys\n",
        "xs_V, ys_V = xs_by_monkey['V'], ys_by_monkey['V']\n",
        "xs_w, ys_w = xs_by_monkey['w'], ys_by_monkey['w']\n",
        "\n",
        "print(\"V:\", xs_V.shape, ys_V.shape)\n",
        "print(\"w:\", xs_w.shape, ys_w.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wa3AsTYSsUHJ"
      },
      "source": [
        "### Monkey data from Ioana dataset[link text](https://)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iMloGKgmeGn",
        "outputId": "4356b083-783c-4b21-ee98-4db252224d3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1AWX5Kxy9CwD5nzxdsaoyYf-dxDI7LK0g\n",
            "To: /teamspace/studios/this_studio/src/monkey_i_data.csv\n",
            "100%|██████████| 899k/899k [00:00<00:00, 91.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 下载并读取 monkey_i_data\n",
        "file_id_i = '1AWX5Kxy9CwD5nzxdsaoyYf-dxDI7LK0g'\n",
        "url_i = f'https://drive.google.com/uc?id={file_id_i}'\n",
        "output_i = 'monkey_i_data.csv'\n",
        "gdown.download(url_i, output_i, quiet=False)\n",
        "monkey_i_data = pd.read_csv(output_i)\n",
        "\n",
        "# ✅ 复制并筛选掉 day <= 5 的 trial\n",
        "df = monkey_i_data.copy()\n",
        "df = df[df['day'] > 3].reset_index(drop=True)  # <-- 这里是关键\n",
        "\n",
        "processed = []\n",
        "global_session_id = 1\n",
        "\n",
        "for day, group in df.groupby('day'):\n",
        "    group = group.reset_index(drop=True)\n",
        "    total_trials = len(group)\n",
        "    max_trials = (total_trials // 80) * 80\n",
        "\n",
        "    if max_trials == 0:\n",
        "        continue\n",
        "\n",
        "    group = group.iloc[:max_trials].copy()\n",
        "    n_sessions = max_trials // 80\n",
        "    group['session_id'] = (group.index // 80) + global_session_id\n",
        "    group['trial_in_session'] = group.index % 80\n",
        "    global_session_id += n_sessions\n",
        "    processed.append(group)\n",
        "\n",
        "final_df = pd.concat(processed, ignore_index=True)\n",
        "monkey_i_data = final_df.copy()\n",
        "\n",
        "#Generate dataset for training, validation\n",
        "def generate_action_n(group):\n",
        "    group['action_n'] = group['task_colour'].shift(-1)  # 将 action 列向上移动一行\n",
        "    group['action_n'].iloc[-1] = -1  # 将每个组的最后一个值设为 -1\n",
        "    return group\n",
        "\n",
        "# 按 participant 分组并应用函数\n",
        "monkey_i_data = monkey_i_data.groupby(['session_id']).apply(generate_action_n)\n",
        "monkey_i_data = monkey_i_data.reset_index(drop=True)\n",
        "\n",
        "monkey_i_rnn = monkey_i_data.sort_values(by=['session_id','trial_in_session'])\n",
        "\n",
        "# 提取action和reward列并转换为numpy数组\n",
        "seq_i_rnn = monkey_i_rnn[['task_colour', 'task_reward','action_n']]\n",
        "\n",
        "# 获取所有 session 的 id（已经筛选为 trial 数 >= 80）\n",
        "session_ids = monkey_i_rnn['session_id'].unique()\n",
        "\n",
        "# 初始化空列表\n",
        "xs_list = []\n",
        "ys_list = []\n",
        "\n",
        "# 遍历每个 session\n",
        "for session_id in session_ids:\n",
        "    session_data = monkey_i_rnn[monkey_i_rnn['session_id'] == session_id]\n",
        "\n",
        "    # 只保留前 80 个 trial（确保一致）\n",
        "    session_data = session_data.iloc[:80]\n",
        "\n",
        "    x = session_data[['task_colour', 'task_reward']].to_numpy().astype(float)  # shape (80, 2)\n",
        "    y = session_data[['action_n']].to_numpy().astype(int)            # shape (80, 1)\n",
        "\n",
        "    xs_list.append(x)\n",
        "    ys_list.append(y)\n",
        "\n",
        "# 堆叠为 numpy array，并转置成 (80, 190, 2) 以及 (80, 190, 1)\n",
        "xs = np.stack(xs_list, axis=0)  # shape (80, n_sessions, 2)\n",
        "ys = np.stack(ys_list, axis=0)  # shape (80, n_sessions, 1)\n",
        "\n",
        "xs_i = np.stack(xs_list, axis=1)  # shape (80, n_sessions, 2)\n",
        "ys_i = np.stack(ys_list, axis=1)  # shape (80, n_sessions, 1)\n",
        "\n",
        "#dataset_m_train, dataset_m_test, dataset_m_validate = rat_data.format_into_datasets(xs,ys, rnn_utils.DatasetRNN, 120, 10, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Svehx1_xqdMb"
      },
      "source": [
        "### Mice data download and tidy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VYkDqS4qEl4",
        "outputId": "61675855-0969-4c5b-f157-b2b66684004c"
      },
      "outputs": [],
      "source": [
        "# # ✅ 使用新的文件 ID\n",
        "# file_id = '1yEVGt8kk1RRkA4VUl80lYolSZ_EmB-zJ'\n",
        "# download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "# # ✅ 指定下载后的文件名\n",
        "# output_file = 'downloaded_file.csv'\n",
        "# gdown.download(download_url, output_file, quiet=False)\n",
        "# mice_data = pd.read_csv(output_file)\n",
        "\n",
        "# # 操作1：每只猴子按照日期从早到晚编码\n",
        "# mice_data['date'] = pd.to_datetime(mice_data['date'])  # 自动识别格式\n",
        "# mice_data.sort_values(by=['subject', 'date', 'trial'], inplace=True)\n",
        "\n",
        "# mice_data['date'] = mice_data.groupby('subject')['date'].rank(method='dense').astype(int)\n",
        "\n",
        "# # 将 choice 中的 'L' 替换为 0，'R' 替换为 1\n",
        "# mice_data['action'] = mice_data['choice'].map({'L': 0, 'R': 1})\n",
        "\n",
        "# # 创建 session key\n",
        "# mice_data['session_key'] = mice_data[['subject', 'date']].astype(str).agg('-'.join, axis=1)\n",
        "\n",
        "# # 编号 session，从1开始\n",
        "# mice_data['session'] = mice_data['session_key'].astype('category').cat.codes + 1\n",
        "\n",
        "# # 可选：删除临时变量\n",
        "# mice_data = mice_data.drop(columns='session_key')\n",
        "\n",
        "# # Dataset Shape information\n",
        "# n_participants_mice = 10\n",
        "# #n_trials_monkey = 80\n",
        "# n_bandits_mice = 2\n",
        "# min_points_mice = 1\n",
        "# max_points_mice = 0\n",
        "\n",
        "# #Creates one hot code for actions\n",
        "# action_cols = ['action_{}'.format(i) for i in range(2)]  # this creates the following vector: 'action_0', 'action_1', 'action_2', 'action_3']\n",
        "# mice_data[action_cols] = jax.nn.one_hot(jnp.array(mice_data['action']), 2)\n",
        "\n",
        "# #Generate dataset for training, validation\n",
        "# def generate_action_n(group):\n",
        "#     group['action_n'] = group['action'].shift(-1)  # 将 action 列向上移动一行\n",
        "#     group['action_n'].iloc[-1] = -1  # 将每个组的最后一个值设为 -1\n",
        "#     return group\n",
        "\n",
        "# # 按 participant 分组并应用函数\n",
        "# mice_data = mice_data.groupby(['session']).apply(generate_action_n)\n",
        "# mice_data = mice_data.reset_index(drop=True)\n",
        "\n",
        "# mice_rnn = mice_data.sort_values(by=['session','trial'])\n",
        "\n",
        "# # 提取action和reward列并转换为numpy数组\n",
        "# seq_rnn = mice_rnn[['action', 'rewarded','action_n']]\n",
        "\n",
        "# # Step 1: 获取所有 session 的最大长度\n",
        "# max_len = mice_rnn.groupby('session').size().max()\n",
        "\n",
        "# # Step 2: 初始化容器\n",
        "# xs_padded = []\n",
        "# ys_padded = []\n",
        "# mask_padded = []\n",
        "\n",
        "# # Step 3: 遍历每个 session 并填充\n",
        "# for _, session_df in mice_rnn.groupby('session'):\n",
        "#     x = session_df[['action', 'rewarded']].values.astype(float)\n",
        "#     y = session_df[['action_n']].values.astype(int)\n",
        "\n",
        "#     pad_len = max_len - len(x)\n",
        "\n",
        "#     # Padding：action/reward = 0，action_n = -1（表示无效）\n",
        "#     x_padded = np.pad(x, ((0, pad_len), (0, 0)), constant_values=0)\n",
        "#     y_padded = np.pad(y, ((0, pad_len), (0, 0)), constant_values=-1)\n",
        "\n",
        "#     # 生成 mask（有效部分 = 1，padding = 0）\n",
        "#     mask = np.pad(np.ones(len(x)), (0, pad_len), constant_values=0)\n",
        "\n",
        "#     xs_padded.append(x_padded)\n",
        "#     ys_padded.append(y_padded)\n",
        "#     mask_padded.append(mask)\n",
        "\n",
        "# # 转为 numpy 数组\n",
        "# xs_array = np.stack(xs_padded)       # shape: (n_sessions, max_len, 2)\n",
        "# ys_array = np.stack(ys_padded)       # shape: (n_sessions, max_len, 1)\n",
        "# mask_array = np.stack(mask_padded)   # shape: (n_sessions, max_len)\n",
        "\n",
        "# dataset_r_train, dataset_r_test, dataset_r_validate = rat_data.format_into_datasets(xs_array, ys_array, rnn_utils.DatasetRNN, 160, 15, 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR82qj9txetS",
        "outputId": "403e05a1-9778-4fb7-e5f6-168b51218edf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1yEVGt8kk1RRkA4VUl80lYolSZ_EmB-zJ\n",
            "To: /teamspace/studios/this_studio/src/downloaded_file.csv\n",
            "100%|██████████| 2.18M/2.18M [00:00<00:00, 127MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ xs_array shape: (150, 548, 2)\n",
            "✅ ys_array shape: (150, 548, 1)\n",
            "✅ mask_array shape: (150, 548)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gdown\n",
        "from jax import nn as jax_nn\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# 下载并读取数据\n",
        "file_id = '1yEVGt8kk1RRkA4VUl80lYolSZ_EmB-zJ'\n",
        "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "output_file = 'downloaded_file.csv'\n",
        "gdown.download(download_url, output_file, quiet=False)\n",
        "mice_data = pd.read_csv(output_file)\n",
        "\n",
        "# 日期排序、构造 session ID\n",
        "mice_data['date'] = pd.to_datetime(mice_data['date'])\n",
        "mice_data.sort_values(by=['subject', 'date', 'trial'], inplace=True)\n",
        "mice_data['date'] = mice_data.groupby('subject')['date'].rank(method='dense').astype(int)\n",
        "mice_data['action'] = mice_data['choice'].map({'L': 0, 'R': 1})\n",
        "mice_data['session_key'] = mice_data[['subject', 'date']].astype(str).agg('-'.join, axis=1)\n",
        "mice_data['session'] = mice_data['session_key'].astype('category').cat.codes + 1\n",
        "mice_data = mice_data.drop(columns='session_key')\n",
        "\n",
        "# one-hot 编码（可选）\n",
        "action_cols = ['action_{}'.format(i) for i in range(2)]\n",
        "mice_data[action_cols] = jax_nn.one_hot(jnp.array(mice_data['action']), 2)\n",
        "\n",
        "# 添加 next action（shift）\n",
        "def generate_action_n(group):\n",
        "    group['action_n'] = group['action'].shift(-1)\n",
        "    group['action_n'].iloc[-1] = -1\n",
        "    return group\n",
        "\n",
        "mice_data = mice_data.groupby('session').apply(generate_action_n).reset_index(drop=True)\n",
        "\n",
        "# 参数设置\n",
        "max_len = 150\n",
        "\n",
        "xs_padded = []\n",
        "ys_padded = []\n",
        "mask_padded = []\n",
        "\n",
        "new_session_id = 1\n",
        "\n",
        "# 遍历每个原始 session\n",
        "for _, session_df in mice_data.groupby('session'):\n",
        "    session_df = session_df.sort_values(by='trial')\n",
        "    x = session_df[['action', 'rewarded']].values.astype(float)\n",
        "    y = session_df[['action_n']].values.astype(int)\n",
        "\n",
        "    n_total = len(x)\n",
        "    start_idx = 0\n",
        "\n",
        "    # 切成每份 150\n",
        "    while start_idx < n_total:\n",
        "        end_idx = min(start_idx + max_len, n_total)\n",
        "        x_chunk = x[start_idx:end_idx]\n",
        "        y_chunk = y[start_idx:end_idx]\n",
        "\n",
        "        pad_len = max_len - len(x_chunk)\n",
        "\n",
        "        # padding\n",
        "        x_padded = np.pad(x_chunk, ((0, pad_len), (0, 0)), constant_values=0)\n",
        "        y_padded = np.pad(y_chunk, ((0, pad_len), (0, 0)), constant_values=-1)\n",
        "        mask = np.pad(np.ones(len(x_chunk)), (0, pad_len), constant_values=0)\n",
        "\n",
        "        xs_padded.append(x_padded)\n",
        "        ys_padded.append(y_padded)\n",
        "        mask_padded.append(mask)\n",
        "\n",
        "        start_idx += max_len\n",
        "        new_session_id += 1\n",
        "\n",
        "# 转 numpy 数组\n",
        "xs_array = np.stack(xs_padded, axis = 1)       # shape: (n_subsessions, 150, 2)\n",
        "ys_array = np.stack(ys_padded, axis = 1)       # shape: (n_subsessions, 150, 1)\n",
        "mask_array = np.stack(mask_padded, axis = 1)   # shape: (n_subsessions, 150)\n",
        "\n",
        "print(\"✅ xs_array shape:\", xs_array.shape)\n",
        "print(\"✅ ys_array shape:\", ys_array.shape)\n",
        "print(\"✅ mask_array shape:\", mask_array.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "2Gfakt4ea7sm",
        "outputId": "d7234a25-fe5d-43bd-c380-a629ed4bb659"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 统计每个 session 的 trial 数量\n",
        "session_lengths = mice_data.groupby(['subject','date']).size()\n",
        "\n",
        "# 画出分布图\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(session_lengths, bins=30, kde=True)\n",
        "plt.xlabel('Number of Trials per Session')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Session Lengths')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KT7dOj3_Kfho"
      },
      "source": [
        "### Human data Suthana 3-armed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uPjJZBKiRDe",
        "outputId": "7312b148-c080-42a7-ada1-c31bbf20cce6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/teamspace/studios/this_studio/src\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "G2PVqnmjK0mw",
        "outputId": "c5725ab8-1435-4c97-9f4b-d1c701a6f5bf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ajVc9xNklThwXBD3q5s3hf2c9bIJcR-F\n",
            "To: /teamspace/studios/this_studio/src/human_data.csv\n",
            "100%|██████████| 1.45M/1.45M [00:00<00:00, 158MB/s]\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "\n",
        "# Google Drive 文件 ID（从你给的分享链接中提取）\n",
        "file_id_human = '1ajVc9xNklThwXBD3q5s3hf2c9bIJcR-F'\n",
        "# 构建直链 URL\n",
        "url_human = f'https://drive.google.com/uc?id={file_id_human}'\n",
        "# 指定本地保存文件名\n",
        "output_human = 'human_data.csv'\n",
        "# 下载文件（第一次会有进度条，后面会复用同名文件）\n",
        "gdown.download(url_human, output_human, quiet=False)\n",
        "# 用 pandas 读取\n",
        "human_su_data = pd.read_csv(output_human)\n",
        "\n",
        "# 为每个 (monkey, date, blockIndex) 内生成 trial_id，从0递增\n",
        "human_su_data['trial_id'] = human_su_data.groupby(['subject']).cumcount()\n",
        "\n",
        "# 创建 session key\n",
        "human_su_data['session_key'] = human_su_data[['subject']].astype(str).agg('-'.join, axis=1)\n",
        "# 编号 session，从1开始\n",
        "human_su_data['session'] = human_su_data['session_key'].astype('category').cat.codes + 1\n",
        "# 可选：删除临时变量\n",
        "human_su_data = human_su_data.drop(columns='session_key')\n",
        "\n",
        "#Generate dataset for training, validation\n",
        "def generate_action_n(group):\n",
        "    group['action_n'] = group['choice'].shift(-1)  # 将 action 列向上移动一行\n",
        "    group['action_n'].iloc[-1] = -1  # 将每个组的最后一个值设为 -1\n",
        "    return group\n",
        "\n",
        "# 按 participant 分组并应用函数\n",
        "human_su_data = human_su_data.groupby(['session']).apply(generate_action_n)\n",
        "human_su_data = human_su_data.reset_index(drop=True)\n",
        "\n",
        "# 确保 monkey_rnn 是按 session 和 trial_id 排好序的\n",
        "human_su_rnn = human_su_data.sort_values(by=['session', 'trial_id'])\n",
        "\n",
        "# 获取所有 session 的 id（已经筛选为 trial 数 >= 80）\n",
        "session_ids = human_su_rnn['session'].unique()\n",
        "\n",
        "# 初始化空列表\n",
        "xs_list = []\n",
        "ys_list = []\n",
        "\n",
        "# 遍历每个 session\n",
        "for session_id in session_ids:\n",
        "    session_data = human_su_rnn[human_su_rnn['session'] == session_id]\n",
        "\n",
        "    # 只保留前 80 个 trial（确保一致）\n",
        "    session_data = session_data.iloc[:160]\n",
        "\n",
        "    x = session_data[['choice', 'outcome']].to_numpy().astype(float)  # shape (80, 2)\n",
        "    y = session_data[['action_n']].to_numpy().astype(int)            # shape (80, 1)\n",
        "\n",
        "    xs_list.append(x)\n",
        "    ys_list.append(y)\n",
        "\n",
        "# 堆叠为 numpy array，并转置成 (80, 190, 2) 以及 (80, 190, 1)\n",
        "xs = np.stack(xs_list, axis=0)  # shape (80, n_sessions, 2)\n",
        "ys = np.stack(ys_list, axis=0)  # shape (80, n_sessions, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdFw99jpK4NE",
        "outputId": "c8d92ada-a1e7-417f-b992-a5cb25b23375"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(808, 160, 2)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1uX3_36hyiW"
      },
      "source": [
        "### Human data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZk19kg5Bb6i",
        "outputId": "2e64c9eb-df0e-40ba-e96b-e245a221c304"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1N_zAy-qrbfjvF8Kbb504IH2JNhR5KI-P\n",
            "To: /teamspace/studios/this_studio/src/downloaded_file.csv\n",
            "100%|██████████| 2.57M/2.57M [00:00<00:00, 205MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of participants: 305\n",
            "Number of trials per participant: 156\n",
            "Number of bandits (available actions): 2\n",
            "Minimum number of points in this game: 1. Maximum number: 0\n"
          ]
        }
      ],
      "source": [
        "# downlowd link for Google Drive\n",
        "file_id = '1N_zAy-qrbfjvF8Kbb504IH2JNhR5KI-P'\n",
        "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "# downlowd dataset as 'downloaded_file.csv'\n",
        "output_file = 'downloaded_file.csv'\n",
        "gdown.download(download_url, output_file, quiet=False)\n",
        "\n",
        "# read data\n",
        "eck_data = pd.read_csv(output_file)\n",
        "\n",
        "selected_columns = ['sID', 'TrialID', 'selected_box', 'reward']\n",
        "\n",
        "eck_sorted = eck_data[selected_columns]\n",
        "eck_sorted['participant'] = eck_sorted.groupby(['sID']).ngroup() + 1\n",
        "eck_sorted['action'] = eck_sorted['selected_box']\n",
        "\n",
        "max_trial = eck_sorted.groupby('participant')['TrialID'].transform('max')\n",
        "eck_sorted['max_trial'] = max_trial\n",
        "eck_sorted = eck_sorted[eck_sorted['max_trial'] >= 120]\n",
        "\n",
        "action_cols = ['action_{}'.format(i) for i in range(2)]  # this creates the following vector: 'action_0', 'action_1', 'action_2', 'action_3']\n",
        "eck_sorted[action_cols] = jax.nn.one_hot(jnp.array(eck_sorted['action']), 2)\n",
        "eck_sorted\n",
        "\n",
        "# Number of participants (we are giving you the solution here already)\n",
        "n_participants = len(np.unique(eck_sorted['participant']))\n",
        "\n",
        "# Number of trials\n",
        "# ============== FILL IN THE BLANKS BELOW ================\n",
        "n_trials = len(np.unique(eck_sorted['TrialID']))\n",
        "\n",
        "# Number of bandits\n",
        "# ============== FILL IN THE BLANKS BELOW ================\n",
        "n_bandits = len(np.unique(eck_sorted['action']))\n",
        "\n",
        "# Min and max points\n",
        "# ============== FILL IN THE BLANKS BELOW ================\n",
        "min_points = 1#np.min(valetin['reward'])\n",
        "max_points = 0#np.max(valetin['reward'])\n",
        "\n",
        "# Print the results\n",
        "print(\"Number of participants: {}\".format(n_participants))\n",
        "print(\"Number of trials per participant: {}\".format(n_trials))\n",
        "print(\"Number of bandits (available actions): {}\".format(n_bandits))\n",
        "print(\"Minimum number of points in this game: {}. Maximum number: {}\".format(min_points, max_points))\n",
        "\n",
        "def generate_action_n(group):\n",
        "    group['action_n'] = group['action'].shift(-1)  # 将 action 列向上移动一行\n",
        "    group['action_n'].iloc[-1] = -1  # 将每个组的最后一个值设为 -1\n",
        "    return group\n",
        "\n",
        "eck_sorted = eck_sorted[eck_sorted['TrialID'] <= 120]\n",
        "\n",
        "# 重置索引（可选）\n",
        "eck_sorted = eck_sorted.reset_index(drop=True)\n",
        "\n",
        "# 按 participant 分组并应用函数\n",
        "eck_sorted = eck_sorted.groupby('participant').apply(generate_action_n)\n",
        "\n",
        "# 如果需要的话，可以重置索引\n",
        "eck_sorted = eck_sorted.reset_index(drop=True)\n",
        "\n",
        "eck_rnn = eck_sorted.sort_values(by=['TrialID', 'participant'])\n",
        "\n",
        "# 提取action和reward列并转换为numpy数组\n",
        "seq_rnn = eck_rnn[['action', 'reward','action_n']]\n",
        "\n",
        "# 获取数据形状\n",
        "n_sessions = 305\n",
        "n_trials = 120\n",
        "\n",
        "# 重塑数据为 (n_sessions, n_trials, 2)\n",
        "\n",
        "xs = seq_rnn[['action','reward']].values.reshape(n_trials, n_sessions, 2).astype(float)\n",
        "\n",
        "# 初始化ys并填充为-1\n",
        "ys = seq_rnn[['action_n']].values.reshape(n_trials, n_sessions, 1).astype(int)\n",
        "\n",
        "dataset_h1_train, dataset_h1_test, dataset_h1_validate = rat_data.format_into_datasets(xs,ys, rnn_utils.DatasetRNN,213, 46, 46)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functions for computing log likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "otkhuOQyx7hB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "#@title Compute log-likelihood\n",
        "def compute_log_likelihood(dataset, model_fun, params):\n",
        "    xs, actual_choices = next(dataset)\n",
        "    n_trials_per_session, n_sessions = actual_choices.shape[:2]\n",
        "    model_outputs, model_states = rnn_utils.eval_model(model_fun, params, xs)\n",
        "\n",
        "    predicted_log_choice_probabilities = np.array(jax.nn.log_softmax(model_outputs[:, :, :2]))\n",
        "\n",
        "    log_likelihoods = []\n",
        "\n",
        "    for sess_i in range(n_sessions):\n",
        "        log_likelihood = 0\n",
        "        n = 0  # Total number of trials for this session.\n",
        "        for trial_i in range(n_trials_per_session):\n",
        "            actual_choice = int(actual_choices[trial_i, sess_i])\n",
        "            if actual_choice >= 0:  # values < 0 are invalid trials which we ignore.\n",
        "                log_likelihood += predicted_log_choice_probabilities[trial_i, sess_i, actual_choice]\n",
        "                n += 1\n",
        "\n",
        "        # Calculate normalized likelihood for this session\n",
        "        if n > 0:\n",
        "            normalized_likelihood = np.exp(log_likelihood / n)\n",
        "            log_likelihoods.append(normalized_likelihood)\n",
        "\n",
        "    # Compute mean and standard deviation across all sessions\n",
        "    mean_likelihood = np.mean(log_likelihoods)\n",
        "    std_likelihood = np.std(log_likelihoods)\n",
        "\n",
        "    print(f'Average Normalized Likelihood: {100 * mean_likelihood:.1f}%')\n",
        "    #print(f'Standard Deviation of Likelihood: {100 * std_likelihood:.1f}%')\n",
        "\n",
        "    return mean_likelihood, std_likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "QsV12JBY2c6D"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# @title Compute average negative log-likelihood (NLL)\n",
        "def compute_negative_log_likelihood(dataset, model_fun, params):\n",
        "    xs, actual_choices = next(dataset)\n",
        "    n_trials_per_session, n_sessions = actual_choices.shape[:2]\n",
        "    model_outputs, model_states = rnn_utils.eval_model(model_fun, params, xs)\n",
        "\n",
        "    # Get log-softmax of the first two outputs (assumed to be choice probabilities)\n",
        "    predicted_log_choice_probabilities = np.array(jax.nn.log_softmax(model_outputs[:, :, :2]))\n",
        "\n",
        "    total_log_likelihood = 0.0\n",
        "    total_valid_trials = 0\n",
        "\n",
        "    for sess_i in range(n_sessions):\n",
        "        for trial_i in range(n_trials_per_session):\n",
        "            actual_choice = int(actual_choices[trial_i, sess_i])\n",
        "            if actual_choice >= 0:  # Skip invalid trials\n",
        "                total_log_likelihood += predicted_log_choice_probabilities[trial_i, sess_i, actual_choice]\n",
        "                total_valid_trials += 1\n",
        "\n",
        "    if total_valid_trials == 0:\n",
        "        raise ValueError(\"No valid trials found.\")\n",
        "\n",
        "    # Average negative log-likelihood (NLL)\n",
        "    avg_nll = -total_log_likelihood / total_valid_trials\n",
        "\n",
        "    print(f'Average Negative Log-Likelihood: {avg_nll:.4f}')\n",
        "\n",
        "    return avg_nll\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Functions for organizing monkey data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "md_dyBtsmmKC"
      },
      "outputs": [],
      "source": [
        "def format_into_datasets_multi_monkey(\n",
        "    xs_list: list[np.ndarray],\n",
        "    ys_list: list[np.ndarray],\n",
        "    dataset_constructor,\n",
        "    n_train_sessions: int,\n",
        "    n_test_sessions: int,\n",
        "    n_validate_sessions: int,\n",
        "    batch_size: int = None,\n",
        "    random_seed: int = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    把多只猴子的 (xs, ys) 按 session 数比例，划分为全局 train/test/validate。\n",
        "\n",
        "    Args:\n",
        "      xs_list, ys_list: 每只猴子的数组，shape=(timesteps, n_sessions_i, feat)\n",
        "      dataset_constructor: DatasetRNN 或其他同接口构造器\n",
        "      n_train_sessions:    全体一共要选多少 session 做训练\n",
        "      n_test_sessions:     全体要选多少 session 做测试\n",
        "      n_validate_sessions: 全体要选多少 session 做验证\n",
        "      batch_size:          传给 DatasetRNN 的 batch_size\n",
        "      random_seed:         np.random.RandomState 用于可复现\n",
        "\n",
        "    Returns:\n",
        "      ds_train, ds_test, ds_val: 三个 DatasetRNN，里面包含来自三只猴子的 session\n",
        "    \"\"\"\n",
        "    # 1. 计算每只猴子 session 数\n",
        "    n_monkeys = len(xs_list)\n",
        "    sess_counts = np.array([xs.shape[1] for xs in xs_list])       # [s1, s2, s3]\n",
        "    total_sess  = sess_counts.sum()\n",
        "\n",
        "    # 2. 按比例分配到每只猴子的各集合\n",
        "    def _proportional_alloc(total, counts):\n",
        "        # 先浮点分配\n",
        "        floats = counts / counts.sum() * total\n",
        "        floors = np.floor(floats).astype(int)\n",
        "        rem = total - floors.sum()\n",
        "        # 按余数大小补齐\n",
        "        remainders = floats - floors\n",
        "        for idx in np.argsort(remainders)[-rem:]:\n",
        "            floors[idx] += 1\n",
        "        return floors\n",
        "\n",
        "    n_train_per_monkey = _proportional_alloc(n_train_sessions, sess_counts)\n",
        "    n_test_per_monkey  = _proportional_alloc(n_test_sessions,  sess_counts)\n",
        "    n_val_per_monkey   = _proportional_alloc(n_validate_sessions, sess_counts)\n",
        "\n",
        "    if random_seed is not None:\n",
        "        rng = np.random.RandomState(random_seed)\n",
        "    else:\n",
        "        rng = np.random\n",
        "\n",
        "    # 3. 对每只猴子分别抽 idx\n",
        "    train_slices, test_slices, val_slices = [], [], []\n",
        "    for i in range(n_monkeys):\n",
        "        n_sess = sess_counts[i]\n",
        "        all_idx = np.arange(n_sess)\n",
        "        rng.shuffle(all_idx)\n",
        "        start = 0\n",
        "        train_idx = all_idx[start:start + n_train_per_monkey[i]]\n",
        "        start += n_train_per_monkey[i]\n",
        "        test_idx  = all_idx[start:start + n_test_per_monkey[i]]\n",
        "        start += n_test_per_monkey[i]\n",
        "        val_idx   = all_idx[start:start + n_val_per_monkey[i]]\n",
        "        train_slices.append(train_idx)\n",
        "        test_slices.append(test_idx)\n",
        "        val_slices.append(val_idx)\n",
        "\n",
        "    # 4. 汇总三只猴子的训练 / 测试 / 验证数据\n",
        "    def _gather(xs_list, ys_list, idx_slices):\n",
        "        # 把每只猴子的选中 sessions 按时间步拼到一起\n",
        "        parts_x, parts_y = [], []\n",
        "        for xs, ys, idx in zip(xs_list, ys_list, idx_slices):\n",
        "            # xs: (timesteps, n_sess_i, feat)\n",
        "            parts_x.append(xs[:, idx, :])\n",
        "            parts_y.append(ys[:, idx, :])\n",
        "        # concat 的结果 shape=(timesteps, sum(n_i), feat)\n",
        "        return np.concatenate(parts_x, axis=1), np.concatenate(parts_y, axis=1)\n",
        "\n",
        "    xs_train, ys_train = _gather(xs_list, ys_list, train_slices)\n",
        "    xs_test,  ys_test  = _gather(xs_list, ys_list, test_slices)\n",
        "    xs_val,   ys_val   = _gather(xs_list, ys_list, val_slices)\n",
        "\n",
        "    # 5. 构造 DatasetRNN\n",
        "    ds_train = dataset_constructor(xs_train, ys_train, batch_size=batch_size)\n",
        "    ds_test  = dataset_constructor(xs_test,  ys_test,  batch_size=batch_size)\n",
        "    ds_val   = dataset_constructor(xs_val,   ys_val,   batch_size=batch_size)\n",
        "\n",
        "    return ds_train, ds_test, ds_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WZzcZE-7TLI2"
      },
      "outputs": [],
      "source": [
        "def format_into_datasets_10_multi(\n",
        "    xs_list: list[np.ndarray],\n",
        "    ys_list: list[np.ndarray],\n",
        "    dataset_constructor,\n",
        "    batch_size: int = None,\n",
        "    random_seed: int = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    对多只猴子独立做 10‐fold CV，每折：\n",
        "      - test = 每只猴子第 i 份 fold\n",
        "      - validation 抽取与 test 大小一致的 session\n",
        "      - 剩余做 train\n",
        "    Args:\n",
        "      xs_list: list of np.ndarray, 每项形状 (timesteps, n_sess_i, feat)\n",
        "      ys_list: list of np.ndarray, 每项形状 (timesteps, n_sess_i, ...)\n",
        "      dataset_constructor: 接受 (xs_sub, ys_sub, batch_size) 的构造器\n",
        "      batch_size: mini‐batch 大小；None 则每 dataset 用全部 episodes\n",
        "      random_seed: int，可复现打乱\n",
        "    Returns:\n",
        "      List of 10 tuples (ds_train, ds_val, ds_test)\n",
        "    \"\"\"\n",
        "    rng = np.random.RandomState(random_seed) if random_seed is not None else np.random\n",
        "\n",
        "    n_monkeys = len(xs_list)\n",
        "    # 1. 对每只猴子生成打乱后的 session 索引，并切成 10 份\n",
        "    folds_per_monkey = []\n",
        "    for xs in xs_list:\n",
        "        n_sess = xs.shape[1]\n",
        "        idx = rng.permutation(n_sess)\n",
        "        folds = np.array_split(idx, 10)\n",
        "        folds_per_monkey.append(folds)\n",
        "\n",
        "    folds_datasets = []\n",
        "    for i in range(10):\n",
        "        # 2a. 测试集：三猴子各自的第 i 份\n",
        "        test_parts = [folds_per_monkey[m][i] for m in range(n_monkeys)]\n",
        "\n",
        "        # 2b. 剩余 sessions 用于 train+val\n",
        "        rem_parts = [\n",
        "            np.hstack([folds_per_monkey[m][j] for j in range(10) if j != i])\n",
        "            for m in range(n_monkeys)\n",
        "        ]\n",
        "\n",
        "        # 2c. 验证集大小与测试集一致：每只猴子抽取与 test_parts[m] 相同数量\n",
        "        val_parts = []\n",
        "        train_parts = []\n",
        "        for m in range(n_monkeys):\n",
        "            rng.shuffle(rem_parts[m])\n",
        "            n_test = len(test_parts[m])\n",
        "            val = rem_parts[m][:n_test]\n",
        "            train = rem_parts[m][n_test:]\n",
        "            val_parts.append(val)\n",
        "            train_parts.append(train)\n",
        "\n",
        "        # 3. 拼接各猴子的 train/val/test xs, ys\n",
        "        def _gather(parts_idx):\n",
        "            xs_sub, ys_sub = [], []\n",
        "            for m, idxs in enumerate(parts_idx):\n",
        "                xs_sub.append(xs_list[m][:, idxs, :])\n",
        "                ys_sub.append(ys_list[m][:, idxs, :])\n",
        "            return np.concatenate(xs_sub, axis=1), np.concatenate(ys_sub, axis=1)\n",
        "\n",
        "        xs_train, ys_train = _gather(train_parts)\n",
        "        xs_val,   ys_val   = _gather(val_parts)\n",
        "        xs_test,  ys_test  = _gather(test_parts)\n",
        "\n",
        "        # 4. 构造 DatasetRNN\n",
        "        ds_train = dataset_constructor(xs_train, ys_train, batch_size=batch_size)\n",
        "        ds_val   = dataset_constructor(xs_val,   ys_val,   batch_size=batch_size)\n",
        "        ds_test  = dataset_constructor(xs_test,  ys_test,  batch_size=batch_size)\n",
        "\n",
        "        folds_datasets.append((ds_train, ds_val, ds_test))\n",
        "\n",
        "    return folds_datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "oCzihu0NJO6R",
        "outputId": "75653c29-30e8-4502-8d28-a428187746e3"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# import pickle\n",
        "\n",
        "# # 挂载 Google Drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8sai1OhWYy3"
      },
      "source": [
        "# Fitting different models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Organize monkey data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og6SOWVy0ipb",
        "outputId": "d9f7cfa5-8412-43ba-dc2b-96ce83c98b47"
      },
      "outputs": [],
      "source": [
        "# 这是为了把上面猴子的数据给合并，然后调用我的这个format_into_datasets，生成三个数据集\n",
        "# 假设你已经有三个 np.ndarray：xs1,ys1；xs2,ys2；xs3,ys3\n",
        "xs_list = [xs_V, xs_w, xs_i]\n",
        "ys_list = [ys_V, ys_w, ys_i]\n",
        "\n",
        "folds_datasets = format_into_datasets_10_multi(\n",
        "    xs_list, ys_list,\n",
        "    dataset_constructor=rnn_utils.DatasetRNN,\n",
        "    batch_size=64,\n",
        "    random_seed=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7vND8bv6rEw",
        "outputId": "960f810b-baec-460d-a603-05692c57f695"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 0: train episodes=320, val episodes=64, test episodes=64\n",
            "Fold 1: train episodes=320, val episodes=64, test episodes=64\n",
            "Fold 2: train episodes=320, val episodes=64, test episodes=64\n",
            "Fold 3: train episodes=320, val episodes=64, test episodes=64\n",
            "Fold 4: train episodes=320, val episodes=64, test episodes=64\n",
            "Fold 5: train episodes=320, val episodes=64, test episodes=64\n",
            "Fold 6: train episodes=320, val episodes=64, test episodes=64\n",
            "Fold 7: train episodes=320, val episodes=64, test episodes=64\n",
            "Fold 8: train episodes=320, val episodes=64, test episodes=64\n",
            "Fold 9: train episodes=320, val episodes=64, test episodes=64\n"
          ]
        }
      ],
      "source": [
        "# folds = rat_data.format_into_datasets_10(\n",
        "#     xs_m, ys_m,\n",
        "#     dataset_constructor=rnn_utils.DatasetRNN,\n",
        "#     n_validate_sessions=36,\n",
        "#     random_seed=123\n",
        "# )\n",
        "\n",
        "for i, (train_ds, val_ds, test_ds) in enumerate(folds_datasets):\n",
        "    print(f\"Fold {i}: \"\n",
        "          f\"train episodes={train_ds._dataset_size}, \"\n",
        "          f\"val episodes={val_ds._dataset_size}, \"\n",
        "          f\"test episodes={test_ds._dataset_size}\")\n",
        "    # 在这里用 train_ds/val_ds 选超参，最后在 test_ds 上跑一次评估\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOGiNjm4FUAx"
      },
      "source": [
        "## Fit RL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcbHWEd8FWAu",
        "outputId": "17535248-3716-4e6c-c159-3ca7b51f5163"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'dataset_m_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 这个就是最简单的Rescolar模型的运行代码，你主要移植的就是这个AgentQ\u001b[39;00m\n\u001b[1;32m      2\u001b[0m rl_params, _ \u001b[38;5;241m=\u001b[39m rnn_utils\u001b[38;5;241m.\u001b[39mfit_model(\n\u001b[1;32m      3\u001b[0m     model_fun\u001b[38;5;241m=\u001b[39mbandits\u001b[38;5;241m.\u001b[39mHkAgentQ,\n\u001b[0;32m----> 4\u001b[0m     dataset_train \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_m_train\u001b[49m,\n\u001b[1;32m      5\u001b[0m     dataset_test \u001b[38;5;241m=\u001b[39m dataset_m_validate,\n\u001b[1;32m      6\u001b[0m     loss_fun\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m      8\u001b[0m         optax\u001b[38;5;241m.\u001b[39madd_decayed_weights(\u001b[38;5;241m1e-5\u001b[39m),  \u001b[38;5;66;03m# L2 正则化\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         optax\u001b[38;5;241m.\u001b[39madam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)  \u001b[38;5;66;03m# Adam 优化器\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     ),\n\u001b[1;32m     11\u001b[0m     n_steps_per_call\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m     12\u001b[0m     n_steps_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m#return_all_losses=True,\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     early_stop_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset_m_train' is not defined"
          ]
        }
      ],
      "source": [
        "# 这个就是最简单的Rescolar模型的运行代码，你主要移植的就是这个AgentQ\n",
        "rl_params, _ = rnn_utils.fit_model(\n",
        "    model_fun=bandits.HkAgentQ,\n",
        "    dataset_train = dataset_m_train,\n",
        "    dataset_test = dataset_m_validate,\n",
        "    loss_fun='categorical',\n",
        "    optimizer= optax.chain(\n",
        "        optax.add_decayed_weights(1e-5),  # L2 正则化\n",
        "        optax.adam(learning_rate=1e-4)  # Adam 优化器\n",
        "    ),\n",
        "    n_steps_per_call=500,\n",
        "    n_steps_max=100000,\n",
        "    #return_all_losses=True,\n",
        "    early_stop_step=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E48EXN_SvESG",
        "outputId": "7938a016-e2e9-4680-c0c6-10b4ac13568a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Normalized Likelihood: 73.5%\n",
            "Average Negative Log-Likelihood: 0.3849\n"
          ]
        }
      ],
      "source": [
        "mean_likelihood, std_likelihood = compute_log_likelihood(dataset_m_test, bandits.HkAgentQ, rl_params)\n",
        "mean_nll = compute_negative_log_likelihood(dataset_m_test, bandits.HkAgentQ, rl_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIOE3Q48xKY5",
        "outputId": "f3078e28-7d61-48ad-88ee-1f0690920f1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 200 of 200; Loss: 4.7465e+02; Test Loss: 4.7362e+02. (Time: 2.8s)Step 200 of 100000; Loss: 3.9309e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.6175e+02; Test Loss: 4.6769e+02. (Time: 2.3s)Step 400 of 100000; Loss: 3.8751e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.9391e+02; Test Loss: 4.6197e+02. (Time: 2.3s)Step 600 of 100000; Loss: 3.8214e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4997e+02; Test Loss: 4.5646e+02. (Time: 2.5s)Step 800 of 100000; Loss: 3.7696e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1765e+02; Test Loss: 4.5113e+02. (Time: 3.5s)Step 1000 of 100000; Loss: 3.7197e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2467e+02; Test Loss: 4.4598e+02. (Time: 2.3s)Step 1200 of 100000; Loss: 3.6714e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3369e+02; Test Loss: 4.4101e+02. (Time: 2.5s)Step 1400 of 100000; Loss: 3.6248e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2351e+02; Test Loss: 4.3620e+02. (Time: 2.4s)Step 1600 of 100000; Loss: 3.5798e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.8525e+02; Test Loss: 4.3154e+02. (Time: 2.4s)Step 1800 of 100000; Loss: 3.5362e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1218e+02; Test Loss: 4.2703e+02. (Time: 3.8s)Step 2000 of 100000; Loss: 3.4940e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1483e+02; Test Loss: 4.2267e+02. (Time: 2.3s)Step 2200 of 100000; Loss: 3.4532e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.2256e+02; Test Loss: 4.1845e+02. (Time: 2.9s)Step 2400 of 100000; Loss: 3.4137e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4864e+02; Test Loss: 4.1436e+02. (Time: 3.1s)Step 2600 of 100000; Loss: 3.3754e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.0523e+02; Test Loss: 4.1040e+02. (Time: 2.9s)Step 2800 of 100000; Loss: 3.3384e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.2898e+02; Test Loss: 4.0657e+02. (Time: 3.1s)Step 3000 of 100000; Loss: 3.3026e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.8134e+02; Test Loss: 4.0286e+02. (Time: 2.3s)Step 3200 of 100000; Loss: 3.2678e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.0275e+02; Test Loss: 3.9926e+02. (Time: 2.4s)Step 3400 of 100000; Loss: 3.2342e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.0177e+02; Test Loss: 3.9578e+02. (Time: 2.3s)Step 3600 of 100000; Loss: 3.2015e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1833e+02; Test Loss: 3.9241e+02. (Time: 2.5s)Step 3800 of 100000; Loss: 3.1699e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9959e+02; Test Loss: 3.8914e+02. (Time: 3.5s)Step 4000 of 100000; Loss: 3.1393e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5839e+02; Test Loss: 3.8599e+02. (Time: 2.3s)Step 4200 of 100000; Loss: 3.1096e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4585e+02; Test Loss: 3.8294e+02. (Time: 2.3s)Step 4400 of 100000; Loss: 3.0809e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7058e+02; Test Loss: 3.7998e+02. (Time: 2.3s)Step 4600 of 100000; Loss: 3.0531e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.6017e+02; Test Loss: 3.7713e+02. (Time: 2.3s)Step 4800 of 100000; Loss: 3.0263e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.2255e+02; Test Loss: 3.7437e+02. (Time: 3.8s)Step 5000 of 100000; Loss: 3.0002e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6906e+02; Test Loss: 3.7171e+02. (Time: 2.3s)Step 5200 of 100000; Loss: 2.9750e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6602e+02; Test Loss: 3.6914e+02. (Time: 2.3s)Step 5400 of 100000; Loss: 2.9507e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.8874e+02; Test Loss: 3.6666e+02. (Time: 2.6s)Step 5600 of 100000; Loss: 2.9271e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9311e+02; Test Loss: 3.6426e+02. (Time: 2.3s)Step 5800 of 100000; Loss: 2.9044e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5576e+02; Test Loss: 3.6196e+02. (Time: 3.9s)Step 6000 of 100000; Loss: 2.8825e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.8887e+02; Test Loss: 3.5974e+02. (Time: 2.4s)Step 6200 of 100000; Loss: 2.8613e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.3354e+02; Test Loss: 3.5761e+02. (Time: 2.3s)Step 6400 of 100000; Loss: 2.8408e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6067e+02; Test Loss: 3.5555e+02. (Time: 2.3s)Step 6600 of 100000; Loss: 2.8210e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.7069e+02; Test Loss: 3.5357e+02. (Time: 2.3s)Step 6800 of 100000; Loss: 2.8020e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.7354e+02; Test Loss: 3.5167e+02. (Time: 3.4s)Step 7000 of 100000; Loss: 2.7836e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.7552e+02; Test Loss: 3.4984e+02. (Time: 2.7s)Step 7200 of 100000; Loss: 2.7659e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2551e+02; Test Loss: 3.4810e+02. (Time: 2.4s)Step 7400 of 100000; Loss: 2.7489e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9770e+02; Test Loss: 3.4642e+02. (Time: 2.4s)Step 7600 of 100000; Loss: 2.7325e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3274e+02; Test Loss: 3.4482e+02. (Time: 2.3s)Step 7800 of 100000; Loss: 2.7167e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.2581e+02; Test Loss: 3.4328e+02. (Time: 3.2s)Step 8000 of 100000; Loss: 2.7016e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.8673e+02; Test Loss: 3.4182e+02. (Time: 3.1s)Step 8200 of 100000; Loss: 2.6871e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.4688e+02; Test Loss: 3.4041e+02. (Time: 2.4s)Step 8400 of 100000; Loss: 2.6731e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3789e+02; Test Loss: 3.3908e+02. (Time: 2.4s)Step 8600 of 100000; Loss: 2.6598e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.7813e+02; Test Loss: 3.3780e+02. (Time: 2.4s)Step 8800 of 100000; Loss: 2.6469e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5967e+02; Test Loss: 3.3659e+02. (Time: 3.0s)Step 9000 of 100000; Loss: 2.6346e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2747e+02; Test Loss: 3.3543e+02. (Time: 3.1s)Step 9200 of 100000; Loss: 2.6229e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6889e+02; Test Loss: 3.3434e+02. (Time: 2.4s)Step 9400 of 100000; Loss: 2.6117e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.0851e+02; Test Loss: 3.3330e+02. (Time: 2.3s)Step 9600 of 100000; Loss: 2.6010e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3846e+02; Test Loss: 3.3231e+02. (Time: 2.3s)Step 9800 of 100000; Loss: 2.5908e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5909e+02; Test Loss: 3.3137e+02. (Time: 2.5s)Step 10000 of 100000; Loss: 2.5810e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.4926e+02; Test Loss: 3.3049e+02. (Time: 3.4s)Step 10200 of 100000; Loss: 2.5717e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6783e+02; Test Loss: 3.2965e+02. (Time: 2.3s)Step 10400 of 100000; Loss: 2.5628e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0983e+02; Test Loss: 3.2886e+02. (Time: 2.3s)Step 10600 of 100000; Loss: 2.5544e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.7142e+02; Test Loss: 3.2811e+02. (Time: 2.3s)Step 10800 of 100000; Loss: 2.5464e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1160e+02; Test Loss: 3.2741e+02. (Time: 2.6s)Step 11000 of 100000; Loss: 2.5388e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1088e+02; Test Loss: 3.2675e+02. (Time: 3.7s)Step 11200 of 100000; Loss: 2.5316e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6947e+02; Test Loss: 3.2613e+02. (Time: 2.3s)Step 11400 of 100000; Loss: 2.5248e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3827e+02; Test Loss: 3.2554e+02. (Time: 2.3s)Step 11600 of 100000; Loss: 2.5183e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2440e+02; Test Loss: 3.2499e+02. (Time: 2.3s)Step 11800 of 100000; Loss: 2.5122e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.8127e+02; Test Loss: 3.2447e+02. (Time: 2.3s)Step 12000 of 100000; Loss: 2.5064e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.4269e+02; Test Loss: 3.2398e+02. (Time: 3.6s)Step 12200 of 100000; Loss: 2.5009e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1383e+02; Test Loss: 3.2353e+02. (Time: 2.3s)Step 12400 of 100000; Loss: 2.4958e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6170e+02; Test Loss: 3.2309e+02. (Time: 2.3s)Step 12600 of 100000; Loss: 2.4909e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9838e+02; Test Loss: 3.2269e+02. (Time: 2.3s)Step 12800 of 100000; Loss: 2.4863e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2913e+02; Test Loss: 3.2230e+02. (Time: 2.2s)Step 13000 of 100000; Loss: 2.4819e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5816e+02; Test Loss: 3.2194e+02. (Time: 3.1s)Step 13200 of 100000; Loss: 2.4778e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3870e+02; Test Loss: 3.2159e+02. (Time: 2.8s)Step 13400 of 100000; Loss: 2.4739e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6745e+02; Test Loss: 3.2126e+02. (Time: 2.3s)Step 13600 of 100000; Loss: 2.4701e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0372e+02; Test Loss: 3.2095e+02. (Time: 2.3s)Step 13800 of 100000; Loss: 2.4666e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6012e+02; Test Loss: 3.2066e+02. (Time: 2.3s)Step 14000 of 100000; Loss: 2.4633e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0164e+02; Test Loss: 3.2038e+02. (Time: 3.2s)Step 14200 of 100000; Loss: 2.4602e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.0593e+02; Test Loss: 3.2011e+02. (Time: 3.1s)Step 14400 of 100000; Loss: 2.4572e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6313e+02; Test Loss: 3.1985e+02. (Time: 2.3s)Step 14600 of 100000; Loss: 2.4544e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3605e+02; Test Loss: 3.1960e+02. (Time: 2.3s)Step 14800 of 100000; Loss: 2.4516e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1992e+02; Test Loss: 3.1936e+02. (Time: 2.3s)Step 15000 of 100000; Loss: 2.4491e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.8574e+02; Test Loss: 3.1913e+02. (Time: 3.3s)Step 15200 of 100000; Loss: 2.4466e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3611e+02; Test Loss: 3.1891e+02. (Time: 4.6s)Step 15400 of 100000; Loss: 2.4442e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0841e+02; Test Loss: 3.1869e+02. (Time: 2.3s)Step 15600 of 100000; Loss: 2.4420e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5995e+02; Test Loss: 3.1849e+02. (Time: 2.3s)Step 15800 of 100000; Loss: 2.4398e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9487e+02; Test Loss: 3.1828e+02. (Time: 2.3s)Step 16000 of 100000; Loss: 2.4377e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2558e+02; Test Loss: 3.1809e+02. (Time: 2.3s)Step 16200 of 100000; Loss: 2.4357e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5883e+02; Test Loss: 3.1789e+02. (Time: 3.7s)Step 16400 of 100000; Loss: 2.4338e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3499e+02; Test Loss: 3.1771e+02. (Time: 2.3s)Step 16600 of 100000; Loss: 2.4319e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6664e+02; Test Loss: 3.1752e+02. (Time: 2.2s)Step 16800 of 100000; Loss: 2.4301e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0047e+02; Test Loss: 3.1735e+02. (Time: 2.3s)Step 17000 of 100000; Loss: 2.4284e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5617e+02; Test Loss: 3.1718e+02. (Time: 2.3s)Step 17200 of 100000; Loss: 2.4267e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9744e+02; Test Loss: 3.1701e+02. (Time: 3.6s)Step 17400 of 100000; Loss: 2.4251e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.0340e+02; Test Loss: 3.1685e+02. (Time: 2.5s)Step 17600 of 100000; Loss: 2.4236e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6083e+02; Test Loss: 3.1669e+02. (Time: 2.7s)Step 17800 of 100000; Loss: 2.4221e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3515e+02; Test Loss: 3.1654e+02. (Time: 2.3s)Step 18000 of 100000; Loss: 2.4206e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1893e+02; Test Loss: 3.1639e+02. (Time: 2.3s)Step 18200 of 100000; Loss: 2.4192e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.8682e+02; Test Loss: 3.1624e+02. (Time: 3.5s)Step 18400 of 100000; Loss: 2.4179e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3363e+02; Test Loss: 3.1610e+02. (Time: 2.6s)Step 18600 of 100000; Loss: 2.4166e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0592e+02; Test Loss: 3.1597e+02. (Time: 2.3s)Step 18800 of 100000; Loss: 2.4153e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5943e+02; Test Loss: 3.1584e+02. (Time: 2.3s)Step 19000 of 100000; Loss: 2.4141e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9316e+02; Test Loss: 3.1570e+02. (Time: 2.3s)Step 19200 of 100000; Loss: 2.4129e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2371e+02; Test Loss: 3.1558e+02. (Time: 3.1s)Step 19400 of 100000; Loss: 2.4118e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5890e+02; Test Loss: 3.1545e+02. (Time: 2.9s)Step 19600 of 100000; Loss: 2.4107e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3353e+02; Test Loss: 3.1533e+02. (Time: 2.3s)Step 19800 of 100000; Loss: 2.4096e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6508e+02; Test Loss: 3.1521e+02. (Time: 2.3s)Step 20000 of 100000; Loss: 2.4085e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9803e+02; Test Loss: 3.1510e+02. (Time: 2.3s)Step 20200 of 100000; Loss: 2.4075e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5463e+02; Test Loss: 3.1499e+02. (Time: 2.6s)Step 20400 of 100000; Loss: 2.4066e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9541e+02; Test Loss: 3.1488e+02. (Time: 3.5s)Step 20600 of 100000; Loss: 2.4056e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.0162e+02; Test Loss: 3.1478e+02. (Time: 2.3s)Step 20800 of 100000; Loss: 2.4047e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5983e+02; Test Loss: 3.1468e+02. (Time: 2.3s)Step 21000 of 100000; Loss: 2.4039e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3468e+02; Test Loss: 3.1458e+02. (Time: 2.3s)Step 21200 of 100000; Loss: 2.4030e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1893e+02; Test Loss: 3.1448e+02. (Time: 2.3s)Step 21400 of 100000; Loss: 2.4022e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.8683e+02; Test Loss: 3.1439e+02. (Time: 3.7s)Step 21600 of 100000; Loss: 2.4014e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3244e+02; Test Loss: 3.1430e+02. (Time: 2.7s)Step 21800 of 100000; Loss: 2.4006e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0451e+02; Test Loss: 3.1421e+02. (Time: 2.3s)Step 22000 of 100000; Loss: 2.3999e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5935e+02; Test Loss: 3.1413e+02. (Time: 2.3s)Step 22200 of 100000; Loss: 2.3992e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9222e+02; Test Loss: 3.1405e+02. (Time: 2.3s)Step 22400 of 100000; Loss: 2.3985e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2264e+02; Test Loss: 3.1397e+02. (Time: 3.8s)Step 22600 of 100000; Loss: 2.3979e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5894e+02; Test Loss: 3.1389e+02. (Time: 2.3s)Step 22800 of 100000; Loss: 2.3972e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3293e+02; Test Loss: 3.1382e+02. (Time: 2.3s)Step 23000 of 100000; Loss: 2.3966e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6379e+02; Test Loss: 3.1374e+02. (Time: 2.3s)Step 23200 of 100000; Loss: 2.3959e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9633e+02; Test Loss: 3.1367e+02. (Time: 2.3s)Step 23400 of 100000; Loss: 2.3953e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5400e+02; Test Loss: 3.1360e+02. (Time: 3.5s)Step 23600 of 100000; Loss: 2.3948e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9438e+02; Test Loss: 3.1353e+02. (Time: 2.6s)Step 23800 of 100000; Loss: 2.3942e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.0048e+02; Test Loss: 3.1347e+02. (Time: 2.3s)Step 24000 of 100000; Loss: 2.3937e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5942e+02; Test Loss: 3.1341e+02. (Time: 2.4s)Step 24200 of 100000; Loss: 2.3932e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3452e+02; Test Loss: 3.1335e+02. (Time: 2.3s)Step 24400 of 100000; Loss: 2.3927e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1920e+02; Test Loss: 3.1329e+02. (Time: 3.3s)Step 24600 of 100000; Loss: 2.3922e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.8676e+02; Test Loss: 3.1323e+02. (Time: 2.7s)Step 24800 of 100000; Loss: 2.3918e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3181e+02; Test Loss: 3.1318e+02. (Time: 2.3s)Step 25000 of 100000; Loss: 2.3913e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0368e+02; Test Loss: 3.1313e+02. (Time: 2.3s)Step 25200 of 100000; Loss: 2.3909e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5949e+02; Test Loss: 3.1308e+02. (Time: 2.3s)Step 25400 of 100000; Loss: 2.3905e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9174e+02; Test Loss: 3.1303e+02. (Time: 2.9s)Step 25600 of 100000; Loss: 2.3901e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2203e+02; Test Loss: 3.1298e+02. (Time: 3.1s)Step 25800 of 100000; Loss: 2.3897e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5904e+02; Test Loss: 3.1293e+02. (Time: 2.3s)Step 26000 of 100000; Loss: 2.3894e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3273e+02; Test Loss: 3.1288e+02. (Time: 2.3s)Step 26200 of 100000; Loss: 2.3890e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6292e+02; Test Loss: 3.1283e+02. (Time: 2.8s)Step 26400 of 100000; Loss: 2.3886e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9521e+02; Test Loss: 3.1279e+02. (Time: 2.7s)Step 26600 of 100000; Loss: 2.3882e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5376e+02; Test Loss: 3.1275e+02. (Time: 3.3s)Step 26800 of 100000; Loss: 2.3879e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9386e+02; Test Loss: 3.1271e+02. (Time: 2.3s)Step 27000 of 100000; Loss: 2.3876e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9981e+02; Test Loss: 3.1267e+02. (Time: 2.3s)Step 27200 of 100000; Loss: 2.3873e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5929e+02; Test Loss: 3.1263e+02. (Time: 2.3s)Step 27400 of 100000; Loss: 2.3870e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3451e+02; Test Loss: 3.1260e+02. (Time: 2.3s)Step 27600 of 100000; Loss: 2.3867e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1949e+02; Test Loss: 3.1256e+02. (Time: 3.7s)Step 27800 of 100000; Loss: 2.3864e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.8676e+02; Test Loss: 3.1253e+02. (Time: 2.3s)Step 28000 of 100000; Loss: 2.3862e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3144e+02; Test Loss: 3.1249e+02. (Time: 2.3s)Step 28200 of 100000; Loss: 2.3859e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0318e+02; Test Loss: 3.1246e+02. (Time: 2.3s)Step 28400 of 100000; Loss: 2.3857e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5969e+02; Test Loss: 3.1243e+02. (Time: 2.3s)Step 28600 of 100000; Loss: 2.3855e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9151e+02; Test Loss: 3.1240e+02. (Time: 3.8s)Step 28800 of 100000; Loss: 2.3852e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2170e+02; Test Loss: 3.1237e+02. (Time: 2.4s)Step 29000 of 100000; Loss: 2.3850e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5918e+02; Test Loss: 3.1234e+02. (Time: 2.4s)Step 29200 of 100000; Loss: 2.3848e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3268e+02; Test Loss: 3.1232e+02. (Time: 2.3s)Step 29400 of 100000; Loss: 2.3845e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6238e+02; Test Loss: 3.1228e+02. (Time: 2.4s)Step 29600 of 100000; Loss: 2.3843e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9448e+02; Test Loss: 3.1226e+02. (Time: 3.5s)Step 29800 of 100000; Loss: 2.3841e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5368e+02; Test Loss: 3.1223e+02. (Time: 2.6s)Step 30000 of 100000; Loss: 2.3839e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9360e+02; Test Loss: 3.1221e+02. (Time: 2.5s)Step 30200 of 100000; Loss: 2.3837e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9941e+02; Test Loss: 3.1219e+02. (Time: 2.4s)Step 30400 of 100000; Loss: 2.3835e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5928e+02; Test Loss: 3.1216e+02. (Time: 2.4s)Step 30600 of 100000; Loss: 2.3834e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3457e+02; Test Loss: 3.1214e+02. (Time: 4.0s)Step 30800 of 100000; Loss: 2.3832e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1974e+02; Test Loss: 3.1212e+02. (Time: 3.8s)Step 31000 of 100000; Loss: 2.3830e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.8680e+02; Test Loss: 3.1210e+02. (Time: 2.3s)Step 31200 of 100000; Loss: 2.3828e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3123e+02; Test Loss: 3.1208e+02. (Time: 2.3s)Step 31400 of 100000; Loss: 2.3827e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0287e+02; Test Loss: 3.1206e+02. (Time: 3.3s)Step 31600 of 100000; Loss: 2.3826e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5987e+02; Test Loss: 3.1204e+02. (Time: 3.2s)Step 31800 of 100000; Loss: 2.3824e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9140e+02; Test Loss: 3.1202e+02. (Time: 2.3s)Step 32000 of 100000; Loss: 2.3823e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2152e+02; Test Loss: 3.1200e+02. (Time: 2.3s)Step 32200 of 100000; Loss: 2.3822e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5931e+02; Test Loss: 3.1199e+02. (Time: 2.4s)Step 32400 of 100000; Loss: 2.3820e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3269e+02; Test Loss: 3.1197e+02. (Time: 2.5s)Step 32600 of 100000; Loss: 2.3819e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6204e+02; Test Loss: 3.1195e+02. (Time: 3.6s)Step 32800 of 100000; Loss: 2.3817e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9401e+02; Test Loss: 3.1193e+02. (Time: 2.4s)Step 33000 of 100000; Loss: 2.3816e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5367e+02; Test Loss: 3.1192e+02. (Time: 2.4s)Step 33200 of 100000; Loss: 2.3815e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9346e+02; Test Loss: 3.1190e+02. (Time: 2.4s)Step 33400 of 100000; Loss: 2.3814e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9918e+02; Test Loss: 3.1189e+02. (Time: 2.4s)Step 33600 of 100000; Loss: 2.3813e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5931e+02; Test Loss: 3.1187e+02. (Time: 3.7s)Step 33800 of 100000; Loss: 2.3812e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3463e+02; Test Loss: 3.1185e+02. (Time: 2.3s)Step 34000 of 100000; Loss: 2.3810e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1992e+02; Test Loss: 3.1184e+02. (Time: 2.4s)Step 34200 of 100000; Loss: 2.3809e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.8686e+02; Test Loss: 3.1183e+02. (Time: 2.4s)Step 34400 of 100000; Loss: 2.3808e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3109e+02; Test Loss: 3.1182e+02. (Time: 2.3s)Step 34600 of 100000; Loss: 2.3807e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0268e+02; Test Loss: 3.1180e+02. (Time: 3.7s)Step 34800 of 100000; Loss: 2.3806e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6002e+02; Test Loss: 3.1180e+02. (Time: 2.4s)Step 35000 of 100000; Loss: 2.3806e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9134e+02; Test Loss: 3.1178e+02. (Time: 2.4s)Step 35200 of 100000; Loss: 2.3805e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2141e+02; Test Loss: 3.1177e+02. (Time: 2.4s)Step 35400 of 100000; Loss: 2.3804e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5942e+02; Test Loss: 3.1176e+02. (Time: 2.3s)Step 35600 of 100000; Loss: 2.3803e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3272e+02; Test Loss: 3.1175e+02. (Time: 3.6s)Step 35800 of 100000; Loss: 2.3802e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6184e+02; Test Loss: 3.1173e+02. (Time: 2.5s)Step 36000 of 100000; Loss: 2.3801e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9371e+02; Test Loss: 3.1172e+02. (Time: 2.3s)Step 36200 of 100000; Loss: 2.3800e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5368e+02; Test Loss: 3.1171e+02. (Time: 2.3s)Step 36400 of 100000; Loss: 2.3800e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9338e+02; Test Loss: 3.1170e+02. (Time: 2.3s)Step 36600 of 100000; Loss: 2.3799e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9904e+02; Test Loss: 3.1170e+02. (Time: 3.2s)Step 36800 of 100000; Loss: 2.3798e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5934e+02; Test Loss: 3.1169e+02. (Time: 2.9s)Step 37000 of 100000; Loss: 2.3798e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3469e+02; Test Loss: 3.1167e+02. (Time: 2.3s)Step 37200 of 100000; Loss: 2.3797e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2006e+02; Test Loss: 3.1167e+02. (Time: 2.3s)Step 37400 of 100000; Loss: 2.3796e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.8691e+02; Test Loss: 3.1166e+02. (Time: 2.9s)Step 37600 of 100000; Loss: 2.3795e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3100e+02; Test Loss: 3.1165e+02. (Time: 3.4s)Step 37800 of 100000; Loss: 2.3795e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0255e+02; Test Loss: 3.1164e+02. (Time: 2.7s)Step 38000 of 100000; Loss: 2.3794e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6013e+02; Test Loss: 3.1164e+02. (Time: 2.3s)Step 38200 of 100000; Loss: 2.3794e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9132e+02; Test Loss: 3.1163e+02. (Time: 2.3s)Step 38400 of 100000; Loss: 2.3794e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2135e+02; Test Loss: 3.1162e+02. (Time: 2.3s)Step 38600 of 100000; Loss: 2.3793e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5951e+02; Test Loss: 3.1162e+02. (Time: 3.1s)Step 38800 of 100000; Loss: 2.3792e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3274e+02; Test Loss: 3.1161e+02. (Time: 3.0s)Step 39000 of 100000; Loss: 2.3792e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6170e+02; Test Loss: 3.1160e+02. (Time: 2.3s)Step 39200 of 100000; Loss: 2.3791e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9351e+02; Test Loss: 3.1159e+02. (Time: 2.3s)Step 39400 of 100000; Loss: 2.3790e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5369e+02; Test Loss: 3.1158e+02. (Time: 2.3s)Step 39600 of 100000; Loss: 2.3790e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9334e+02; Test Loss: 3.1158e+02. (Time: 2.7s)Step 39800 of 100000; Loss: 2.3789e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9895e+02; Test Loss: 3.1157e+02. (Time: 3.4s)Step 40000 of 100000; Loss: 2.3789e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5937e+02; Test Loss: 3.1157e+02. (Time: 2.3s)Step 40200 of 100000; Loss: 2.3789e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3474e+02; Test Loss: 3.1156e+02. (Time: 2.3s)Step 40400 of 100000; Loss: 2.3788e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2015e+02; Test Loss: 3.1155e+02. (Time: 2.3s)Step 40600 of 100000; Loss: 2.3788e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.8696e+02; Test Loss: 3.1155e+02. (Time: 2.4s)Step 40800 of 100000; Loss: 2.3787e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3095e+02; Test Loss: 3.1154e+02. (Time: 3.7s)Step 41000 of 100000; Loss: 2.3787e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0247e+02; Test Loss: 3.1154e+02. (Time: 2.3s)Step 41200 of 100000; Loss: 2.3787e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6022e+02; Test Loss: 3.1154e+02. (Time: 2.3s)Step 41400 of 100000; Loss: 2.3787e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9130e+02; Test Loss: 3.1153e+02. (Time: 2.3s)Step 41600 of 100000; Loss: 2.3786e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2131e+02; Test Loss: 3.1153e+02. (Time: 2.3s)Step 41800 of 100000; Loss: 2.3786e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5957e+02; Test Loss: 3.1152e+02. (Time: 3.8s)Step 42000 of 100000; Loss: 2.3785e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3277e+02; Test Loss: 3.1152e+02. (Time: 2.3s)Step 42200 of 100000; Loss: 2.3785e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6162e+02; Test Loss: 3.1151e+02. (Time: 2.3s)Step 42400 of 100000; Loss: 2.3784e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9337e+02; Test Loss: 3.1150e+02. (Time: 2.3s)Step 42600 of 100000; Loss: 2.3784e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5370e+02; Test Loss: 3.1150e+02. (Time: 2.3s)Step 42800 of 100000; Loss: 2.3784e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9331e+02; Test Loss: 3.1149e+02. (Time: 3.2s)Step 43000 of 100000; Loss: 2.3783e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9889e+02; Test Loss: 3.1149e+02. (Time: 2.7s)Step 43200 of 100000; Loss: 2.3783e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5939e+02; Test Loss: 3.1149e+02. (Time: 2.3s)Step 43400 of 100000; Loss: 2.3783e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3477e+02; Test Loss: 3.1148e+02. (Time: 2.3s)Step 43600 of 100000; Loss: 2.3782e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2022e+02; Test Loss: 3.1148e+02. (Time: 2.3s)Step 43800 of 100000; Loss: 2.3782e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.8699e+02; Test Loss: 3.1148e+02. (Time: 2.8s)Step 44000 of 100000; Loss: 2.3782e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3091e+02; Test Loss: 3.1147e+02. (Time: 3.7s)Step 44200 of 100000; Loss: 2.3781e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0241e+02; Test Loss: 3.1147e+02. (Time: 2.3s)Step 44400 of 100000; Loss: 2.3781e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6027e+02; Test Loss: 3.1147e+02. (Time: 2.3s)Step 44600 of 100000; Loss: 2.3782e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9130e+02; Test Loss: 3.1147e+02. (Time: 2.3s)Step 44800 of 100000; Loss: 2.3781e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2128e+02; Test Loss: 3.1146e+02. (Time: 2.7s)Step 45000 of 100000; Loss: 2.3781e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5962e+02; Test Loss: 3.1146e+02. (Time: 3.3s)Step 45200 of 100000; Loss: 2.3781e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3278e+02; Test Loss: 3.1145e+02. (Time: 2.4s)Step 45400 of 100000; Loss: 2.3780e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6156e+02; Test Loss: 3.1145e+02. (Time: 2.3s)Step 45600 of 100000; Loss: 2.3780e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9328e+02; Test Loss: 3.1145e+02. (Time: 2.4s)Step 45800 of 100000; Loss: 2.3779e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5371e+02; Test Loss: 3.1144e+02. (Time: 2.5s)Step 46000 of 100000; Loss: 2.3779e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9329e+02; Test Loss: 3.1144e+02. (Time: 3.6s)Step 46200 of 100000; Loss: 2.3779e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9886e+02; Test Loss: 3.1144e+02. (Time: 3.8s)Step 46400 of 100000; Loss: 2.3779e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5941e+02; Test Loss: 3.1143e+02. (Time: 2.4s)Step 46600 of 100000; Loss: 2.3779e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3480e+02; Test Loss: 3.1143e+02. (Time: 2.3s)Step 46800 of 100000; Loss: 2.3778e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2027e+02; Test Loss: 3.1143e+02. (Time: 3.3s)Step 47000 of 100000; Loss: 2.3778e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.8702e+02; Test Loss: 3.1143e+02. (Time: 2.7s)Step 47200 of 100000; Loss: 2.3778e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3088e+02; Test Loss: 3.1142e+02. (Time: 2.3s)Step 47400 of 100000; Loss: 2.3778e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0238e+02; Test Loss: 3.1142e+02. (Time: 2.3s)Step 47600 of 100000; Loss: 2.3778e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6032e+02; Test Loss: 3.1142e+02. (Time: 2.3s)Step 47800 of 100000; Loss: 2.3778e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9129e+02; Test Loss: 3.1142e+02. (Time: 3.1s)Step 48000 of 100000; Loss: 2.3778e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2127e+02; Test Loss: 3.1142e+02. (Time: 3.0s)Step 48200 of 100000; Loss: 2.3778e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5965e+02; Test Loss: 3.1142e+02. (Time: 2.3s)Step 48400 of 100000; Loss: 2.3778e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3279e+02; Test Loss: 3.1141e+02. (Time: 2.3s)Step 48600 of 100000; Loss: 2.3777e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6152e+02; Test Loss: 3.1141e+02. (Time: 2.3s)Step 48800 of 100000; Loss: 2.3777e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9322e+02; Test Loss: 3.1141e+02. (Time: 2.5s)Step 49000 of 100000; Loss: 2.3777e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5372e+02; Test Loss: 3.1140e+02. (Time: 3.5s)Step 49200 of 100000; Loss: 2.3777e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9328e+02; Test Loss: 3.1140e+02. (Time: 2.3s)Step 49400 of 100000; Loss: 2.3776e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9883e+02; Test Loss: 3.1140e+02. (Time: 2.3s)Step 49600 of 100000; Loss: 2.3776e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5942e+02; Test Loss: 3.1140e+02. (Time: 2.3s)Step 49800 of 100000; Loss: 2.3776e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3482e+02; Test Loss: 3.1140e+02. (Time: 2.4s)Step 50000 of 100000; Loss: 2.3776e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2030e+02; Test Loss: 3.1139e+02. (Time: 3.9s)Step 50200 of 100000; Loss: 2.3776e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.8704e+02; Test Loss: 3.1139e+02. (Time: 2.4s)Step 50400 of 100000; Loss: 2.3776e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3086e+02; Test Loss: 3.1139e+02. (Time: 2.3s)Step 50600 of 100000; Loss: 2.3776e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0235e+02; Test Loss: 3.1139e+02. (Time: 2.3s)Step 50800 of 100000; Loss: 2.3776e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6034e+02; Test Loss: 3.1139e+02. (Time: 2.3s)Step 51000 of 100000; Loss: 2.3776e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9129e+02; Test Loss: 3.1139e+02. (Time: 3.6s)Step 51200 of 100000; Loss: 2.3776e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2126e+02; Test Loss: 3.1139e+02. (Time: 2.5s)Step 51400 of 100000; Loss: 2.3776e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5967e+02; Test Loss: 3.1139e+02. (Time: 2.3s)Step 51600 of 100000; Loss: 2.3776e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3280e+02; Test Loss: 3.1139e+02. (Time: 3.0s)Step 51800 of 100000; Loss: 2.3775e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6150e+02; Test Loss: 3.1138e+02. (Time: 2.3s)Step 52000 of 100000; Loss: 2.3775e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9318e+02; Test Loss: 3.1138e+02. (Time: 3.8s)Step 52200 of 100000; Loss: 2.3775e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5372e+02; Test Loss: 3.1138e+02. (Time: 2.3s)Step 52400 of 100000; Loss: 2.3775e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9327e+02; Test Loss: 3.1138e+02. (Time: 2.3s)Step 52600 of 100000; Loss: 2.3775e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9882e+02; Test Loss: 3.1138e+02. (Time: 2.3s)Step 52800 of 100000; Loss: 2.3775e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5943e+02; Test Loss: 3.1138e+02. (Time: 2.3s)Step 53000 of 100000; Loss: 2.3775e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3483e+02; Test Loss: 3.1137e+02. (Time: 3.4s)Step 53200 of 100000; Loss: 2.3774e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2032e+02; Test Loss: 3.1137e+02. (Time: 2.6s)Step 53400 of 100000; Loss: 2.3774e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.8705e+02; Test Loss: 3.1137e+02. (Time: 2.3s)Step 53600 of 100000; Loss: 2.3774e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3085e+02; Test Loss: 3.1137e+02. (Time: 2.3s)Step 53800 of 100000; Loss: 2.3774e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0233e+02; Test Loss: 3.1137e+02. (Time: 2.3s)Step 54000 of 100000; Loss: 2.3774e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6036e+02; Test Loss: 3.1137e+02. (Time: 3.1s)Step 54200 of 100000; Loss: 2.3774e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9129e+02; Test Loss: 3.1137e+02. (Time: 2.9s)Step 54400 of 100000; Loss: 2.3774e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2125e+02; Test Loss: 3.1137e+02. (Time: 2.4s)Step 54600 of 100000; Loss: 2.3774e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5969e+02; Test Loss: 3.1137e+02. (Time: 2.4s)Step 54800 of 100000; Loss: 2.3774e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3281e+02; Test Loss: 3.1137e+02. (Time: 2.4s)Step 55000 of 100000; Loss: 2.3774e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6148e+02; Test Loss: 3.1136e+02. (Time: 3.0s)Step 55200 of 100000; Loss: 2.3773e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9315e+02; Test Loss: 3.1136e+02. (Time: 3.2s)Step 55400 of 100000; Loss: 2.3773e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5372e+02; Test Loss: 3.1136e+02. (Time: 2.5s)Step 55600 of 100000; Loss: 2.3773e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9327e+02; Test Loss: 3.1136e+02. (Time: 2.4s)Step 55800 of 100000; Loss: 2.3773e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9881e+02; Test Loss: 3.1136e+02. (Time: 2.4s)Step 56000 of 100000; Loss: 2.3773e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5944e+02; Test Loss: 3.1136e+02. (Time: 3.0s)Step 56200 of 100000; Loss: 2.3773e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3484e+02; Test Loss: 3.1136e+02. (Time: 3.1s)Step 56400 of 100000; Loss: 2.3773e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2034e+02; Test Loss: 3.1136e+02. (Time: 2.3s)Step 56600 of 100000; Loss: 2.3773e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.8706e+02; Test Loss: 3.1136e+02. (Time: 2.3s)Step 56800 of 100000; Loss: 2.3773e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3084e+02; Test Loss: 3.1136e+02. (Time: 2.3s)Step 57000 of 100000; Loss: 2.3773e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0232e+02; Test Loss: 3.1136e+02. (Time: 2.6s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n"
          ]
        }
      ],
      "source": [
        "# 这个相当于是最好的认知模型，在rescolar里面加了一些参数\n",
        "rl_pc_params, _ = rnn_utils.fit_model(\n",
        "    model_fun=bandits.Hk_PreserveConAgentQ,\n",
        "    dataset_train = dataset_m_train,\n",
        "    dataset_test = dataset_m_validate,\n",
        "    loss_fun='categorical',\n",
        "    optimizer= optax.chain(\n",
        "        optax.add_decayed_weights(1e-5),  # L2 正则化\n",
        "        optax.adam(learning_rate=1e-4)  # Adam 优化器\n",
        "    ),\n",
        "    n_steps_per_call=200,\n",
        "    n_steps_max=100000,\n",
        "    #return_all_losses=True,\n",
        "    early_stop_step=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSHaQoiCzevm",
        "outputId": "504ce31e-62ab-4e9c-a4be-2379d9259df1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Normalized Likelihood: 73.4%\n",
            "Average Negative Log-Likelihood: 0.3985\n"
          ]
        }
      ],
      "source": [
        "mean_likelihood, std_likelihood = compute_log_likelihood(dataset_m_test, bandits.Hk_PreserveConAgentQ, rl_pc_params)\n",
        "mean_nll = compute_negative_log_likelihood(dataset_m_test, bandits.Hk_PreserveConAgentQ, rl_pc_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6R92-p0v7xR4",
        "outputId": "7ca8ba90-2629-4d63-bb97-3ec0ee30dfe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 0:\n",
            "  Monkey 0: train=76, val=10, test=10\n",
            "  Monkey 1: train=74, val=10, test=10\n",
            "  Monkey 2: train=117, val=15, test=15\n",
            "Fold 1:\n",
            "  Monkey 0: train=76, val=10, test=10\n",
            "  Monkey 1: train=74, val=10, test=10\n",
            "  Monkey 2: train=117, val=15, test=15\n",
            "Fold 2:\n",
            "  Monkey 0: train=76, val=10, test=10\n",
            "  Monkey 1: train=74, val=10, test=10\n",
            "  Monkey 2: train=117, val=15, test=15\n",
            "Fold 3:\n",
            "  Monkey 0: train=76, val=10, test=10\n",
            "  Monkey 1: train=74, val=10, test=10\n",
            "  Monkey 2: train=117, val=15, test=15\n",
            "Fold 4:\n",
            "  Monkey 0: train=76, val=10, test=10\n",
            "  Monkey 1: train=76, val=9, test=9\n",
            "  Monkey 2: train=117, val=15, test=15\n",
            "Fold 5:\n",
            "  Monkey 0: train=76, val=10, test=10\n",
            "  Monkey 1: train=76, val=9, test=9\n",
            "  Monkey 2: train=117, val=15, test=15\n",
            "Fold 6:\n",
            "  Monkey 0: train=78, val=9, test=9\n",
            "  Monkey 1: train=76, val=9, test=9\n",
            "  Monkey 2: train=117, val=15, test=15\n",
            "Fold 7:\n",
            "  Monkey 0: train=78, val=9, test=9\n",
            "  Monkey 1: train=76, val=9, test=9\n",
            "  Monkey 2: train=119, val=14, test=14\n",
            "Fold 8:\n",
            "  Monkey 0: train=78, val=9, test=9\n",
            "  Monkey 1: train=76, val=9, test=9\n",
            "  Monkey 2: train=119, val=14, test=14\n",
            "Fold 9:\n",
            "  Monkey 0: train=78, val=9, test=9\n",
            "  Monkey 1: train=76, val=9, test=9\n",
            "  Monkey 2: train=119, val=14, test=14\n",
            "=== Fold 0 ===\n",
            "Step 500 of 500; Loss: 2.5941e+03; Test Loss: 1.0188e+03. (Time: 2.8s)Step 500 of 1000000; Loss: 1.0246e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1091e+03; Test Loss: 9.4449e+02. (Time: 2.0s)Step 1000 of 1000000; Loss: 9.4672e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.0843e+02; Test Loss: 9.1750e+02. (Time: 2.0s)Step 1500 of 1000000; Loss: 9.1826e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.8601e+01; Test Loss: 9.0878e+02. (Time: 2.1s)Step 2000 of 1000000; Loss: 9.0902e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2186e+03; Test Loss: 9.0578e+02. (Time: 2.6s)Step 2500 of 1000000; Loss: 9.0587e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3857e+03; Test Loss: 9.0435e+02. (Time: 2.6s)Step 3000 of 1000000; Loss: 9.0440e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.9007e+02; Test Loss: 9.0353e+02. (Time: 2.0s)Step 3500 of 1000000; Loss: 9.0357e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2171e+02; Test Loss: 9.0314e+02. (Time: 2.0s)Step 4000 of 1000000; Loss: 9.0316e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.9708e+01; Test Loss: 9.0303e+02. (Time: 2.0s)Step 4500 of 1000000; Loss: 9.0303e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2075e+03; Test Loss: 9.0305e+02. (Time: 2.0s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3421\n",
            "Fold 0 test Avg NLL: 0.3421\n",
            "\n",
            "=== Fold 1 ===\n",
            "Step 500 of 500; Loss: 2.5901e+03; Test Loss: 9.5123e+02. (Time: 2.8s)Step 500 of 1000000; Loss: 9.5729e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1115e+03; Test Loss: 8.7315e+02. (Time: 2.2s)Step 1000 of 1000000; Loss: 8.7550e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.9094e+02; Test Loss: 8.4434e+02. (Time: 2.0s)Step 1500 of 1000000; Loss: 8.4517e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.1783e+01; Test Loss: 8.3439e+02. (Time: 2.0s)Step 2000 of 1000000; Loss: 8.3468e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1736e+03; Test Loss: 8.3036e+02. (Time: 2.0s)Step 2500 of 1000000; Loss: 8.3051e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.4010e+03; Test Loss: 8.2790e+02. (Time: 2.0s)Step 3000 of 1000000; Loss: 8.2800e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.9177e+02; Test Loss: 8.2607e+02. (Time: 3.0s)Step 3500 of 1000000; Loss: 8.2615e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9855e+02; Test Loss: 8.2479e+02. (Time: 2.1s)Step 4000 of 1000000; Loss: 8.2484e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.3112e+01; Test Loss: 8.2394e+02. (Time: 2.0s)Step 4500 of 1000000; Loss: 8.2398e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1578e+03; Test Loss: 8.2342e+02. (Time: 2.1s)Step 5000 of 1000000; Loss: 8.2344e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.4175e+03; Test Loss: 8.2310e+02. (Time: 2.0s)Step 5500 of 1000000; Loss: 8.2311e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.8605e+02; Test Loss: 8.2290e+02. (Time: 2.1s)Step 6000 of 1000000; Loss: 8.2291e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8462e+02; Test Loss: 8.2279e+02. (Time: 3.0s)Step 6500 of 1000000; Loss: 8.2279e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1568e+01; Test Loss: 8.2272e+02. (Time: 3.5s)Step 7000 of 1000000; Loss: 8.2272e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1555e+03; Test Loss: 8.2268e+02. (Time: 2.0s)Step 7500 of 1000000; Loss: 8.2268e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.4238e+03; Test Loss: 8.2265e+02. (Time: 2.0s)Step 8000 of 1000000; Loss: 8.2265e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.8547e+02; Test Loss: 8.2263e+02. (Time: 2.0s)Step 8500 of 1000000; Loss: 8.2263e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8200e+02; Test Loss: 8.2261e+02. (Time: 3.1s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3956\n",
            "Fold 1 test Avg NLL: 0.3956\n",
            "\n",
            "=== Fold 2 ===\n",
            "Step 500 of 500; Loss: 2.5978e+03; Test Loss: 9.5639e+02. (Time: 2.0s)Step 500 of 1000000; Loss: 9.6216e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1196e+03; Test Loss: 8.8455e+02. (Time: 2.0s)Step 1000 of 1000000; Loss: 8.8661e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.3530e+02; Test Loss: 8.6050e+02. (Time: 2.1s)Step 1500 of 1000000; Loss: 8.6113e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.8177e+01; Test Loss: 8.5330e+02. (Time: 2.0s)Step 2000 of 1000000; Loss: 8.5350e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2696e+03; Test Loss: 8.5066e+02. (Time: 2.4s)Step 2500 of 1000000; Loss: 8.5076e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3934e+03; Test Loss: 8.4896e+02. (Time: 2.6s)Step 3000 of 1000000; Loss: 8.4903e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0056e+03; Test Loss: 8.4768e+02. (Time: 2.0s)Step 3500 of 1000000; Loss: 8.4773e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5065e+02; Test Loss: 8.4684e+02. (Time: 2.0s)Step 4000 of 1000000; Loss: 8.4688e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.7665e+01; Test Loss: 8.4639e+02. (Time: 2.1s)Step 4500 of 1000000; Loss: 8.4640e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2627e+03; Test Loss: 8.4618e+02. (Time: 2.1s)Step 5000 of 1000000; Loss: 8.4618e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.4057e+03; Test Loss: 8.4609e+02. (Time: 2.7s)Step 5500 of 1000000; Loss: 8.4609e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.9786e+02; Test Loss: 8.4606e+02. (Time: 2.5s)Step 6000 of 1000000; Loss: 8.4606e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.4052e+02; Test Loss: 8.4606e+02. (Time: 2.0s)Step 6500 of 1000000; Loss: 8.4606e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.5909e+01; Test Loss: 8.4608e+02. (Time: 2.0s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3120\n",
            "Fold 2 test Avg NLL: 0.3120\n",
            "\n",
            "=== Fold 3 ===\n",
            "Step 500 of 500; Loss: 2.6721e+03; Test Loss: 9.2042e+02. (Time: 2.0s)Step 500 of 1000000; Loss: 9.2642e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0955e+03; Test Loss: 8.4479e+02. (Time: 2.1s)Step 1000 of 1000000; Loss: 8.4700e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.0993e+02; Test Loss: 8.1825e+02. (Time: 3.2s)Step 1500 of 1000000; Loss: 8.1898e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.8800e+01; Test Loss: 8.0955e+02. (Time: 2.1s)Step 2000 of 1000000; Loss: 8.0980e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1499e+03; Test Loss: 8.0607e+02. (Time: 2.1s)Step 2500 of 1000000; Loss: 8.0620e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5323e+03; Test Loss: 8.0383e+02. (Time: 2.0s)Step 3000 of 1000000; Loss: 8.0393e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.6326e+02; Test Loss: 8.0211e+02. (Time: 2.0s)Step 3500 of 1000000; Loss: 8.0218e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1937e+02; Test Loss: 8.0086e+02. (Time: 2.3s)Step 4000 of 1000000; Loss: 8.0091e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.9928e+01; Test Loss: 8.0004e+02. (Time: 2.7s)Step 4500 of 1000000; Loss: 8.0007e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1390e+03; Test Loss: 7.9953e+02. (Time: 2.0s)Step 5000 of 1000000; Loss: 7.9955e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5445e+03; Test Loss: 7.9922e+02. (Time: 2.1s)Step 5500 of 1000000; Loss: 7.9923e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.6005e+02; Test Loss: 7.9902e+02. (Time: 2.1s)Step 6000 of 1000000; Loss: 7.9903e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0673e+02; Test Loss: 7.9890e+02. (Time: 2.1s)Step 6500 of 1000000; Loss: 7.9891e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.8509e+01; Test Loss: 7.9884e+02. (Time: 2.7s)Step 7000 of 1000000; Loss: 7.9884e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1381e+03; Test Loss: 7.9881e+02. (Time: 2.5s)Step 7500 of 1000000; Loss: 7.9881e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5488e+03; Test Loss: 7.9878e+02. (Time: 2.1s)Step 8000 of 1000000; Loss: 7.9878e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.5996e+02; Test Loss: 7.9875e+02. (Time: 2.1s)Step 8500 of 1000000; Loss: 7.9875e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0427e+02; Test Loss: 7.9874e+02. (Time: 2.1s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3562\n",
            "Fold 3 test Avg NLL: 0.3562\n",
            "\n",
            "=== Fold 4 ===\n",
            "Step 500 of 500; Loss: 2.6398e+03; Test Loss: 8.8703e+02. (Time: 2.0s)Step 500 of 1000000; Loss: 8.9353e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2545e+03; Test Loss: 8.0243e+02. (Time: 3.2s)Step 1000 of 1000000; Loss: 8.0502e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.0947e+02; Test Loss: 7.6936e+02. (Time: 2.1s)Step 1500 of 1000000; Loss: 7.7036e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.4133e+01; Test Loss: 7.5678e+02. (Time: 2.1s)Step 2000 of 1000000; Loss: 7.5717e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1405e+03; Test Loss: 7.5183e+02. (Time: 2.1s)Step 2500 of 1000000; Loss: 7.5199e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.4270e+03; Test Loss: 7.4974e+02. (Time: 2.1s)Step 3000 of 1000000; Loss: 7.4981e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1653e+03; Test Loss: 7.4894e+02. (Time: 2.4s)Step 3500 of 1000000; Loss: 7.4896e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2152e+02; Test Loss: 7.4879e+02. (Time: 2.7s)Step 4000 of 1000000; Loss: 7.4879e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.3355e+01; Test Loss: 7.4895e+02. (Time: 2.2s)Step 4500 of 1000000; Loss: 7.4894e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1291e+03; Test Loss: 7.4925e+02. (Time: 2.1s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3687\n",
            "Fold 4 test Avg NLL: 0.3687\n",
            "\n",
            "=== Fold 5 ===\n",
            "Step 500 of 500; Loss: 2.4990e+03; Test Loss: 9.3510e+02. (Time: 2.1s)Step 500 of 1000000; Loss: 9.4040e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2405e+03; Test Loss: 8.7033e+02. (Time: 2.1s)Step 1000 of 1000000; Loss: 8.7214e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.3391e+02; Test Loss: 8.5026e+02. (Time: 3.1s)Step 1500 of 1000000; Loss: 8.5074e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.5509e+01; Test Loss: 8.4570e+02. (Time: 2.2s)Step 2000 of 1000000; Loss: 8.4578e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2128e+03; Test Loss: 8.4523e+02. (Time: 2.1s)Step 2500 of 1000000; Loss: 8.4522e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2582e+03; Test Loss: 8.4545e+02. (Time: 2.1s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3463\n",
            "Fold 5 test Avg NLL: 0.3463\n",
            "\n",
            "=== Fold 6 ===\n",
            "Step 500 of 500; Loss: 2.5744e+03; Test Loss: 8.9404e+02. (Time: 2.1s)Step 500 of 1000000; Loss: 8.9977e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.3182e+03; Test Loss: 8.2052e+02. (Time: 2.3s)Step 1000 of 1000000; Loss: 8.2273e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1088e+02; Test Loss: 7.9332e+02. (Time: 2.9s)Step 1500 of 1000000; Loss: 7.9410e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.0951e+01; Test Loss: 7.8386e+02. (Time: 2.1s)Step 2000 of 1000000; Loss: 7.8414e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2485e+03; Test Loss: 7.8033e+02. (Time: 3.6s)Step 2500 of 1000000; Loss: 7.8044e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3697e+03; Test Loss: 7.7873e+02. (Time: 2.1s)Step 3000 of 1000000; Loss: 7.7879e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2374e+03; Test Loss: 7.7796e+02. (Time: 2.5s)Step 3500 of 1000000; Loss: 7.7799e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3060e+02; Test Loss: 7.7765e+02. (Time: 2.7s)Step 4000 of 1000000; Loss: 7.7766e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.8374e+01; Test Loss: 7.7758e+02. (Time: 2.0s)Step 4500 of 1000000; Loss: 7.7758e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2399e+03; Test Loss: 7.7761e+02. (Time: 2.0s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.2809\n",
            "Fold 6 test Avg NLL: 0.2809\n",
            "\n",
            "=== Fold 7 ===\n",
            "Step 500 of 500; Loss: 2.5501e+03; Test Loss: 8.0201e+02. (Time: 2.1s)Step 500 of 1000000; Loss: 8.0840e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.3440e+03; Test Loss: 7.1815e+02. (Time: 2.1s)Step 1000 of 1000000; Loss: 7.2075e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1907e+02; Test Loss: 6.8444e+02. (Time: 2.8s)Step 1500 of 1000000; Loss: 6.8548e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.3585e+01; Test Loss: 6.7065e+02. (Time: 2.4s)Step 2000 of 1000000; Loss: 6.7109e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2062e+03; Test Loss: 6.6424e+02. (Time: 2.1s)Step 2500 of 1000000; Loss: 6.6447e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3205e+03; Test Loss: 6.6062e+02. (Time: 2.1s)Step 3000 of 1000000; Loss: 6.6076e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2648e+03; Test Loss: 6.5832e+02. (Time: 2.0s)Step 3500 of 1000000; Loss: 6.5841e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3413e+02; Test Loss: 6.5682e+02. (Time: 2.1s)Step 4000 of 1000000; Loss: 6.5688e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.2107e+01; Test Loss: 6.5583e+02. (Time: 3.1s)Step 4500 of 1000000; Loss: 6.5587e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1939e+03; Test Loss: 6.5520e+02. (Time: 2.1s)Step 5000 of 1000000; Loss: 6.5522e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3327e+03; Test Loss: 6.5481e+02. (Time: 2.2s)Step 5500 of 1000000; Loss: 6.5483e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2616e+03; Test Loss: 6.5458e+02. (Time: 2.1s)Step 6000 of 1000000; Loss: 6.5459e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2367e+02; Test Loss: 6.5444e+02. (Time: 2.1s)Step 6500 of 1000000; Loss: 6.5445e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.0572e+01; Test Loss: 6.5435e+02. (Time: 2.4s)Step 7000 of 1000000; Loss: 6.5435e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1928e+03; Test Loss: 6.5428e+02. (Time: 2.8s)Step 7500 of 1000000; Loss: 6.5429e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3366e+03; Test Loss: 6.5424e+02. (Time: 2.1s)Step 8000 of 1000000; Loss: 6.5425e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2614e+03; Test Loss: 6.5423e+02. (Time: 2.2s)Step 8500 of 1000000; Loss: 6.5422e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2218e+02; Test Loss: 6.5422e+02. (Time: 2.1s)Step 9000 of 1000000; Loss: 6.5422e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.0366e+01; Test Loss: 6.5420e+02. (Time: 2.1s)Step 9500 of 1000000; Loss: 6.5420e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1928e+03; Test Loss: 6.5419e+02. (Time: 2.9s)Step 10000 of 1000000; Loss: 6.5419e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3371e+03; Test Loss: 6.5418e+02. (Time: 2.3s)Step 10500 of 1000000; Loss: 6.5419e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2614e+03; Test Loss: 6.5419e+02. (Time: 2.1s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3741\n",
            "Fold 7 test Avg NLL: 0.3741\n",
            "\n",
            "=== Fold 8 ===\n",
            "Step 500 of 500; Loss: 2.6210e+03; Test Loss: 8.0485e+02. (Time: 2.2s)Step 500 of 1000000; Loss: 8.1092e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2667e+03; Test Loss: 7.2635e+02. (Time: 2.1s)Step 1000 of 1000000; Loss: 7.2875e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.3466e+02; Test Loss: 6.9601e+02. (Time: 2.3s)Step 1500 of 1000000; Loss: 6.9692e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.0648e+01; Test Loss: 6.8444e+02. (Time: 2.9s)Step 2000 of 1000000; Loss: 6.8479e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1565e+03; Test Loss: 6.7955e+02. (Time: 2.0s)Step 2500 of 1000000; Loss: 6.7972e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3929e+03; Test Loss: 6.7700e+02. (Time: 2.0s)Step 3000 of 1000000; Loss: 6.7709e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1791e+03; Test Loss: 6.7549e+02. (Time: 2.0s)Step 3500 of 1000000; Loss: 6.7555e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.4734e+02; Test Loss: 6.7461e+02. (Time: 2.0s)Step 4000 of 1000000; Loss: 6.7464e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.7317e+01; Test Loss: 6.7411e+02. (Time: 2.5s)Step 4500 of 1000000; Loss: 6.7413e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1457e+03; Test Loss: 6.7386e+02. (Time: 2.6s)Step 5000 of 1000000; Loss: 6.7387e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.4019e+03; Test Loss: 6.7375e+02. (Time: 2.0s)Step 5500 of 1000000; Loss: 6.7375e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1786e+03; Test Loss: 6.7371e+02. (Time: 2.0s)Step 6000 of 1000000; Loss: 6.7371e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3749e+02; Test Loss: 6.7370e+02. (Time: 2.0s)Step 6500 of 1000000; Loss: 6.7370e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.5361e+01; Test Loss: 6.7370e+02. (Time: 2.1s)Step 7000 of 1000000; Loss: 6.7370e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1447e+03; Test Loss: 6.7370e+02. (Time: 2.8s)Step 7500 of 1000000; Loss: 6.7370e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.4049e+03; Test Loss: 6.7370e+02. (Time: 2.4s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3983\n",
            "Fold 8 test Avg NLL: 0.3983\n",
            "\n",
            "=== Fold 9 ===\n",
            "Step 500 of 500; Loss: 2.6267e+03; Test Loss: 7.9258e+02. (Time: 2.1s)Step 500 of 1000000; Loss: 7.9897e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1887e+03; Test Loss: 7.0914e+02. (Time: 2.2s)Step 1000 of 1000000; Loss: 7.1171e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.2895e+02; Test Loss: 6.7602e+02. (Time: 2.2s)Step 1500 of 1000000; Loss: 6.7704e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.7274e+01; Test Loss: 6.6268e+02. (Time: 2.6s)Step 2000 of 1000000; Loss: 6.6311e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1474e+03; Test Loss: 6.5653e+02. (Time: 3.2s)Step 2500 of 1000000; Loss: 6.5675e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.4967e+03; Test Loss: 6.5302e+02. (Time: 2.1s)Step 3000 of 1000000; Loss: 6.5316e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0639e+03; Test Loss: 6.5077e+02. (Time: 2.1s)Step 3500 of 1000000; Loss: 6.5086e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.4240e+02; Test Loss: 6.4928e+02. (Time: 2.1s)Step 4000 of 1000000; Loss: 6.4934e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.2891e+01; Test Loss: 6.4830e+02. (Time: 2.1s)Step 4500 of 1000000; Loss: 6.4834e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1329e+03; Test Loss: 6.4767e+02. (Time: 3.1s)Step 5000 of 1000000; Loss: 6.4769e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5102e+03; Test Loss: 6.4727e+02. (Time: 2.4s)Step 5500 of 1000000; Loss: 6.4728e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0632e+03; Test Loss: 6.4703e+02. (Time: 2.2s)Step 6000 of 1000000; Loss: 6.4704e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3216e+02; Test Loss: 6.4688e+02. (Time: 2.2s)Step 6500 of 1000000; Loss: 6.4688e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.0588e+01; Test Loss: 6.4677e+02. (Time: 2.1s)Step 7000 of 1000000; Loss: 6.4678e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1308e+03; Test Loss: 6.4670e+02. (Time: 2.8s)Step 7500 of 1000000; Loss: 6.4671e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5143e+03; Test Loss: 6.4665e+02. (Time: 2.8s)Step 8000 of 1000000; Loss: 6.4665e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0637e+03; Test Loss: 6.4663e+02. (Time: 2.2s)Step 8500 of 1000000; Loss: 6.4663e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3050e+02; Test Loss: 6.4662e+02. (Time: 3.9s)Step 9000 of 1000000; Loss: 6.4662e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.0227e+01; Test Loss: 6.4660e+02. (Time: 2.3s)Step 9500 of 1000000; Loss: 6.4660e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1306e+03; Test Loss: 6.4659e+02. (Time: 3.3s)Step 10000 of 1000000; Loss: 6.4659e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5150e+03; Test Loss: 6.4658e+02. (Time: 2.3s)Step 10500 of 1000000; Loss: 6.4658e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0637e+03; Test Loss: 6.4658e+02. (Time: 2.3s)Step 11000 of 1000000; Loss: 6.4658e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3025e+02; Test Loss: 6.4658e+02. (Time: 2.6s)Step 11500 of 1000000; Loss: 6.4658e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.0173e+01; Test Loss: 6.4658e+02. (Time: 2.7s)Step 12000 of 1000000; Loss: 6.4658e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1306e+03; Test Loss: 6.4657e+02. (Time: 3.3s)Step 12500 of 1000000; Loss: 6.4657e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5151e+03; Test Loss: 6.4657e+02. (Time: 2.1s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.4225\n",
            "Fold 9 test Avg NLL: 0.4225\n",
            "\n",
            "All folds Avg NLL: [0.34205997 0.39563712 0.31203717 0.35617021 0.36865333 0.34628177\n",
            " 0.28091237 0.37406701 0.39825135 0.42252034]\n",
            "Mean NLL over folds: 0.3597\n",
            "Stddev over folds : 0.0422\n",
            "Std. Error (SE)    : 0.0133\n"
          ]
        }
      ],
      "source": [
        "xs_list = [xs_V, xs_w, xs_i]\n",
        "ys_list = [ys_V, ys_w, ys_i]\n",
        "\n",
        "folds = format_into_datasets_10_multi(\n",
        "    xs_list, ys_list,\n",
        "    dataset_constructor=rnn_utils.DatasetRNN,\n",
        "    batch_size=64,\n",
        "    random_seed=42\n",
        ")\n",
        "\n",
        "n_folds = len(folds)\n",
        "avg_nlls = np.zeros(n_folds)\n",
        "\n",
        "for i, (train_ds, val_ds, test_ds) in enumerate(folds):\n",
        "    print(f\"=== Fold {i} ===\")\n",
        "    # 用 train/val 训练并选超参\n",
        "    params, _ = rnn_utils.fit_model(\n",
        "        model_fun=bandits.Hk_PreserveConAgentQ,\n",
        "        dataset_train=train_ds,\n",
        "        dataset_test=val_ds,      # 用 val_ds 做 early-stop\n",
        "        optimizer=optax.chain(\n",
        "            optax.add_decayed_weights(1e-5),\n",
        "            optax.adam(learning_rate=1e-3)\n",
        "        ),\n",
        "        n_steps_per_call=500,\n",
        "        n_steps_max=1000000,\n",
        "        early_stop_step=200,\n",
        "    )\n",
        "\n",
        "    # 在 test_ds 上计算平均 NLL\n",
        "    avg_nll = compute_negative_log_likelihood(test_ds, bandits.Hk_PreserveConAgentQ, params)\n",
        "    print(f\"Fold {i} test Avg NLL: {avg_nll:.4f}\\n\")\n",
        "    avg_nlls[i] = avg_nll\n",
        "\n",
        "# 汇总\n",
        "mean_nll = avg_nlls.mean()\n",
        "std_nll  = avg_nlls.std(ddof=1)    # 样本标准差\n",
        "se_nll   = std_nll / np.sqrt(n_folds)\n",
        "\n",
        "print(\"All folds Avg NLL:\", avg_nlls)\n",
        "print(f\"Mean NLL over folds: {mean_nll:.4f}\")\n",
        "print(f\"Stddev over folds : {std_nll:.4f}\")\n",
        "print(f\"Std. Error (SE)    : {se_nll:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmgO7HywNl5-"
      },
      "source": [
        "## Fit Vanilla RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS3w091rqjxY"
      },
      "source": [
        "### For Monkey"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Find #steps for each learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 200 of 200; Loss: 3.0400e+03; Test Loss: 1.0495e+03. (Time: 2.2s)updating best model ..\n",
            "Step 200 of 5000; Loss: 1.0738e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.4095e+02; Test Loss: 7.9454e+02. (Time: 2.6s)updating best model ..\n",
            "Step 400 of 5000; Loss: 8.1270e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.3733e+01; Test Loss: 7.6827e+02. (Time: 2.2s)updating best model ..\n",
            "Step 600 of 5000; Loss: 7.6995e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3733e+00; Test Loss: 7.5969e+02. (Time: 2.7s)updating best model ..\n",
            "Step 800 of 5000; Loss: 7.6003e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9361e+03; Test Loss: 7.5879e+02. (Time: 2.3s)updating best model ..\n",
            "Step 1000 of 5000; Loss: 7.5896e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2553e+03; Test Loss: 7.5941e+02. (Time: 2.2s)Step 1200 of 5000; Loss: 7.5922e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1093e+02; Test Loss: 7.5893e+02. (Time: 2.2s)Step 1400 of 5000; Loss: 7.5899e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1794e+01; Test Loss: 7.5758e+02. (Time: 2.2s)updating best model ..\n",
            "Step 1600 of 5000; Loss: 7.5854e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6510e+00; Test Loss: 7.6287e+02. (Time: 2.4s)Step 1800 of 5000; Loss: 7.5955e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8633e+03; Test Loss: 7.6164e+02. (Time: 2.1s)Step 2000 of 5000; Loss: 7.6476e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1992e+03; Test Loss: 8.6823e+02. (Time: 2.4s)Step 2200 of 5000; Loss: 8.1798e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.8337e+02; Test Loss: 8.8778e+02. (Time: 2.1s)Step 2400 of 5000; Loss: 9.1054e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5880e+01; Test Loss: 1.0300e+03. (Time: 2.2s)Step 2600 of 5000; Loss: 9.5479e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6437e+00; Test Loss: 1.0847e+03. (Time: 2.4s)Step 2800 of 5000; Loss: 1.0627e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8179e+03; Test Loss: 1.1222e+03. (Time: 2.2s)Step 3000 of 5000; Loss: 9.0545e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1562e+03; Test Loss: 3.1531e+05. (Time: 2.2s)Step 3200 of 5000; Loss: 1.9618e+05. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.6290e+02; Test Loss: 6.4355e+05. (Time: 2.5s)Step 3400 of 5000; Loss: 5.1395e+05. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2183e+01; Test Loss: 1.2692e+06. (Time: 2.4s)Step 3600 of 5000; Loss: 9.5889e+05. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0897e+00; Test Loss: 9.3397e+05. (Time: 2.8s)Step 3800 of 5000; Loss: 1.6722e+06. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7743e+03; Test Loss: 1.2252e+06. (Time: 2.8s)Step 4000 of 5000; Loss: 2.4108e+06. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1315e+03; Test Loss: 5.0530e+06. (Time: 3.2s)Step 4200 of 5000; Loss: 3.7840e+06. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.5061e+02; Test Loss: 6.0454e+06. (Time: 3.2s)Step 4400 of 5000; Loss: 5.5717e+06. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1324e+01; Test Loss: 3.2924e+06. (Time: 2.1s)Step 4600 of 5000; Loss: 6.6662e+05. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7679e+00; Test Loss: 1.0038e+07. (Time: 2.5s)Step 4800 of 5000; Loss: 1.2209e+07. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7250e+03; Test Loss: 1.4270e+07. (Time: 2.2s)\n",
            "Maximum iterations reached. Final test loss: 1.5804e+07\n",
            "Step 5000 of 5000; Loss: 1.5804e+07. (Time: 0.0s)\n",
            "Average Negative Log-Likelihood: 0.3597\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset_m_train, dataset_m_test, dataset_m_validate = format_into_datasets_multi_monkey(\n",
        "    xs_list, ys_list,\n",
        "    dataset_constructor=rnn_utils.DatasetRNN,\n",
        "    n_train_sessions=267,    # 全体一共要选 200 个 session 做训练\n",
        "    n_test_sessions= 35,     # 全体 40 做测试\n",
        "    n_validate_sessions=35,  # 全体 40 做验证\n",
        "    batch_size=64,\n",
        "    random_seed=42\n",
        ")\n",
        "n_hidden = 64\n",
        "def make_vanilla_rnn():\n",
        "    model = hk.DeepRNN(\n",
        "        [hk.VanillaRNN(n_hidden), hk.Linear(output_size=2)]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Fit the model\n",
        "vanillaRNN_params, _, all_losses = rnn_utils.fit_model(\n",
        "    model_fun=make_vanilla_rnn,\n",
        "    dataset_train=dataset_m_train,\n",
        "    dataset_test=dataset_m_validate,\n",
        "    loss_fun='categorical',\n",
        "    optimizer=optax.chain(\n",
        "        optax.add_decayed_weights(1e-4),  # L2 regularization\n",
        "        optax.adam(learning_rate=1e-4)    # Adam optimizer\n",
        "    ),\n",
        "    n_steps_per_call=200,\n",
        "    n_steps_max=5000,\n",
        "    return_all_losses=True,\n",
        "    early_stop_step=200\n",
        ")\n",
        "\n",
        "# Extract validation losses (assuming all_losses is a dict with 'test' key)\n",
        "avg_nll = compute_negative_log_likelihood(dataset_m_validate, make_vanilla_rnn, vanillaRNN_params)\n",
        "plt.plot(all_losses)\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Val losses')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run for 5 times, see how results differ --> 都一样\n",
        "avg_nlls = [0.3597, 0.3597, 0.3597]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6CjxUyw9WLx"
      },
      "source": [
        "#### This is for hyperparameter search of Vanilla RNN on Monkey\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khylH5F1OKHK",
        "outputId": "f3472f01-1827-4509-efc3-71bfb2cba503"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running configuration: hidden_size=4, lr=0.001, weight_decay=0.001, batch_size=32\n",
            "Step 200 of 200; Loss: 1.5939e+03; Test Loss: 4.6920e+01. (Time: 2.1s)Step 200 of 1000000; Loss: 6.8287e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.3335e+02; Test Loss: 3.7367e+01. (Time: 2.2s)Step 400 of 1000000; Loss: 6.2188e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1664e+03; Test Loss: 3.9299e+00. (Time: 7.0s)Step 600 of 1000000; Loss: 4.4289e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3098e+03; Test Loss: 3.5063e+00. (Time: 2.7s)Step 800 of 1000000; Loss: 4.3315e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9149e+01; Test Loss: 3.1404e+00. (Time: 2.2s)Step 1000 of 1000000; Loss: 4.2756e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0793e+03; Test Loss: 2.8327e+00. (Time: 2.3s)Step 1200 of 1000000; Loss: 4.2333e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2614e+03; Test Loss: 2.5768e+00. (Time: 2.5s)Step 1400 of 1000000; Loss: 4.2046e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.7415e+01; Test Loss: 2.3577e+00. (Time: 3.1s)Step 1600 of 1000000; Loss: 4.1770e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0337e+03; Test Loss: 2.1586e+00. (Time: 2.4s)Step 1800 of 1000000; Loss: 4.1490e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2343e+03; Test Loss: 1.9842e+00. (Time: 2.2s)Step 2000 of 1000000; Loss: 4.1164e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2591e+01; Test Loss: 1.8241e+00. (Time: 2.3s)Step 2200 of 1000000; Loss: 4.0823e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.9806e+02; Test Loss: 1.6681e+00. (Time: 2.2s)Step 2400 of 1000000; Loss: 4.0431e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2049e+03; Test Loss: 1.5210e+00. (Time: 2.9s)Step 2600 of 1000000; Loss: 4.0030e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5232e+01; Test Loss: 1.3824e+00. (Time: 2.8s)Step 2800 of 1000000; Loss: 3.9891e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6549e+02; Test Loss: 1.2592e+00. (Time: 2.2s)Step 3000 of 1000000; Loss: 3.9621e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2415e+03; Test Loss: 1.0199e+00. (Time: 2.2s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.4000\n",
            "Running configuration: hidden_size=4, lr=0.001, weight_decay=0.001, batch_size=64\n",
            "Step 200 of 200; Loss: 3.2083e+03; Test Loss: 1.3476e+03. (Time: 1.9s)Step 200 of 1000000; Loss: 1.3686e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8872e+03; Test Loss: 1.2263e+03. (Time: 2.1s)Step 400 of 1000000; Loss: 1.2432e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1311e+02; Test Loss: 8.8268e+02. (Time: 3.1s)Step 600 of 1000000; Loss: 8.8753e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2616e+01; Test Loss: 8.6556e+02. (Time: 2.1s)Step 800 of 1000000; Loss: 8.6729e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3132e+03; Test Loss: 8.5481e+02. (Time: 2.0s)Step 1000 of 1000000; Loss: 8.5596e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5563e+03; Test Loss: 8.4709e+02. (Time: 2.0s)Step 1200 of 1000000; Loss: 8.4797e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0245e+02; Test Loss: 8.4170e+02. (Time: 2.0s)Step 1400 of 1000000; Loss: 8.4224e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1920e+02; Test Loss: 8.3760e+02. (Time: 2.5s)Step 1600 of 1000000; Loss: 8.3806e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.9089e+00; Test Loss: 8.3392e+02. (Time: 2.7s)Step 1800 of 1000000; Loss: 8.3434e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1771e+03; Test Loss: 8.3072e+02. (Time: 1.9s)Step 2000 of 1000000; Loss: 8.3108e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4792e+03; Test Loss: 8.2789e+02. (Time: 2.1s)Step 2200 of 1000000; Loss: 8.2824e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.3912e+02; Test Loss: 8.2504e+02. (Time: 2.1s)Step 2400 of 1000000; Loss: 8.2538e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.0290e+01; Test Loss: 8.2193e+02. (Time: 2.1s)Step 2600 of 1000000; Loss: 8.2234e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.7188e+00; Test Loss: 8.1964e+02. (Time: 2.8s)Step 2800 of 1000000; Loss: 8.1984e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1103e+03; Test Loss: 8.1753e+02. (Time: 2.6s)Step 3000 of 1000000; Loss: 8.1779e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4301e+03; Test Loss: 8.1569e+02. (Time: 2.1s)Step 3200 of 1000000; Loss: 8.1589e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.0975e+02; Test Loss: 8.1400e+02. (Time: 1.9s)Step 3400 of 1000000; Loss: 8.1424e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.0758e+01; Test Loss: 8.1184e+02. (Time: 2.1s)Step 3600 of 1000000; Loss: 8.1210e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.7567e+00; Test Loss: 8.0960e+02. (Time: 2.1s)Step 3800 of 1000000; Loss: 8.0988e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0534e+03; Test Loss: 8.0707e+02. (Time: 3.2s)Step 4000 of 1000000; Loss: 8.0742e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3886e+03; Test Loss: 8.0389e+02. (Time: 2.3s)Step 4200 of 1000000; Loss: 8.0431e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.8795e+02; Test Loss: 8.0007e+02. (Time: 2.7s)Step 4400 of 1000000; Loss: 8.0058e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.6895e+01; Test Loss: 7.9562e+02. (Time: 2.1s)Step 4600 of 1000000; Loss: 7.9616e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.8927e+00; Test Loss: 7.9135e+02. (Time: 2.0s)Step 4800 of 1000000; Loss: 7.9183e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9943e+03; Test Loss: 7.8744e+02. (Time: 3.0s)Step 5000 of 1000000; Loss: 7.8791e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3440e+03; Test Loss: 7.8456e+02. (Time: 2.3s)Step 5200 of 1000000; Loss: 7.8489e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.6556e+02; Test Loss: 7.8193e+02. (Time: 1.9s)Step 5400 of 1000000; Loss: 7.8221e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3693e+01; Test Loss: 7.7988e+02. (Time: 1.9s)Step 5600 of 1000000; Loss: 7.8009e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9281e+00; Test Loss: 7.7806e+02. (Time: 2.0s)Step 5800 of 1000000; Loss: 7.7823e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9660e+03; Test Loss: 7.7667e+02. (Time: 2.0s)Step 6000 of 1000000; Loss: 7.7684e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3301e+03; Test Loss: 7.7540e+02. (Time: 3.0s)Step 6200 of 1000000; Loss: 7.7555e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.5525e+02; Test Loss: 7.7391e+02. (Time: 2.0s)Step 6400 of 1000000; Loss: 7.7411e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7022e+01; Test Loss: 7.7241e+02. (Time: 1.9s)Step 6600 of 1000000; Loss: 7.7258e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3529e+00; Test Loss: 7.7111e+02. (Time: 1.9s)Step 6800 of 1000000; Loss: 7.7131e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9544e+03; Test Loss: 7.7010e+02. (Time: 1.9s)Step 7000 of 1000000; Loss: 7.7026e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3244e+03; Test Loss: 7.6931e+02. (Time: 2.1s)Step 7200 of 1000000; Loss: 7.6944e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.4533e+02; Test Loss: 7.6834e+02. (Time: 3.1s)Step 7400 of 1000000; Loss: 7.6842e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2593e+01; Test Loss: 7.6745e+02. (Time: 2.3s)Step 7600 of 1000000; Loss: 7.6754e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8850e+00; Test Loss: 7.6577e+02. (Time: 2.0s)Step 7800 of 1000000; Loss: 7.6595e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9484e+03; Test Loss: 7.6502e+02. (Time: 1.9s)Step 8000 of 1000000; Loss: 7.6512e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3209e+03; Test Loss: 7.6423e+02. (Time: 1.9s)Step 8200 of 1000000; Loss: 7.6432e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.3809e+02; Test Loss: 7.6385e+02. (Time: 1.9s)Step 8400 of 1000000; Loss: 7.6392e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1464e+01; Test Loss: 7.6324e+02. (Time: 3.4s)Step 8600 of 1000000; Loss: 7.6329e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8959e+00; Test Loss: 7.6264e+02. (Time: 2.1s)Step 8800 of 1000000; Loss: 7.6270e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9448e+03; Test Loss: 7.6186e+02. (Time: 2.2s)Step 9000 of 1000000; Loss: 7.6196e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3185e+03; Test Loss: 7.6166e+02. (Time: 2.0s)Step 9200 of 1000000; Loss: 7.6171e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.3525e+02; Test Loss: 7.6110e+02. (Time: 1.9s)Step 9400 of 1000000; Loss: 7.6116e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0999e+01; Test Loss: 7.6043e+02. (Time: 2.1s)Step 9600 of 1000000; Loss: 7.6053e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8162e+00; Test Loss: 7.5917e+02. (Time: 3.2s)Step 9800 of 1000000; Loss: 7.5926e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9390e+03; Test Loss: 7.5782e+02. (Time: 2.1s)Step 10000 of 1000000; Loss: 7.5801e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3108e+03; Test Loss: 7.5695e+02. (Time: 2.1s)Step 10200 of 1000000; Loss: 7.5717e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.3026e+02; Test Loss: 7.5583e+02. (Time: 2.0s)Step 10400 of 1000000; Loss: 7.5601e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9936e+01; Test Loss: 7.5471e+02. (Time: 2.0s)Step 10600 of 1000000; Loss: 7.5476e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6301e+00; Test Loss: 7.5407e+02. (Time: 2.6s)Step 10800 of 1000000; Loss: 7.5404e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9335e+03; Test Loss: 7.5324e+02. (Time: 2.5s)Step 11000 of 1000000; Loss: 7.5343e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3064e+03; Test Loss: 7.5241e+02. (Time: 2.2s)Step 11200 of 1000000; Loss: 7.5260e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.2657e+02; Test Loss: 7.5163e+02. (Time: 2.0s)Step 11400 of 1000000; Loss: 7.5172e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7983e+01; Test Loss: 7.5114e+02. (Time: 1.9s)Step 11600 of 1000000; Loss: 7.5124e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2833e+00; Test Loss: 7.5111e+02. (Time: 2.0s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3567\n",
            "Running configuration: hidden_size=4, lr=0.001, weight_decay=0.0001, batch_size=32\n",
            "Step 200 of 200; Loss: 1.5939e+03; Test Loss: 4.6921e+01. (Time: 3.1s)Step 200 of 1000000; Loss: 6.8287e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.3334e+02; Test Loss: 3.7366e+01. (Time: 2.6s)Step 400 of 1000000; Loss: 6.2188e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1664e+03; Test Loss: 3.9299e+00. (Time: 2.1s)Step 600 of 1000000; Loss: 4.4289e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3104e+03; Test Loss: 3.5063e+00. (Time: 2.3s)Step 800 of 1000000; Loss: 4.3312e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9474e+01; Test Loss: 3.1419e+00. (Time: 2.2s)Step 1000 of 1000000; Loss: 4.2760e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0788e+03; Test Loss: 2.8261e+00. (Time: 3.1s)Step 1200 of 1000000; Loss: 4.2347e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2629e+03; Test Loss: 2.5723e+00. (Time: 2.8s)Step 1400 of 1000000; Loss: 4.2065e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.3573e+01; Test Loss: 2.3563e+00. (Time: 2.2s)Step 1600 of 1000000; Loss: 4.1830e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0359e+03; Test Loss: 2.1629e+00. (Time: 2.2s)Step 1800 of 1000000; Loss: 4.1610e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2388e+03; Test Loss: 1.9901e+00. (Time: 2.1s)Step 2000 of 1000000; Loss: 4.1408e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.6006e+01; Test Loss: 1.8338e+00. (Time: 2.8s)Step 2200 of 1000000; Loss: 4.1192e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0068e+03; Test Loss: 1.6883e+00. (Time: 3.2s)Step 2400 of 1000000; Loss: 4.0929e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2169e+03; Test Loss: 1.5559e+00. (Time: 2.4s)Step 2600 of 1000000; Loss: 4.0626e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5309e+01; Test Loss: 1.4425e+00. (Time: 2.3s)Step 2800 of 1000000; Loss: 4.0336e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.7825e+02; Test Loss: 1.3380e+00. (Time: 2.2s)Step 3000 of 1000000; Loss: 4.0031e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1939e+03; Test Loss: 1.2342e+00. (Time: 2.4s)Step 3200 of 1000000; Loss: 3.9783e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1421e+01; Test Loss: 1.1291e+00. (Time: 3.2s)Step 3400 of 1000000; Loss: 3.9598e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5588e+02; Test Loss: 1.0327e+00. (Time: 2.3s)Step 3600 of 1000000; Loss: 3.9444e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1828e+03; Test Loss: 9.4706e-01. (Time: 2.4s)Step 3800 of 1000000; Loss: 3.9315e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6963e+01; Test Loss: 8.7221e-01. (Time: 2.4s)Step 4000 of 1000000; Loss: 3.9202e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.4144e+02; Test Loss: 8.0920e-01. (Time: 2.5s)Step 4200 of 1000000; Loss: 3.9128e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1802e+03; Test Loss: 7.5563e-01. (Time: 3.1s)Step 4400 of 1000000; Loss: 3.9063e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4066e+01; Test Loss: 7.0929e-01. (Time: 2.4s)Step 4600 of 1000000; Loss: 3.9015e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3406e+02; Test Loss: 6.6880e-01. (Time: 2.5s)Step 4800 of 1000000; Loss: 3.8960e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1805e+03; Test Loss: 6.2878e-01. (Time: 2.4s)Step 5000 of 1000000; Loss: 3.8897e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1930e+01; Test Loss: 5.9442e-01. (Time: 2.6s)Step 5200 of 1000000; Loss: 3.8849e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2952e+02; Test Loss: 5.6810e-01. (Time: 3.3s)Step 5400 of 1000000; Loss: 3.8815e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1812e+03; Test Loss: 5.4721e-01. (Time: 2.4s)Step 5600 of 1000000; Loss: 3.8785e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0576e+01; Test Loss: 5.2896e-01. (Time: 2.2s)Step 5800 of 1000000; Loss: 3.8749e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2713e+02; Test Loss: 5.1445e-01. (Time: 2.3s)Step 6000 of 1000000; Loss: 3.8718e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1823e+03; Test Loss: 5.0264e-01. (Time: 2.6s)Step 6200 of 1000000; Loss: 3.8685e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.7858e+00; Test Loss: 4.9283e-01. (Time: 3.1s)Step 6400 of 1000000; Loss: 3.8660e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2610e+02; Test Loss: 4.8524e-01. (Time: 2.2s)Step 6600 of 1000000; Loss: 3.8639e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1831e+03; Test Loss: 4.7962e-01. (Time: 2.3s)Step 6800 of 1000000; Loss: 3.8617e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3571e+00; Test Loss: 4.7504e-01. (Time: 2.2s)Step 7000 of 1000000; Loss: 3.8595e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2528e+02; Test Loss: 4.7160e-01. (Time: 2.3s)Step 7200 of 1000000; Loss: 3.8573e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1839e+03; Test Loss: 4.6912e-01. (Time: 3.4s)Step 7400 of 1000000; Loss: 3.8549e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.1435e+00; Test Loss: 4.6693e-01. (Time: 2.3s)Step 7600 of 1000000; Loss: 3.8530e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2436e+02; Test Loss: 4.6486e-01. (Time: 2.3s)Step 7800 of 1000000; Loss: 3.8513e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1843e+03; Test Loss: 4.6378e-01. (Time: 2.3s)Step 8000 of 1000000; Loss: 3.8494e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0291e+00; Test Loss: 4.6313e-01. (Time: 2.2s)Step 8200 of 1000000; Loss: 3.8477e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2295e+02; Test Loss: 4.6206e-01. (Time: 3.6s)Step 8400 of 1000000; Loss: 3.8460e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1836e+03; Test Loss: 4.5949e-01. (Time: 2.2s)Step 8600 of 1000000; Loss: 3.8435e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4362e+01; Test Loss: 6.4454e-01. (Time: 2.4s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3847\n",
            "Running configuration: hidden_size=4, lr=0.001, weight_decay=0.0001, batch_size=64\n",
            "Step 200 of 200; Loss: 3.2083e+03; Test Loss: 1.3476e+03. (Time: 2.1s)Step 200 of 1000000; Loss: 1.3686e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8872e+03; Test Loss: 1.2264e+03. (Time: 1.9s)Step 400 of 1000000; Loss: 1.2432e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1320e+02; Test Loss: 8.8269e+02. (Time: 3.0s)Step 600 of 1000000; Loss: 8.8755e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2758e+01; Test Loss: 8.6584e+02. (Time: 2.3s)Step 800 of 1000000; Loss: 8.6773e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3126e+03; Test Loss: 8.5449e+02. (Time: 2.0s)Step 1000 of 1000000; Loss: 8.5569e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5503e+03; Test Loss: 8.4634e+02. (Time: 1.9s)Step 1200 of 1000000; Loss: 8.4722e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9384e+02; Test Loss: 8.4048e+02. (Time: 2.1s)Step 1400 of 1000000; Loss: 8.4112e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0734e+02; Test Loss: 8.3499e+02. (Time: 6.7s)Step 1600 of 1000000; Loss: 8.3570e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.5392e+00; Test Loss: 8.2924e+02. (Time: 1.9s)Step 1800 of 1000000; Loss: 8.2997e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1585e+03; Test Loss: 8.2269e+02. (Time: 1.9s)Step 2000 of 1000000; Loss: 8.2364e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4466e+03; Test Loss: 8.1487e+02. (Time: 2.1s)Step 2200 of 1000000; Loss: 8.1581e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.2141e+02; Test Loss: 8.0676e+02. (Time: 1.9s)Step 2400 of 1000000; Loss: 8.0779e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.0058e+01; Test Loss: 7.9899e+02. (Time: 2.8s)Step 2600 of 1000000; Loss: 7.9965e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.7287e+00; Test Loss: 7.9366e+02. (Time: 2.4s)Step 2800 of 1000000; Loss: 7.9420e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2840e+03; Test Loss: 8.4889e+02. (Time: 1.9s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.4031\n",
            "Running configuration: hidden_size=4, lr=0.001, weight_decay=1e-05, batch_size=32\n",
            "Step 200 of 200; Loss: 1.5939e+03; Test Loss: 4.6921e+01. (Time: 2.3s)Step 200 of 1000000; Loss: 6.8287e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.3334e+02; Test Loss: 3.7366e+01. (Time: 2.1s)Step 400 of 1000000; Loss: 6.2188e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1664e+03; Test Loss: 3.9300e+00. (Time: 2.7s)Step 600 of 1000000; Loss: 4.4289e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3098e+03; Test Loss: 3.5057e+00. (Time: 3.3s)Step 800 of 1000000; Loss: 4.3316e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6835e+01; Test Loss: 3.1370e+00. (Time: 2.2s)Step 1000 of 1000000; Loss: 4.2780e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0788e+03; Test Loss: 2.8273e+00. (Time: 2.3s)Step 1200 of 1000000; Loss: 4.2345e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2627e+03; Test Loss: 2.5742e+00. (Time: 2.1s)Step 1400 of 1000000; Loss: 4.2060e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.0200e+01; Test Loss: 2.3588e+00. (Time: 2.4s)Step 1600 of 1000000; Loss: 4.1820e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0355e+03; Test Loss: 2.1629e+00. (Time: 3.4s)Step 1800 of 1000000; Loss: 4.1586e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2377e+03; Test Loss: 1.9873e+00. (Time: 2.2s)Step 2000 of 1000000; Loss: 4.1366e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4544e+01; Test Loss: 1.8283e+00. (Time: 2.2s)Step 2200 of 1000000; Loss: 4.1117e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0049e+03; Test Loss: 1.6799e+00. (Time: 2.3s)Step 2400 of 1000000; Loss: 4.0790e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2136e+03; Test Loss: 1.5481e+00. (Time: 2.2s)Step 2600 of 1000000; Loss: 4.0438e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6305e+01; Test Loss: 1.4313e+00. (Time: 3.3s)Step 2800 of 1000000; Loss: 4.0108e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.7463e+02; Test Loss: 1.3167e+00. (Time: 2.5s)Step 3000 of 1000000; Loss: 3.9824e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1913e+03; Test Loss: 1.2014e+00. (Time: 2.4s)Step 3200 of 1000000; Loss: 3.9623e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0720e+01; Test Loss: 1.0953e+00. (Time: 2.2s)Step 3400 of 1000000; Loss: 3.9457e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5259e+02; Test Loss: 1.0016e+00. (Time: 2.4s)Step 3600 of 1000000; Loss: 3.9318e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1823e+03; Test Loss: 9.2214e-01. (Time: 3.3s)Step 3800 of 1000000; Loss: 3.9216e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6511e+01; Test Loss: 8.5194e-01. (Time: 2.4s)Step 4000 of 1000000; Loss: 3.9139e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3951e+02; Test Loss: 7.9092e-01. (Time: 2.3s)Step 4200 of 1000000; Loss: 3.9075e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1804e+03; Test Loss: 7.4062e-01. (Time: 2.2s)Step 4400 of 1000000; Loss: 3.9027e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3685e+01; Test Loss: 6.9549e-01. (Time: 2.3s)Step 4600 of 1000000; Loss: 3.8970e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3278e+02; Test Loss: 6.5286e-01. (Time: 2.7s)Step 4800 of 1000000; Loss: 3.8918e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1807e+03; Test Loss: 6.1602e-01. (Time: 3.0s)Step 5000 of 1000000; Loss: 3.8865e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1600e+01; Test Loss: 5.8625e-01. (Time: 2.1s)Step 5200 of 1000000; Loss: 3.8820e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2886e+02; Test Loss: 5.6226e-01. (Time: 2.3s)Step 5400 of 1000000; Loss: 3.8785e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1818e+03; Test Loss: 5.4191e-01. (Time: 2.2s)Step 5600 of 1000000; Loss: 3.8747e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0366e+01; Test Loss: 5.2490e-01. (Time: 2.3s)Step 5800 of 1000000; Loss: 3.8719e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2712e+02; Test Loss: 5.1155e-01. (Time: 3.5s)Step 6000 of 1000000; Loss: 3.8689e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1827e+03; Test Loss: 5.0178e-01. (Time: 2.1s)Step 6200 of 1000000; Loss: 3.8665e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6825e+00; Test Loss: 4.9357e-01. (Time: 2.3s)Step 6400 of 1000000; Loss: 3.8641e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2629e+02; Test Loss: 4.8662e-01. (Time: 2.4s)Step 6600 of 1000000; Loss: 3.8614e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1833e+03; Test Loss: 4.8097e-01. (Time: 2.5s)Step 6800 of 1000000; Loss: 3.8587e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2823e+00; Test Loss: 4.7617e-01. (Time: 3.4s)Step 7000 of 1000000; Loss: 3.8566e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2573e+02; Test Loss: 4.7201e-01. (Time: 2.4s)Step 7200 of 1000000; Loss: 3.8542e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1836e+03; Test Loss: 4.6887e-01. (Time: 2.4s)Step 7400 of 1000000; Loss: 3.8519e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0293e+00; Test Loss: 4.6619e-01. (Time: 2.1s)Step 7600 of 1000000; Loss: 3.8497e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2510e+02; Test Loss: 4.6402e-01. (Time: 2.3s)Step 7800 of 1000000; Loss: 3.8473e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1838e+03; Test Loss: 4.6178e-01. (Time: 3.3s)Step 8000 of 1000000; Loss: 3.8444e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.8862e+00; Test Loss: 4.5984e-01. (Time: 2.3s)Step 8200 of 1000000; Loss: 3.8425e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2361e+02; Test Loss: 4.5879e-01. (Time: 2.2s)Step 8400 of 1000000; Loss: 3.8409e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1842e+03; Test Loss: 4.5774e-01. (Time: 2.3s)Step 8600 of 1000000; Loss: 3.8389e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.8838e+00; Test Loss: 4.5624e-01. (Time: 2.1s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3777\n",
            "Running configuration: hidden_size=4, lr=0.001, weight_decay=1e-05, batch_size=64\n",
            "Step 200 of 200; Loss: 3.2083e+03; Test Loss: 1.3476e+03. (Time: 3.1s)Step 200 of 1000000; Loss: 1.3686e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8872e+03; Test Loss: 1.2264e+03. (Time: 2.3s)Step 400 of 1000000; Loss: 1.2432e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1320e+02; Test Loss: 8.8269e+02. (Time: 2.0s)Step 600 of 1000000; Loss: 8.8755e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7060e+01; Test Loss: 8.6594e+02. (Time: 1.9s)Step 800 of 1000000; Loss: 8.6912e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3122e+03; Test Loss: 8.5447e+02. (Time: 1.9s)Step 1000 of 1000000; Loss: 8.5564e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5527e+03; Test Loss: 8.4663e+02. (Time: 2.0s)Step 1200 of 1000000; Loss: 8.4746e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9606e+02; Test Loss: 8.4109e+02. (Time: 3.1s)Step 1400 of 1000000; Loss: 8.4169e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1566e+02; Test Loss: 8.3648e+02. (Time: 2.0s)Step 1600 of 1000000; Loss: 8.3702e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.7199e+00; Test Loss: 8.3220e+02. (Time: 1.9s)Step 1800 of 1000000; Loss: 8.3268e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1703e+03; Test Loss: 8.2840e+02. (Time: 1.9s)Step 2000 of 1000000; Loss: 8.2889e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4641e+03; Test Loss: 8.2431e+02. (Time: 2.0s)Step 2200 of 1000000; Loss: 8.2485e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.3353e+02; Test Loss: 8.1951e+02. (Time: 2.0s)Step 2400 of 1000000; Loss: 8.2013e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.4233e+01; Test Loss: 8.1417e+02. (Time: 3.1s)Step 2600 of 1000000; Loss: 8.1467e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.4920e+00; Test Loss: 8.0878e+02. (Time: 2.1s)Step 2800 of 1000000; Loss: 8.0943e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0768e+03; Test Loss: 8.0389e+02. (Time: 1.9s)Step 3000 of 1000000; Loss: 8.0449e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3895e+03; Test Loss: 7.9892e+02. (Time: 1.9s)Step 3200 of 1000000; Loss: 7.9956e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.9473e+02; Test Loss: 7.9454e+02. (Time: 2.0s)Step 3400 of 1000000; Loss: 7.9501e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.1756e+01; Test Loss: 7.9084e+02. (Time: 2.1s)Step 3600 of 1000000; Loss: 7.9126e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1041e+00; Test Loss: 7.8739e+02. (Time: 3.1s)Step 3800 of 1000000; Loss: 7.8777e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9996e+03; Test Loss: 7.8443e+02. (Time: 2.0s)Step 4000 of 1000000; Loss: 7.8477e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3408e+03; Test Loss: 7.8206e+02. (Time: 1.9s)Step 4200 of 1000000; Loss: 7.8233e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.6726e+02; Test Loss: 7.7982e+02. (Time: 1.9s)Step 4400 of 1000000; Loss: 7.8006e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5408e+01; Test Loss: 7.7815e+02. (Time: 2.1s)Step 4600 of 1000000; Loss: 7.7834e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.8271e+00; Test Loss: 7.7620e+02. (Time: 2.3s)Step 4800 of 1000000; Loss: 7.7651e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9650e+03; Test Loss: 7.7406e+02. (Time: 3.1s)Step 5000 of 1000000; Loss: 7.7430e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3275e+03; Test Loss: 7.7264e+02. (Time: 1.9s)Step 5200 of 1000000; Loss: 7.7280e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.5173e+02; Test Loss: 7.7122e+02. (Time: 1.9s)Step 5400 of 1000000; Loss: 7.7137e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6347e+01; Test Loss: 7.7020e+02. (Time: 2.1s)Step 5600 of 1000000; Loss: 7.7031e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0975e+00; Test Loss: 7.6937e+02. (Time: 2.0s)Step 5800 of 1000000; Loss: 7.6946e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9515e+03; Test Loss: 7.6853e+02. (Time: 2.3s)Step 6000 of 1000000; Loss: 7.6860e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3248e+03; Test Loss: 7.6859e+02. (Time: 2.8s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3650\n",
            "Running configuration: hidden_size=4, lr=0.0001, weight_decay=0.001, batch_size=32\n",
            "Step 200 of 200; Loss: 1.6613e+03; Test Loss: 9.1322e+01. (Time: 2.2s)Step 200 of 1000000; Loss: 8.6920e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3299e+03; Test Loss: 8.0096e+01. (Time: 2.2s)Step 400 of 1000000; Loss: 8.1599e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5465e+03; Test Loss: 7.2032e+01. (Time: 2.1s)Step 600 of 1000000; Loss: 7.8017e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6134e+03; Test Loss: 6.5331e+01. (Time: 2.2s)Step 800 of 1000000; Loss: 7.5131e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0156e+03; Test Loss: 6.0407e+01. (Time: 3.4s)Step 1000 of 1000000; Loss: 7.3031e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5479e+03; Test Loss: 5.6742e+01. (Time: 2.1s)Step 1200 of 1000000; Loss: 7.1506e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6027e+03; Test Loss: 5.3781e+01. (Time: 2.3s)Step 1400 of 1000000; Loss: 7.0260e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.6829e+02; Test Loss: 5.1382e+01. (Time: 2.2s)Step 1600 of 1000000; Loss: 6.9186e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5411e+03; Test Loss: 4.9196e+01. (Time: 2.2s)Step 1800 of 1000000; Loss: 6.8186e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5842e+03; Test Loss: 4.7099e+01. (Time: 2.8s)Step 2000 of 1000000; Loss: 6.7198e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.7523e+02; Test Loss: 4.5778e+01. (Time: 2.8s)Step 2200 of 1000000; Loss: 6.6503e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5157e+03; Test Loss: 4.4488e+01. (Time: 2.1s)Step 2400 of 1000000; Loss: 6.5849e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5595e+03; Test Loss: 4.3474e+01. (Time: 2.2s)Step 2600 of 1000000; Loss: 6.5225e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.2175e+02; Test Loss: 4.2526e+01. (Time: 2.2s)Step 2800 of 1000000; Loss: 6.4590e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4690e+03; Test Loss: 4.1618e+01. (Time: 2.3s)Step 3000 of 1000000; Loss: 6.3923e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5219e+03; Test Loss: 4.0683e+01. (Time: 3.1s)Step 3200 of 1000000; Loss: 6.3204e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.7180e+02; Test Loss: 3.9718e+01. (Time: 2.2s)Step 3400 of 1000000; Loss: 6.2418e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4005e+03; Test Loss: 3.8750e+01. (Time: 2.4s)Step 3600 of 1000000; Loss: 6.1580e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4659e+03; Test Loss: 3.7776e+01. (Time: 2.3s)Step 3800 of 1000000; Loss: 6.0666e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.2380e+02; Test Loss: 3.6822e+01. (Time: 2.1s)Step 4000 of 1000000; Loss: 5.9599e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2811e+03; Test Loss: 3.5766e+01. (Time: 3.3s)Step 4200 of 1000000; Loss: 5.8448e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3770e+03; Test Loss: 3.4522e+01. (Time: 2.1s)Step 4400 of 1000000; Loss: 5.7188e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5209e+02; Test Loss: 3.2550e+01. (Time: 2.4s)Step 4600 of 1000000; Loss: 5.5791e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1795e+03; Test Loss: 2.9309e+01. (Time: 2.3s)Step 4800 of 1000000; Loss: 5.4266e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3400e+03; Test Loss: 2.1474e+01. (Time: 2.2s)Step 5000 of 1000000; Loss: 5.1355e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4270e+02; Test Loss: 3.9396e+00. (Time: 3.1s)Step 5200 of 1000000; Loss: 4.4862e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1523e+03; Test Loss: 3.7413e+00. (Time: 2.4s)Step 5400 of 1000000; Loss: 4.4291e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3279e+03; Test Loss: 3.6671e+00. (Time: 2.4s)Step 5600 of 1000000; Loss: 4.3982e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1900e+02; Test Loss: 3.6071e+00. (Time: 2.3s)Step 5800 of 1000000; Loss: 4.3795e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1337e+03; Test Loss: 3.5489e+00. (Time: 2.2s)Step 6000 of 1000000; Loss: 4.3644e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3150e+03; Test Loss: 3.4905e+00. (Time: 2.8s)Step 6200 of 1000000; Loss: 4.3512e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0855e+02; Test Loss: 3.4312e+00. (Time: 2.9s)Step 6400 of 1000000; Loss: 4.3384e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1166e+03; Test Loss: 3.3706e+00. (Time: 2.3s)Step 6600 of 1000000; Loss: 4.3258e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3026e+03; Test Loss: 3.3094e+00. (Time: 2.2s)Step 6800 of 1000000; Loss: 4.3128e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0061e+02; Test Loss: 3.2476e+00. (Time: 2.1s)Step 7000 of 1000000; Loss: 4.2992e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1002e+03; Test Loss: 3.1851e+00. (Time: 2.3s)Step 7200 of 1000000; Loss: 4.2860e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2910e+03; Test Loss: 3.1227e+00. (Time: 3.3s)Step 7400 of 1000000; Loss: 4.2752e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3438e+01; Test Loss: 3.0602e+00. (Time: 2.3s)Step 7600 of 1000000; Loss: 4.2661e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0855e+03; Test Loss: 2.9979e+00. (Time: 2.1s)Step 7800 of 1000000; Loss: 4.2575e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2804e+03; Test Loss: 2.9363e+00. (Time: 2.1s)Step 8000 of 1000000; Loss: 4.2497e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.6254e+01; Test Loss: 2.8756e+00. (Time: 2.2s)Step 8200 of 1000000; Loss: 4.2421e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0719e+03; Test Loss: 2.8155e+00. (Time: 3.4s)Step 8400 of 1000000; Loss: 4.2350e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2713e+03; Test Loss: 2.7564e+00. (Time: 2.4s)Step 8600 of 1000000; Loss: 4.2284e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.9036e+01; Test Loss: 2.6987e+00. (Time: 2.2s)Step 8800 of 1000000; Loss: 4.2220e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0603e+03; Test Loss: 2.6423e+00. (Time: 2.1s)Step 9000 of 1000000; Loss: 4.2162e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2633e+03; Test Loss: 2.5871e+00. (Time: 7.2s)Step 9200 of 1000000; Loss: 4.2108e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.2516e+01; Test Loss: 2.5332e+00. (Time: 2.2s)Step 9400 of 1000000; Loss: 4.2058e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0505e+03; Test Loss: 2.4804e+00. (Time: 2.1s)Step 9600 of 1000000; Loss: 4.2011e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2561e+03; Test Loss: 2.4289e+00. (Time: 2.4s)Step 9800 of 1000000; Loss: 4.1966e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.6852e+01; Test Loss: 2.3787e+00. (Time: 2.3s)Step 10000 of 1000000; Loss: 4.1920e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0443e+03; Test Loss: 2.3297e+00. (Time: 3.6s)Step 10200 of 1000000; Loss: 4.1877e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2500e+03; Test Loss: 2.2817e+00. (Time: 2.3s)Step 10400 of 1000000; Loss: 4.1840e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.1402e+01; Test Loss: 2.2347e+00. (Time: 2.1s)Step 10600 of 1000000; Loss: 4.1797e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0361e+03; Test Loss: 2.1891e+00. (Time: 2.2s)Step 10800 of 1000000; Loss: 4.1758e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2449e+03; Test Loss: 2.1449e+00. (Time: 2.1s)Step 11000 of 1000000; Loss: 4.1718e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.6105e+01; Test Loss: 2.1021e+00. (Time: 3.0s)Step 11200 of 1000000; Loss: 4.1681e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0295e+03; Test Loss: 2.0599e+00. (Time: 2.7s)Step 11400 of 1000000; Loss: 4.1640e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2405e+03; Test Loss: 2.0188e+00. (Time: 2.1s)Step 11600 of 1000000; Loss: 4.1602e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.0484e+01; Test Loss: 1.9789e+00. (Time: 2.3s)Step 11800 of 1000000; Loss: 4.1563e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0236e+03; Test Loss: 1.9400e+00. (Time: 2.3s)Step 12000 of 1000000; Loss: 4.1525e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2364e+03; Test Loss: 1.9022e+00. (Time: 2.7s)Step 12200 of 1000000; Loss: 4.1486e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.6382e+01; Test Loss: 1.8657e+00. (Time: 3.1s)Step 12400 of 1000000; Loss: 4.1447e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0182e+03; Test Loss: 1.8298e+00. (Time: 2.3s)Step 12600 of 1000000; Loss: 4.1407e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2330e+03; Test Loss: 1.7945e+00. (Time: 2.3s)Step 12800 of 1000000; Loss: 4.1366e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.3423e+01; Test Loss: 1.7604e+00. (Time: 2.2s)Step 13000 of 1000000; Loss: 4.1321e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0145e+03; Test Loss: 1.7270e+00. (Time: 2.4s)Step 13200 of 1000000; Loss: 4.1276e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2300e+03; Test Loss: 1.6950e+00. (Time: 3.3s)Step 13400 of 1000000; Loss: 4.1227e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.0625e+01; Test Loss: 1.6640e+00. (Time: 2.2s)Step 13600 of 1000000; Loss: 4.1178e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0094e+03; Test Loss: 1.6335e+00. (Time: 2.3s)Step 13800 of 1000000; Loss: 4.1130e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2259e+03; Test Loss: 1.6034e+00. (Time: 2.1s)Step 14000 of 1000000; Loss: 4.1077e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.8215e+01; Test Loss: 1.5744e+00. (Time: 2.1s)Step 14200 of 1000000; Loss: 4.1027e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0048e+03; Test Loss: 1.5456e+00. (Time: 3.3s)Step 14400 of 1000000; Loss: 4.0968e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2226e+03; Test Loss: 1.5177e+00. (Time: 2.3s)Step 14600 of 1000000; Loss: 4.0906e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5734e+01; Test Loss: 1.4904e+00. (Time: 2.4s)Step 14800 of 1000000; Loss: 4.0847e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.9983e+02; Test Loss: 1.4645e+00. (Time: 2.3s)Step 15000 of 1000000; Loss: 4.0801e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2197e+03; Test Loss: 1.4398e+00. (Time: 2.2s)Step 15200 of 1000000; Loss: 4.0754e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2431e+01; Test Loss: 1.4168e+00. (Time: 3.1s)Step 15400 of 1000000; Loss: 4.0720e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.9557e+02; Test Loss: 1.3964e+00. (Time: 2.5s)Step 15600 of 1000000; Loss: 4.0687e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2169e+03; Test Loss: 1.3774e+00. (Time: 2.2s)Step 15800 of 1000000; Loss: 4.0655e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4472e+01; Test Loss: 1.3586e+00. (Time: 2.4s)Step 16000 of 1000000; Loss: 4.0623e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.9136e+02; Test Loss: 1.3409e+00. (Time: 2.3s)Step 16200 of 1000000; Loss: 4.0586e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2139e+03; Test Loss: 1.3240e+00. (Time: 3.1s)Step 16400 of 1000000; Loss: 4.0549e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3666e+01; Test Loss: 1.3077e+00. (Time: 2.8s)Step 16600 of 1000000; Loss: 4.0510e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.8688e+02; Test Loss: 1.2918e+00. (Time: 2.1s)Step 16800 of 1000000; Loss: 4.0470e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2107e+03; Test Loss: 1.2764e+00. (Time: 2.3s)Step 17000 of 1000000; Loss: 4.0430e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2946e+01; Test Loss: 1.2611e+00. (Time: 2.2s)Step 17200 of 1000000; Loss: 4.0390e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.8189e+02; Test Loss: 1.2457e+00. (Time: 2.4s)Step 17400 of 1000000; Loss: 4.0352e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2078e+03; Test Loss: 1.2303e+00. (Time: 3.1s)Step 17600 of 1000000; Loss: 4.0311e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2224e+01; Test Loss: 1.2146e+00. (Time: 2.3s)Step 17800 of 1000000; Loss: 4.0265e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.7661e+02; Test Loss: 1.1985e+00. (Time: 2.2s)Step 18000 of 1000000; Loss: 4.0212e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2044e+03; Test Loss: 1.1825e+00. (Time: 2.3s)Step 18200 of 1000000; Loss: 4.0161e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1465e+01; Test Loss: 1.1666e+00. (Time: 2.2s)Step 18400 of 1000000; Loss: 4.0112e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.7108e+02; Test Loss: 1.1508e+00. (Time: 3.7s)Step 18600 of 1000000; Loss: 4.0061e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2007e+03; Test Loss: 1.1356e+00. (Time: 2.4s)Step 18800 of 1000000; Loss: 4.0015e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0760e+01; Test Loss: 1.1207e+00. (Time: 2.2s)Step 19000 of 1000000; Loss: 3.9966e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6535e+02; Test Loss: 1.1057e+00. (Time: 2.3s)Step 19200 of 1000000; Loss: 3.9920e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1965e+03; Test Loss: 1.0905e+00. (Time: 2.2s)Step 19400 of 1000000; Loss: 3.9868e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0072e+01; Test Loss: 1.0760e+00. (Time: 3.7s)Step 19600 of 1000000; Loss: 3.9839e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6071e+02; Test Loss: 1.0610e+00. (Time: 2.5s)Step 19800 of 1000000; Loss: 3.9812e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1936e+03; Test Loss: 1.0460e+00. (Time: 2.2s)Step 20000 of 1000000; Loss: 3.9786e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9332e+01; Test Loss: 1.0312e+00. (Time: 2.2s)Step 20200 of 1000000; Loss: 3.9762e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5706e+02; Test Loss: 1.0164e+00. (Time: 2.3s)Step 20400 of 1000000; Loss: 3.9739e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1913e+03; Test Loss: 1.0015e+00. (Time: 3.5s)Step 20600 of 1000000; Loss: 3.9718e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8590e+01; Test Loss: 9.8700e-01. (Time: 2.2s)Step 20800 of 1000000; Loss: 3.9691e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5314e+02; Test Loss: 9.7232e-01. (Time: 2.4s)Step 21000 of 1000000; Loss: 3.9663e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1889e+03; Test Loss: 9.5744e-01. (Time: 2.2s)Step 21200 of 1000000; Loss: 3.9650e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7827e+01; Test Loss: 9.4279e-01. (Time: 2.3s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3900\n",
            "Running configuration: hidden_size=4, lr=0.0001, weight_decay=0.001, batch_size=64\n",
            "Step 200 of 200; Loss: 3.2944e+03; Test Loss: 1.7245e+03. (Time: 3.0s)Step 200 of 1000000; Loss: 1.7403e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9238e+03; Test Loss: 1.6250e+03. (Time: 1.9s)Step 400 of 1000000; Loss: 1.6352e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4029e+03; Test Loss: 1.5562e+03. (Time: 2.0s)Step 600 of 1000000; Loss: 1.5637e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.8869e+02; Test Loss: 1.5000e+03. (Time: 2.1s)Step 800 of 1000000; Loss: 1.5062e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1291e+03; Test Loss: 1.4597e+03. (Time: 2.1s)Step 1000 of 1000000; Loss: 1.4639e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2159e+03; Test Loss: 1.4297e+03. (Time: 2.4s)Step 1200 of 1000000; Loss: 1.4330e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3498e+03; Test Loss: 1.4055e+03. (Time: 2.9s)Step 1400 of 1000000; Loss: 1.4083e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7363e+03; Test Loss: 1.3844e+03. (Time: 2.0s)Step 1600 of 1000000; Loss: 1.3869e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9483e+02; Test Loss: 1.3643e+03. (Time: 2.1s)Step 1800 of 1000000; Loss: 1.3667e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1004e+03; Test Loss: 1.3445e+03. (Time: 1.9s)Step 2000 of 1000000; Loss: 1.3470e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1774e+03; Test Loss: 1.3295e+03. (Time: 2.0s)Step 2200 of 1000000; Loss: 1.3311e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1154e+03; Test Loss: 1.3166e+03. (Time: 2.5s)Step 2400 of 1000000; Loss: 1.3181e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4691e+03; Test Loss: 1.3043e+03. (Time: 2.7s)Step 2600 of 1000000; Loss: 1.3058e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5451e+02; Test Loss: 1.2915e+03. (Time: 2.1s)Step 2800 of 1000000; Loss: 1.2931e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9788e+03; Test Loss: 1.2779e+03. (Time: 2.0s)Step 3000 of 1000000; Loss: 1.2796e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0693e+03; Test Loss: 1.2632e+03. (Time: 2.0s)Step 3200 of 1000000; Loss: 1.2651e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9476e+03; Test Loss: 1.2472e+03. (Time: 1.9s)Step 3400 of 1000000; Loss: 1.2492e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3032e+03; Test Loss: 1.2302e+03. (Time: 2.7s)Step 3600 of 1000000; Loss: 1.2324e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2384e+02; Test Loss: 1.2113e+03. (Time: 2.4s)Step 3800 of 1000000; Loss: 1.2138e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7148e+03; Test Loss: 1.1892e+03. (Time: 1.9s)Step 4000 of 1000000; Loss: 1.1920e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.8295e+03; Test Loss: 1.1653e+03. (Time: 2.0s)Step 4200 of 1000000; Loss: 1.1684e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7084e+03; Test Loss: 1.1387e+03. (Time: 2.0s)Step 4400 of 1000000; Loss: 1.1422e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0750e+03; Test Loss: 1.1079e+03. (Time: 1.9s)Step 4600 of 1000000; Loss: 1.1118e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6322e+02; Test Loss: 1.0688e+03. (Time: 2.5s)Step 4800 of 1000000; Loss: 1.0749e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4398e+03; Test Loss: 9.4388e+02. (Time: 2.7s)Step 5000 of 1000000; Loss: 9.7258e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6700e+03; Test Loss: 8.9219e+02. (Time: 1.9s)Step 5200 of 1000000; Loss: 8.9345e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0593e+03; Test Loss: 8.8376e+02. (Time: 1.9s)Step 5400 of 1000000; Loss: 8.8453e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1250e+02; Test Loss: 8.7922e+02. (Time: 1.9s)Step 5600 of 1000000; Loss: 8.7968e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0544e+01; Test Loss: 8.7591e+02. (Time: 2.0s)Step 5800 of 1000000; Loss: 8.7627e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3732e+03; Test Loss: 8.7311e+02. (Time: 2.3s)Step 6000 of 1000000; Loss: 8.7344e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6257e+03; Test Loss: 8.7048e+02. (Time: 3.0s)Step 6200 of 1000000; Loss: 8.7078e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0099e+03; Test Loss: 8.6794e+02. (Time: 2.0s)Step 6400 of 1000000; Loss: 8.6825e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8377e+02; Test Loss: 8.6522e+02. (Time: 2.1s)Step 6600 of 1000000; Loss: 8.6557e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6604e+01; Test Loss: 8.6228e+02. (Time: 2.0s)Step 6800 of 1000000; Loss: 8.6266e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3216e+03; Test Loss: 8.5944e+02. (Time: 2.0s)Step 7000 of 1000000; Loss: 8.5977e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5869e+03; Test Loss: 8.5718e+02. (Time: 2.5s)Step 7200 of 1000000; Loss: 8.5743e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6484e+02; Test Loss: 8.5522e+02. (Time: 2.6s)Step 7400 of 1000000; Loss: 8.5545e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6149e+02; Test Loss: 8.5339e+02. (Time: 2.1s)Step 7600 of 1000000; Loss: 8.5362e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1070e+01; Test Loss: 8.5159e+02. (Time: 2.0s)Step 7800 of 1000000; Loss: 8.5180e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2753e+03; Test Loss: 8.4998e+02. (Time: 2.0s)Step 8000 of 1000000; Loss: 8.5017e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5541e+03; Test Loss: 8.4841e+02. (Time: 2.0s)Step 8200 of 1000000; Loss: 8.4861e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.1990e+02; Test Loss: 8.4691e+02. (Time: 2.9s)Step 8400 of 1000000; Loss: 8.4708e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3912e+02; Test Loss: 8.4546e+02. (Time: 2.6s)Step 8600 of 1000000; Loss: 8.4563e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6798e+00; Test Loss: 8.4418e+02. (Time: 2.0s)Step 8800 of 1000000; Loss: 8.4433e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2363e+03; Test Loss: 8.4297e+02. (Time: 2.0s)Step 9000 of 1000000; Loss: 8.4312e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5249e+03; Test Loss: 8.4178e+02. (Time: 2.0s)Step 9200 of 1000000; Loss: 8.4193e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9144e+02; Test Loss: 8.4060e+02. (Time: 2.0s)Step 9400 of 1000000; Loss: 8.4075e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2186e+02; Test Loss: 8.3936e+02. (Time: 3.1s)Step 9600 of 1000000; Loss: 8.3952e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.7540e+00; Test Loss: 8.3825e+02. (Time: 2.2s)Step 9800 of 1000000; Loss: 8.3838e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2067e+03; Test Loss: 8.3712e+02. (Time: 2.0s)Step 10000 of 1000000; Loss: 8.3723e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5002e+03; Test Loss: 8.3608e+02. (Time: 2.1s)Step 10200 of 1000000; Loss: 8.3622e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.6948e+02; Test Loss: 8.3518e+02. (Time: 2.1s)Step 10400 of 1000000; Loss: 8.3527e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0514e+02; Test Loss: 8.3432e+02. (Time: 2.1s)Step 10600 of 1000000; Loss: 8.3441e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.8020e+00; Test Loss: 8.3348e+02. (Time: 3.0s)Step 10800 of 1000000; Loss: 8.3357e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1799e+03; Test Loss: 8.3265e+02. (Time: 2.1s)Step 11000 of 1000000; Loss: 8.3275e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4822e+03; Test Loss: 8.3179e+02. (Time: 2.0s)Step 11200 of 1000000; Loss: 8.3191e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.5181e+02; Test Loss: 8.3110e+02. (Time: 2.1s)Step 11400 of 1000000; Loss: 8.3113e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.1896e+01; Test Loss: 8.3017e+02. (Time: 2.0s)Step 11600 of 1000000; Loss: 8.3026e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.9217e+00; Test Loss: 8.2935e+02. (Time: 2.4s)Step 11800 of 1000000; Loss: 8.2945e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1591e+03; Test Loss: 8.2853e+02. (Time: 2.7s)Step 12000 of 1000000; Loss: 8.2863e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4668e+03; Test Loss: 8.2767e+02. (Time: 1.9s)Step 12200 of 1000000; Loss: 8.2778e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.3768e+02; Test Loss: 8.2679e+02. (Time: 1.9s)Step 12400 of 1000000; Loss: 8.2691e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.1706e+01; Test Loss: 8.2587e+02. (Time: 2.0s)Step 12600 of 1000000; Loss: 8.2600e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.0941e+00; Test Loss: 8.2482e+02. (Time: 2.2s)Step 12800 of 1000000; Loss: 8.2496e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1391e+03; Test Loss: 8.2377e+02. (Time: 2.5s)Step 13000 of 1000000; Loss: 8.2390e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4527e+03; Test Loss: 8.2269e+02. (Time: 2.6s)Step 13200 of 1000000; Loss: 8.2283e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.2588e+02; Test Loss: 8.2159e+02. (Time: 2.0s)Step 13400 of 1000000; Loss: 8.2172e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.2673e+01; Test Loss: 8.2047e+02. (Time: 2.0s)Step 13600 of 1000000; Loss: 8.2062e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3348e+00; Test Loss: 8.1896e+02. (Time: 2.0s)Step 13800 of 1000000; Loss: 8.1924e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1189e+03; Test Loss: 8.1771e+02. (Time: 2.0s)Step 14000 of 1000000; Loss: 8.1786e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4390e+03; Test Loss: 8.1659e+02. (Time: 6.4s)Step 14200 of 1000000; Loss: 8.1671e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.1471e+02; Test Loss: 8.1588e+02. (Time: 2.0s)Step 14400 of 1000000; Loss: 8.1598e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.9734e+01; Test Loss: 8.1529e+02. (Time: 1.9s)Step 14600 of 1000000; Loss: 8.1536e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.6307e+00; Test Loss: 8.1476e+02. (Time: 2.0s)Step 14800 of 1000000; Loss: 8.1483e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1006e+03; Test Loss: 8.1414e+02. (Time: 2.8s)Step 15000 of 1000000; Loss: 8.1422e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4277e+03; Test Loss: 8.1338e+02. (Time: 2.6s)Step 15200 of 1000000; Loss: 8.1347e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.0457e+02; Test Loss: 8.1262e+02. (Time: 2.0s)Step 15400 of 1000000; Loss: 8.1271e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2969e+01; Test Loss: 8.1188e+02. (Time: 2.1s)Step 15600 of 1000000; Loss: 8.1197e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.2377e+00; Test Loss: 8.1109e+02. (Time: 2.1s)Step 15800 of 1000000; Loss: 8.1118e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0793e+03; Test Loss: 8.1025e+02. (Time: 2.1s)Step 16000 of 1000000; Loss: 8.1036e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4139e+03; Test Loss: 8.0942e+02. (Time: 3.1s)Step 16200 of 1000000; Loss: 8.0953e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.9741e+02; Test Loss: 8.0857e+02. (Time: 2.3s)Step 16400 of 1000000; Loss: 8.0867e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.9129e+01; Test Loss: 8.0772e+02. (Time: 2.3s)Step 16600 of 1000000; Loss: 8.0782e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.8623e+00; Test Loss: 8.0658e+02. (Time: 2.1s)Step 16800 of 1000000; Loss: 8.0670e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0576e+03; Test Loss: 8.0557e+02. (Time: 2.0s)Step 17000 of 1000000; Loss: 8.0569e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4001e+03; Test Loss: 8.0442e+02. (Time: 2.3s)Step 17200 of 1000000; Loss: 8.0458e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.8980e+02; Test Loss: 8.0333e+02. (Time: 2.8s)Step 17400 of 1000000; Loss: 8.0346e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5350e+01; Test Loss: 8.0228e+02. (Time: 1.9s)Step 17600 of 1000000; Loss: 8.0242e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5479e+00; Test Loss: 8.0122e+02. (Time: 2.1s)Step 17800 of 1000000; Loss: 8.0134e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0362e+03; Test Loss: 8.0018e+02. (Time: 2.0s)Step 18000 of 1000000; Loss: 8.0031e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3837e+03; Test Loss: 7.9924e+02. (Time: 2.1s)Step 18200 of 1000000; Loss: 7.9936e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.8265e+02; Test Loss: 7.9829e+02. (Time: 2.4s)Step 18400 of 1000000; Loss: 7.9836e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1893e+01; Test Loss: 7.9759e+02. (Time: 2.9s)Step 18600 of 1000000; Loss: 7.9767e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2646e+00; Test Loss: 7.9693e+02. (Time: 2.1s)Step 18800 of 1000000; Loss: 7.9700e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0192e+03; Test Loss: 7.9636e+02. (Time: 2.1s)Step 19000 of 1000000; Loss: 7.9641e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3707e+03; Test Loss: 7.9583e+02. (Time: 2.1s)Step 19200 of 1000000; Loss: 7.9590e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.7592e+02; Test Loss: 7.9521e+02. (Time: 2.1s)Step 19400 of 1000000; Loss: 7.9529e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.8338e+01; Test Loss: 7.9464e+02. (Time: 3.3s)Step 19600 of 1000000; Loss: 7.9471e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9472e+00; Test Loss: 7.9408e+02. (Time: 2.0s)Step 19800 of 1000000; Loss: 7.9414e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0070e+03; Test Loss: 7.9350e+02. (Time: 2.1s)Step 20000 of 1000000; Loss: 7.9356e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3608e+03; Test Loss: 7.9306e+02. (Time: 1.9s)Step 20200 of 1000000; Loss: 7.9313e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.6966e+02; Test Loss: 7.9261e+02. (Time: 2.1s)Step 20400 of 1000000; Loss: 7.9267e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5142e+01; Test Loss: 7.9223e+02. (Time: 2.0s)Step 20600 of 1000000; Loss: 7.9226e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6639e+00; Test Loss: 7.9189e+02. (Time: 3.2s)Step 20800 of 1000000; Loss: 7.9191e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9958e+03; Test Loss: 7.9150e+02. (Time: 1.9s)Step 21000 of 1000000; Loss: 7.9155e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3532e+03; Test Loss: 7.9120e+02. (Time: 1.9s)Step 21200 of 1000000; Loss: 7.9125e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.6062e+02; Test Loss: 7.9098e+02. (Time: 2.1s)Step 21400 of 1000000; Loss: 7.9094e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2343e+01; Test Loss: 7.9079e+02. (Time: 2.1s)Step 21600 of 1000000; Loss: 7.9081e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4699e+00; Test Loss: 7.9076e+02. (Time: 2.3s)Step 21800 of 1000000; Loss: 7.9075e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9818e+03; Test Loss: 7.9125e+02. (Time: 2.9s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3757\n",
            "Running configuration: hidden_size=4, lr=0.0001, weight_decay=0.0001, batch_size=32\n",
            "Step 200 of 200; Loss: 1.6613e+03; Test Loss: 9.1322e+01. (Time: 2.3s)Step 200 of 1000000; Loss: 8.6920e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3299e+03; Test Loss: 8.0096e+01. (Time: 2.2s)Step 400 of 1000000; Loss: 8.1599e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5465e+03; Test Loss: 7.2032e+01. (Time: 2.4s)Step 600 of 1000000; Loss: 7.8017e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6134e+03; Test Loss: 6.5331e+01. (Time: 2.5s)Step 800 of 1000000; Loss: 7.5131e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0156e+03; Test Loss: 6.0407e+01. (Time: 3.4s)Step 1000 of 1000000; Loss: 7.3031e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5479e+03; Test Loss: 5.6742e+01. (Time: 2.3s)Step 1200 of 1000000; Loss: 7.1506e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6027e+03; Test Loss: 5.3781e+01. (Time: 2.2s)Step 1400 of 1000000; Loss: 7.0260e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.6829e+02; Test Loss: 5.1382e+01. (Time: 2.4s)Step 1600 of 1000000; Loss: 6.9186e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5411e+03; Test Loss: 4.9196e+01. (Time: 2.3s)Step 1800 of 1000000; Loss: 6.8186e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5842e+03; Test Loss: 4.7099e+01. (Time: 3.4s)Step 2000 of 1000000; Loss: 6.7198e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.7523e+02; Test Loss: 4.5778e+01. (Time: 2.3s)Step 2200 of 1000000; Loss: 6.6503e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5157e+03; Test Loss: 4.4488e+01. (Time: 2.4s)Step 2400 of 1000000; Loss: 6.5849e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5595e+03; Test Loss: 4.3474e+01. (Time: 2.3s)Step 2600 of 1000000; Loss: 6.5225e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.2176e+02; Test Loss: 4.2526e+01. (Time: 2.3s)Step 2800 of 1000000; Loss: 6.4590e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4690e+03; Test Loss: 4.1618e+01. (Time: 3.4s)Step 3000 of 1000000; Loss: 6.3923e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5219e+03; Test Loss: 4.0683e+01. (Time: 2.3s)Step 3200 of 1000000; Loss: 6.3204e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.7179e+02; Test Loss: 3.9718e+01. (Time: 2.2s)Step 3400 of 1000000; Loss: 6.2419e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4006e+03; Test Loss: 3.8750e+01. (Time: 2.1s)Step 3600 of 1000000; Loss: 6.1581e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4659e+03; Test Loss: 3.7780e+01. (Time: 2.3s)Step 3800 of 1000000; Loss: 6.0667e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.2381e+02; Test Loss: 3.6822e+01. (Time: 3.6s)Step 4000 of 1000000; Loss: 5.9601e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2812e+03; Test Loss: 3.5766e+01. (Time: 2.2s)Step 4200 of 1000000; Loss: 5.8451e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3770e+03; Test Loss: 3.4523e+01. (Time: 2.1s)Step 4400 of 1000000; Loss: 5.7191e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5217e+02; Test Loss: 3.2554e+01. (Time: 2.5s)Step 4600 of 1000000; Loss: 5.5793e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1795e+03; Test Loss: 2.9319e+01. (Time: 2.4s)Step 4800 of 1000000; Loss: 5.4270e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3400e+03; Test Loss: 2.1519e+01. (Time: 3.4s)Step 5000 of 1000000; Loss: 5.1373e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4287e+02; Test Loss: 3.9418e+00. (Time: 2.4s)Step 5200 of 1000000; Loss: 4.4866e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1525e+03; Test Loss: 3.7418e+00. (Time: 2.3s)Step 5400 of 1000000; Loss: 4.4293e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3279e+03; Test Loss: 3.6671e+00. (Time: 2.2s)Step 5600 of 1000000; Loss: 4.3982e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1899e+02; Test Loss: 3.6071e+00. (Time: 2.3s)Step 5800 of 1000000; Loss: 4.3795e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1338e+03; Test Loss: 3.5490e+00. (Time: 3.1s)Step 6000 of 1000000; Loss: 4.3644e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3150e+03; Test Loss: 3.4906e+00. (Time: 2.9s)Step 6200 of 1000000; Loss: 4.3512e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0855e+02; Test Loss: 3.4313e+00. (Time: 2.3s)Step 6400 of 1000000; Loss: 4.3385e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1167e+03; Test Loss: 3.3708e+00. (Time: 2.5s)Step 6600 of 1000000; Loss: 4.3258e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3027e+03; Test Loss: 3.3095e+00. (Time: 2.3s)Step 6800 of 1000000; Loss: 4.3128e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0061e+02; Test Loss: 3.2477e+00. (Time: 3.0s)Step 7000 of 1000000; Loss: 4.2993e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1003e+03; Test Loss: 3.1853e+00. (Time: 2.6s)Step 7200 of 1000000; Loss: 4.2861e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2910e+03; Test Loss: 3.1229e+00. (Time: 2.4s)Step 7400 of 1000000; Loss: 4.2753e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3442e+01; Test Loss: 3.0604e+00. (Time: 2.2s)Step 7600 of 1000000; Loss: 4.2661e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0855e+03; Test Loss: 2.9981e+00. (Time: 2.3s)Step 7800 of 1000000; Loss: 4.2575e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2804e+03; Test Loss: 2.9365e+00. (Time: 2.9s)Step 8000 of 1000000; Loss: 4.2497e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.6265e+01; Test Loss: 2.8758e+00. (Time: 2.8s)Step 8200 of 1000000; Loss: 4.2421e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0732e+03; Test Loss: 2.8158e+00. (Time: 2.3s)Step 8400 of 1000000; Loss: 4.2351e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2714e+03; Test Loss: 2.7567e+00. (Time: 2.2s)Step 8600 of 1000000; Loss: 4.2285e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.9038e+01; Test Loss: 2.6990e+00. (Time: 2.3s)Step 8800 of 1000000; Loss: 4.2221e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0609e+03; Test Loss: 2.6425e+00. (Time: 2.4s)Step 9000 of 1000000; Loss: 4.2162e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2634e+03; Test Loss: 2.5873e+00. (Time: 3.2s)Step 9200 of 1000000; Loss: 4.2109e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.2551e+01; Test Loss: 2.5334e+00. (Time: 2.2s)Step 9400 of 1000000; Loss: 4.2059e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0504e+03; Test Loss: 2.4807e+00. (Time: 2.2s)Step 9600 of 1000000; Loss: 4.2012e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2563e+03; Test Loss: 2.4291e+00. (Time: 2.2s)Step 9800 of 1000000; Loss: 4.1965e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.6826e+01; Test Loss: 2.3791e+00. (Time: 2.1s)Step 10000 of 1000000; Loss: 4.1920e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0421e+03; Test Loss: 2.3299e+00. (Time: 3.6s)Step 10200 of 1000000; Loss: 4.1878e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2500e+03; Test Loss: 2.2816e+00. (Time: 2.4s)Step 10400 of 1000000; Loss: 4.1839e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.1413e+01; Test Loss: 2.2348e+00. (Time: 2.4s)Step 10600 of 1000000; Loss: 4.1796e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0350e+03; Test Loss: 2.1893e+00. (Time: 2.2s)Step 10800 of 1000000; Loss: 4.1757e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2447e+03; Test Loss: 2.1450e+00. (Time: 2.3s)Step 11000 of 1000000; Loss: 4.1716e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5975e+01; Test Loss: 2.1021e+00. (Time: 3.2s)Step 11200 of 1000000; Loss: 4.1677e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0295e+03; Test Loss: 2.0598e+00. (Time: 2.6s)Step 11400 of 1000000; Loss: 4.1638e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2405e+03; Test Loss: 2.0186e+00. (Time: 2.4s)Step 11600 of 1000000; Loss: 4.1600e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.9924e+01; Test Loss: 1.9787e+00. (Time: 2.4s)Step 11800 of 1000000; Loss: 4.1560e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0236e+03; Test Loss: 1.9398e+00. (Time: 2.1s)Step 12000 of 1000000; Loss: 4.1522e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2364e+03; Test Loss: 1.9019e+00. (Time: 2.8s)Step 12200 of 1000000; Loss: 4.1482e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.6331e+01; Test Loss: 1.8654e+00. (Time: 2.9s)Step 12400 of 1000000; Loss: 4.1443e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0182e+03; Test Loss: 1.8294e+00. (Time: 2.5s)Step 12600 of 1000000; Loss: 4.1403e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2329e+03; Test Loss: 1.7940e+00. (Time: 2.3s)Step 12800 of 1000000; Loss: 4.1361e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.3354e+01; Test Loss: 1.7599e+00. (Time: 2.1s)Step 13000 of 1000000; Loss: 4.1316e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0133e+03; Test Loss: 1.7266e+00. (Time: 3.0s)Step 13200 of 1000000; Loss: 4.1274e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2294e+03; Test Loss: 1.6941e+00. (Time: 3.0s)Step 13400 of 1000000; Loss: 4.1228e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.0644e+01; Test Loss: 1.6632e+00. (Time: 2.5s)Step 13600 of 1000000; Loss: 4.1185e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0096e+03; Test Loss: 1.6329e+00. (Time: 2.3s)Step 13800 of 1000000; Loss: 4.1138e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2262e+03; Test Loss: 1.6030e+00. (Time: 2.3s)Step 14000 of 1000000; Loss: 4.1094e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.8259e+01; Test Loss: 1.5741e+00. (Time: 2.7s)Step 14200 of 1000000; Loss: 4.1042e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0049e+03; Test Loss: 1.5457e+00. (Time: 2.9s)Step 14400 of 1000000; Loss: 4.0990e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2229e+03; Test Loss: 1.5180e+00. (Time: 2.4s)Step 14600 of 1000000; Loss: 4.0934e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5973e+01; Test Loss: 1.4911e+00. (Time: 2.2s)Step 14800 of 1000000; Loss: 4.0875e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0001e+03; Test Loss: 1.4654e+00. (Time: 2.3s)Step 15000 of 1000000; Loss: 4.0831e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2201e+03; Test Loss: 1.4406e+00. (Time: 2.6s)Step 15200 of 1000000; Loss: 4.0787e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3621e+01; Test Loss: 1.4179e+00. (Time: 3.2s)Step 15400 of 1000000; Loss: 4.0754e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.9603e+02; Test Loss: 1.3969e+00. (Time: 2.2s)Step 15600 of 1000000; Loss: 4.0723e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2174e+03; Test Loss: 1.3771e+00. (Time: 2.3s)Step 15800 of 1000000; Loss: 4.0694e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0433e+01; Test Loss: 1.3583e+00. (Time: 2.4s)Step 16000 of 1000000; Loss: 4.0660e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.9198e+02; Test Loss: 1.3409e+00. (Time: 2.7s)Step 16200 of 1000000; Loss: 4.0618e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2147e+03; Test Loss: 1.3252e+00. (Time: 3.3s)Step 16400 of 1000000; Loss: 4.0579e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3711e+01; Test Loss: 1.3098e+00. (Time: 2.2s)Step 16600 of 1000000; Loss: 4.0541e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.8767e+02; Test Loss: 1.2946e+00. (Time: 2.3s)Step 16800 of 1000000; Loss: 4.0502e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2116e+03; Test Loss: 1.2797e+00. (Time: 2.3s)Step 17000 of 1000000; Loss: 4.0462e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3033e+01; Test Loss: 1.2650e+00. (Time: 2.2s)Step 17200 of 1000000; Loss: 4.0425e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.8291e+02; Test Loss: 1.2500e+00. (Time: 3.7s)Step 17400 of 1000000; Loss: 4.0387e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2088e+03; Test Loss: 1.2351e+00. (Time: 2.3s)Step 17600 of 1000000; Loss: 4.0344e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2353e+01; Test Loss: 1.2200e+00. (Time: 2.2s)Step 17800 of 1000000; Loss: 4.0295e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.7759e+02; Test Loss: 1.2045e+00. (Time: 2.4s)Step 18000 of 1000000; Loss: 4.0246e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2054e+03; Test Loss: 1.1890e+00. (Time: 2.4s)Step 18200 of 1000000; Loss: 4.0196e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1653e+01; Test Loss: 1.1734e+00. (Time: 3.3s)Step 18400 of 1000000; Loss: 4.0146e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.7190e+02; Test Loss: 1.1572e+00. (Time: 2.2s)Step 18600 of 1000000; Loss: 4.0094e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2013e+03; Test Loss: 1.1416e+00. (Time: 2.2s)Step 18800 of 1000000; Loss: 4.0041e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0957e+01; Test Loss: 1.1264e+00. (Time: 2.2s)Step 19000 of 1000000; Loss: 3.9985e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6605e+02; Test Loss: 1.1108e+00. (Time: 2.1s)Step 19200 of 1000000; Loss: 3.9933e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1974e+03; Test Loss: 1.0958e+00. (Time: 3.0s)Step 19400 of 1000000; Loss: 3.9904e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0221e+01; Test Loss: 1.0815e+00. (Time: 2.5s)Step 19600 of 1000000; Loss: 3.9872e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6158e+02; Test Loss: 1.0671e+00. (Time: 2.4s)Step 19800 of 1000000; Loss: 3.9836e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1946e+03; Test Loss: 1.0526e+00. (Time: 2.2s)Step 20000 of 1000000; Loss: 3.9803e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9511e+01; Test Loss: 1.0383e+00. (Time: 7.2s)Step 20200 of 1000000; Loss: 3.9779e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5804e+02; Test Loss: 1.0240e+00. (Time: 2.4s)Step 20400 of 1000000; Loss: 3.9755e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1921e+03; Test Loss: 1.0096e+00. (Time: 2.4s)Step 20600 of 1000000; Loss: 3.9735e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8782e+01; Test Loss: 9.9525e-01. (Time: 2.2s)Step 20800 of 1000000; Loss: 3.9710e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5469e+02; Test Loss: 9.8077e-01. (Time: 2.5s)Step 21000 of 1000000; Loss: 3.9686e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1901e+03; Test Loss: 9.6627e-01. (Time: 3.3s)Step 21200 of 1000000; Loss: 3.9666e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8045e+01; Test Loss: 9.5221e-01. (Time: 2.3s)Step 21400 of 1000000; Loss: 3.9639e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5121e+02; Test Loss: 9.3826e-01. (Time: 2.4s)Step 21600 of 1000000; Loss: 3.9602e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1879e+03; Test Loss: 9.2405e-01. (Time: 2.2s)Step 21800 of 1000000; Loss: 3.9597e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7303e+01; Test Loss: 9.1037e-01. (Time: 2.1s)Step 22000 of 1000000; Loss: 3.9594e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.4741e+02; Test Loss: 8.9710e-01. (Time: 3.6s)Step 22200 of 1000000; Loss: 3.9586e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1863e+03; Test Loss: 8.8448e-01. (Time: 2.3s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3896\n",
            "Running configuration: hidden_size=4, lr=0.0001, weight_decay=0.0001, batch_size=64\n",
            "Step 200 of 200; Loss: 3.2944e+03; Test Loss: 1.7245e+03. (Time: 2.1s)Step 200 of 1000000; Loss: 1.7403e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9238e+03; Test Loss: 1.6250e+03. (Time: 2.2s)Step 400 of 1000000; Loss: 1.6352e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4029e+03; Test Loss: 1.5562e+03. (Time: 2.1s)Step 600 of 1000000; Loss: 1.5637e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.8869e+02; Test Loss: 1.5000e+03. (Time: 3.1s)Step 800 of 1000000; Loss: 1.5062e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1291e+03; Test Loss: 1.4597e+03. (Time: 2.4s)Step 1000 of 1000000; Loss: 1.4639e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2159e+03; Test Loss: 1.4297e+03. (Time: 2.0s)Step 1200 of 1000000; Loss: 1.4330e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3498e+03; Test Loss: 1.4055e+03. (Time: 2.1s)Step 1400 of 1000000; Loss: 1.4083e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7363e+03; Test Loss: 1.3844e+03. (Time: 2.2s)Step 1600 of 1000000; Loss: 1.3869e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9483e+02; Test Loss: 1.3643e+03. (Time: 2.1s)Step 1800 of 1000000; Loss: 1.3667e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1004e+03; Test Loss: 1.3445e+03. (Time: 3.3s)Step 2000 of 1000000; Loss: 1.3470e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1774e+03; Test Loss: 1.3295e+03. (Time: 2.2s)Step 2200 of 1000000; Loss: 1.3311e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1154e+03; Test Loss: 1.3166e+03. (Time: 2.0s)Step 2400 of 1000000; Loss: 1.3181e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4691e+03; Test Loss: 1.3043e+03. (Time: 2.1s)Step 2600 of 1000000; Loss: 1.3058e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5451e+02; Test Loss: 1.2915e+03. (Time: 2.2s)Step 2800 of 1000000; Loss: 1.2931e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9788e+03; Test Loss: 1.2779e+03. (Time: 2.7s)Step 3000 of 1000000; Loss: 1.2796e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0693e+03; Test Loss: 1.2632e+03. (Time: 2.4s)Step 3200 of 1000000; Loss: 1.2650e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9476e+03; Test Loss: 1.2472e+03. (Time: 2.0s)Step 3400 of 1000000; Loss: 1.2492e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3032e+03; Test Loss: 1.2302e+03. (Time: 2.2s)Step 3600 of 1000000; Loss: 1.2324e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2384e+02; Test Loss: 1.2113e+03. (Time: 2.1s)Step 3800 of 1000000; Loss: 1.2138e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7148e+03; Test Loss: 1.1892e+03. (Time: 2.0s)Step 4000 of 1000000; Loss: 1.1920e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.8295e+03; Test Loss: 1.1653e+03. (Time: 3.0s)Step 4200 of 1000000; Loss: 1.1684e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7084e+03; Test Loss: 1.1387e+03. (Time: 2.2s)Step 4400 of 1000000; Loss: 1.1422e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0750e+03; Test Loss: 1.1079e+03. (Time: 2.0s)Step 4600 of 1000000; Loss: 1.1118e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6320e+02; Test Loss: 1.0688e+03. (Time: 2.0s)Step 4800 of 1000000; Loss: 1.0749e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4398e+03; Test Loss: 9.4376e+02. (Time: 2.1s)Step 5000 of 1000000; Loss: 9.7245e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6700e+03; Test Loss: 8.9217e+02. (Time: 2.1s)Step 5200 of 1000000; Loss: 8.9344e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0596e+03; Test Loss: 8.8387e+02. (Time: 3.2s)Step 5400 of 1000000; Loss: 8.8460e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1260e+02; Test Loss: 8.7926e+02. (Time: 2.1s)Step 5600 of 1000000; Loss: 8.7973e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0561e+01; Test Loss: 8.7592e+02. (Time: 1.9s)Step 5800 of 1000000; Loss: 8.7629e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3733e+03; Test Loss: 8.7312e+02. (Time: 2.1s)Step 6000 of 1000000; Loss: 8.7345e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6258e+03; Test Loss: 8.7050e+02. (Time: 1.9s)Step 6200 of 1000000; Loss: 8.7081e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0100e+03; Test Loss: 8.6795e+02. (Time: 2.5s)Step 6400 of 1000000; Loss: 8.6826e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8380e+02; Test Loss: 8.6524e+02. (Time: 2.7s)Step 6600 of 1000000; Loss: 8.6558e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6615e+01; Test Loss: 8.6231e+02. (Time: 1.9s)Step 6800 of 1000000; Loss: 8.6269e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3217e+03; Test Loss: 8.5946e+02. (Time: 2.1s)Step 7000 of 1000000; Loss: 8.5978e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5869e+03; Test Loss: 8.5718e+02. (Time: 2.1s)Step 7200 of 1000000; Loss: 8.5743e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6488e+02; Test Loss: 8.5522e+02. (Time: 1.9s)Step 7400 of 1000000; Loss: 8.5545e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6151e+02; Test Loss: 8.5338e+02. (Time: 2.3s)Step 7600 of 1000000; Loss: 8.5361e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1086e+01; Test Loss: 8.5159e+02. (Time: 2.8s)Step 7800 of 1000000; Loss: 8.5181e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2752e+03; Test Loss: 8.4998e+02. (Time: 2.2s)Step 8000 of 1000000; Loss: 8.5017e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5540e+03; Test Loss: 8.4842e+02. (Time: 2.1s)Step 8200 of 1000000; Loss: 8.4861e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.1999e+02; Test Loss: 8.4689e+02. (Time: 2.1s)Step 8400 of 1000000; Loss: 8.4707e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3911e+02; Test Loss: 8.4548e+02. (Time: 2.1s)Step 8600 of 1000000; Loss: 8.4563e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6786e+00; Test Loss: 8.4419e+02. (Time: 3.1s)Step 8800 of 1000000; Loss: 8.4433e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2363e+03; Test Loss: 8.4298e+02. (Time: 2.2s)Step 9000 of 1000000; Loss: 8.4312e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5249e+03; Test Loss: 8.4178e+02. (Time: 1.9s)Step 9200 of 1000000; Loss: 8.4193e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9145e+02; Test Loss: 8.4061e+02. (Time: 2.2s)Step 9400 of 1000000; Loss: 8.4075e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2189e+02; Test Loss: 8.3939e+02. (Time: 1.9s)Step 9600 of 1000000; Loss: 8.3953e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.7543e+00; Test Loss: 8.3830e+02. (Time: 2.1s)Step 9800 of 1000000; Loss: 8.3839e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2039e+03; Test Loss: 8.3716e+02. (Time: 3.1s)Step 10000 of 1000000; Loss: 8.3725e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5002e+03; Test Loss: 8.3622e+02. (Time: 2.2s)Step 10200 of 1000000; Loss: 8.3633e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.6949e+02; Test Loss: 8.3524e+02. (Time: 2.1s)Step 10400 of 1000000; Loss: 8.3531e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0519e+02; Test Loss: 8.3433e+02. (Time: 2.0s)Step 10600 of 1000000; Loss: 8.3443e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.7990e+00; Test Loss: 8.3350e+02. (Time: 2.1s)Step 10800 of 1000000; Loss: 8.3359e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1802e+03; Test Loss: 8.3266e+02. (Time: 2.8s)Step 11000 of 1000000; Loss: 8.3277e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4826e+03; Test Loss: 8.3184e+02. (Time: 2.6s)Step 11200 of 1000000; Loss: 8.3202e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.5184e+02; Test Loss: 8.3106e+02. (Time: 2.0s)Step 11400 of 1000000; Loss: 8.3118e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.1897e+01; Test Loss: 8.3021e+02. (Time: 1.9s)Step 11600 of 1000000; Loss: 8.3031e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.9191e+00; Test Loss: 8.2940e+02. (Time: 2.0s)Step 11800 of 1000000; Loss: 8.2951e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1593e+03; Test Loss: 8.2859e+02. (Time: 1.9s)Step 12000 of 1000000; Loss: 8.2869e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4669e+03; Test Loss: 8.2775e+02. (Time: 2.7s)Step 12200 of 1000000; Loss: 8.2786e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.3771e+02; Test Loss: 8.2686e+02. (Time: 2.5s)Step 12400 of 1000000; Loss: 8.2698e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.1733e+01; Test Loss: 8.2596e+02. (Time: 2.0s)Step 12600 of 1000000; Loss: 8.2608e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.0949e+00; Test Loss: 8.2491e+02. (Time: 2.1s)Step 12800 of 1000000; Loss: 8.2505e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1393e+03; Test Loss: 8.2386e+02. (Time: 2.0s)Step 13000 of 1000000; Loss: 8.2400e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4528e+03; Test Loss: 8.2280e+02. (Time: 2.2s)Step 13200 of 1000000; Loss: 8.2293e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.2594e+02; Test Loss: 8.2169e+02. (Time: 3.2s)Step 13400 of 1000000; Loss: 8.2182e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.2794e+01; Test Loss: 8.2059e+02. (Time: 2.2s)Step 13600 of 1000000; Loss: 8.2075e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3346e+00; Test Loss: 8.1923e+02. (Time: 2.2s)Step 13800 of 1000000; Loss: 8.1940e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1242e+03; Test Loss: 8.1785e+02. (Time: 2.0s)Step 14000 of 1000000; Loss: 8.1789e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4392e+03; Test Loss: 8.1667e+02. (Time: 2.0s)Step 14200 of 1000000; Loss: 8.1685e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.1490e+02; Test Loss: 8.1582e+02. (Time: 1.9s)Step 14400 of 1000000; Loss: 8.1588e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.0448e+01; Test Loss: 8.1537e+02. (Time: 3.2s)Step 14600 of 1000000; Loss: 8.1544e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.6358e+00; Test Loss: 8.1481e+02. (Time: 2.1s)Step 14800 of 1000000; Loss: 8.1489e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1007e+03; Test Loss: 8.1416e+02. (Time: 2.1s)Step 15000 of 1000000; Loss: 8.1425e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4279e+03; Test Loss: 8.1341e+02. (Time: 2.2s)Step 15200 of 1000000; Loss: 8.1350e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.0461e+02; Test Loss: 8.1264e+02. (Time: 2.1s)Step 15400 of 1000000; Loss: 8.1274e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2998e+01; Test Loss: 8.1191e+02. (Time: 2.5s)Step 15600 of 1000000; Loss: 8.1200e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.2455e+00; Test Loss: 8.1111e+02. (Time: 2.5s)Step 15800 of 1000000; Loss: 8.1121e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0795e+03; Test Loss: 8.1028e+02. (Time: 2.1s)Step 16000 of 1000000; Loss: 8.1039e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4140e+03; Test Loss: 8.0946e+02. (Time: 2.0s)Step 16200 of 1000000; Loss: 8.0956e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.9748e+02; Test Loss: 8.0860e+02. (Time: 2.0s)Step 16400 of 1000000; Loss: 8.0871e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.9165e+01; Test Loss: 8.0775e+02. (Time: 2.2s)Step 16600 of 1000000; Loss: 8.0786e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.8678e+00; Test Loss: 8.0661e+02. (Time: 2.7s)Step 16800 of 1000000; Loss: 8.0673e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0578e+03; Test Loss: 8.0561e+02. (Time: 2.5s)Step 17000 of 1000000; Loss: 8.0573e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4002e+03; Test Loss: 8.0449e+02. (Time: 2.0s)Step 17200 of 1000000; Loss: 8.0465e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.8989e+02; Test Loss: 8.0338e+02. (Time: 2.2s)Step 17400 of 1000000; Loss: 8.0351e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5386e+01; Test Loss: 8.0234e+02. (Time: 2.2s)Step 17600 of 1000000; Loss: 8.0247e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5537e+00; Test Loss: 8.0128e+02. (Time: 2.1s)Step 17800 of 1000000; Loss: 8.0139e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0364e+03; Test Loss: 8.0024e+02. (Time: 3.1s)Step 18000 of 1000000; Loss: 8.0036e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3838e+03; Test Loss: 7.9929e+02. (Time: 2.0s)Step 18200 of 1000000; Loss: 7.9941e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.8274e+02; Test Loss: 7.9831e+02. (Time: 2.0s)Step 18400 of 1000000; Loss: 7.9839e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1932e+01; Test Loss: 7.9763e+02. (Time: 2.0s)Step 18600 of 1000000; Loss: 7.9771e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2725e+00; Test Loss: 7.9697e+02. (Time: 2.0s)Step 18800 of 1000000; Loss: 7.9705e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0192e+03; Test Loss: 7.9637e+02. (Time: 2.0s)Step 19000 of 1000000; Loss: 7.9643e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3708e+03; Test Loss: 7.9586e+02. (Time: 3.0s)Step 19200 of 1000000; Loss: 7.9593e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.7599e+02; Test Loss: 7.9526e+02. (Time: 2.0s)Step 19400 of 1000000; Loss: 7.9533e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.8382e+01; Test Loss: 7.9467e+02. (Time: 2.1s)Step 19600 of 1000000; Loss: 7.9475e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9563e+00; Test Loss: 7.9411e+02. (Time: 1.9s)Step 19800 of 1000000; Loss: 7.9418e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0071e+03; Test Loss: 7.9354e+02. (Time: 1.9s)Step 20000 of 1000000; Loss: 7.9359e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3608e+03; Test Loss: 7.9309e+02. (Time: 2.2s)Step 20200 of 1000000; Loss: 7.9315e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.6973e+02; Test Loss: 7.9262e+02. (Time: 3.0s)Step 20400 of 1000000; Loss: 7.9268e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5185e+01; Test Loss: 7.9225e+02. (Time: 1.9s)Step 20600 of 1000000; Loss: 7.9228e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6743e+00; Test Loss: 7.9189e+02. (Time: 1.9s)Step 20800 of 1000000; Loss: 7.9192e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9958e+03; Test Loss: 7.9153e+02. (Time: 2.0s)Step 21000 of 1000000; Loss: 7.9157e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3533e+03; Test Loss: 7.9123e+02. (Time: 2.1s)Step 21200 of 1000000; Loss: 7.9127e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.6401e+02; Test Loss: 7.9091e+02. (Time: 2.8s)Step 21400 of 1000000; Loss: 7.9095e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2401e+01; Test Loss: 7.9069e+02. (Time: 2.7s)Step 21600 of 1000000; Loss: 7.9069e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4663e+00; Test Loss: 7.9087e+02. (Time: 2.1s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3755\n",
            "Running configuration: hidden_size=4, lr=0.0001, weight_decay=1e-05, batch_size=32\n",
            "Step 200 of 200; Loss: 1.6613e+03; Test Loss: 9.1322e+01. (Time: 2.3s)Step 200 of 1000000; Loss: 8.6920e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3299e+03; Test Loss: 8.0096e+01. (Time: 2.2s)Step 400 of 1000000; Loss: 8.1599e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5465e+03; Test Loss: 7.2032e+01. (Time: 2.7s)Step 600 of 1000000; Loss: 7.8017e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6134e+03; Test Loss: 6.5331e+01. (Time: 3.1s)Step 800 of 1000000; Loss: 7.5131e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0156e+03; Test Loss: 6.0407e+01. (Time: 2.1s)Step 1000 of 1000000; Loss: 7.3031e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5479e+03; Test Loss: 5.6742e+01. (Time: 2.3s)Step 1200 of 1000000; Loss: 7.1506e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6027e+03; Test Loss: 5.3781e+01. (Time: 2.4s)Step 1400 of 1000000; Loss: 7.0260e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.6829e+02; Test Loss: 5.1382e+01. (Time: 2.6s)Step 1600 of 1000000; Loss: 6.9186e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5410e+03; Test Loss: 4.9196e+01. (Time: 3.2s)Step 1800 of 1000000; Loss: 6.8186e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5842e+03; Test Loss: 4.7099e+01. (Time: 2.2s)Step 2000 of 1000000; Loss: 6.7198e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.7523e+02; Test Loss: 4.5778e+01. (Time: 2.3s)Step 2200 of 1000000; Loss: 6.6503e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5157e+03; Test Loss: 4.4488e+01. (Time: 2.3s)Step 2400 of 1000000; Loss: 6.5849e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5595e+03; Test Loss: 4.3474e+01. (Time: 2.4s)Step 2600 of 1000000; Loss: 6.5225e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.2175e+02; Test Loss: 4.2526e+01. (Time: 3.4s)Step 2800 of 1000000; Loss: 6.4590e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4690e+03; Test Loss: 4.1618e+01. (Time: 2.3s)Step 3000 of 1000000; Loss: 6.3923e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5219e+03; Test Loss: 4.0683e+01. (Time: 2.2s)Step 3200 of 1000000; Loss: 6.3204e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.7180e+02; Test Loss: 3.9718e+01. (Time: 2.3s)Step 3400 of 1000000; Loss: 6.2418e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4005e+03; Test Loss: 3.8750e+01. (Time: 2.2s)Step 3600 of 1000000; Loss: 6.1579e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4659e+03; Test Loss: 3.7776e+01. (Time: 3.5s)Step 3800 of 1000000; Loss: 6.0665e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.2381e+02; Test Loss: 3.6822e+01. (Time: 2.8s)Step 4000 of 1000000; Loss: 5.9599e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2811e+03; Test Loss: 3.5766e+01. (Time: 2.3s)Step 4200 of 1000000; Loss: 5.8448e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3770e+03; Test Loss: 3.4522e+01. (Time: 2.2s)Step 4400 of 1000000; Loss: 5.7188e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5209e+02; Test Loss: 3.2550e+01. (Time: 2.2s)Step 4600 of 1000000; Loss: 5.5791e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1795e+03; Test Loss: 2.9309e+01. (Time: 7.1s)Step 4800 of 1000000; Loss: 5.4266e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3399e+03; Test Loss: 2.1476e+01. (Time: 2.5s)Step 5000 of 1000000; Loss: 5.1356e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4271e+02; Test Loss: 3.9397e+00. (Time: 2.3s)Step 5200 of 1000000; Loss: 4.4862e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1523e+03; Test Loss: 3.7413e+00. (Time: 3.8s)Step 5400 of 1000000; Loss: 4.4291e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3279e+03; Test Loss: 3.6670e+00. (Time: 2.4s)Step 5600 of 1000000; Loss: 4.3982e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1899e+02; Test Loss: 3.6071e+00. (Time: 2.2s)Step 5800 of 1000000; Loss: 4.3794e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1337e+03; Test Loss: 3.5489e+00. (Time: 2.2s)Step 6000 of 1000000; Loss: 4.3644e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3150e+03; Test Loss: 3.4905e+00. (Time: 2.3s)Step 6200 of 1000000; Loss: 4.3512e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0855e+02; Test Loss: 3.4311e+00. (Time: 3.4s)Step 6400 of 1000000; Loss: 4.3384e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1166e+03; Test Loss: 3.3706e+00. (Time: 2.4s)Step 6600 of 1000000; Loss: 4.3258e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3026e+03; Test Loss: 3.3093e+00. (Time: 2.1s)Step 6800 of 1000000; Loss: 4.3128e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0061e+02; Test Loss: 3.2475e+00. (Time: 2.3s)Step 7000 of 1000000; Loss: 4.2992e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1003e+03; Test Loss: 3.1851e+00. (Time: 2.4s)Step 7200 of 1000000; Loss: 4.2860e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2910e+03; Test Loss: 3.1227e+00. (Time: 3.2s)Step 7400 of 1000000; Loss: 4.2752e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3440e+01; Test Loss: 3.0602e+00. (Time: 2.3s)Step 7600 of 1000000; Loss: 4.2661e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0855e+03; Test Loss: 2.9979e+00. (Time: 2.4s)Step 7800 of 1000000; Loss: 4.2575e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2804e+03; Test Loss: 2.9363e+00. (Time: 2.5s)Step 8000 of 1000000; Loss: 4.2497e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.6261e+01; Test Loss: 2.8756e+00. (Time: 2.3s)Step 8200 of 1000000; Loss: 4.2421e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0721e+03; Test Loss: 2.8155e+00. (Time: 3.5s)Step 8400 of 1000000; Loss: 4.2350e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2713e+03; Test Loss: 2.7565e+00. (Time: 2.6s)Step 8600 of 1000000; Loss: 4.2284e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.9041e+01; Test Loss: 2.6988e+00. (Time: 2.3s)Step 8800 of 1000000; Loss: 4.2220e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0619e+03; Test Loss: 2.6423e+00. (Time: 2.2s)Step 9000 of 1000000; Loss: 4.2161e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2634e+03; Test Loss: 2.5871e+00. (Time: 2.2s)Step 9200 of 1000000; Loss: 4.2109e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.2571e+01; Test Loss: 2.5331e+00. (Time: 2.9s)Step 9400 of 1000000; Loss: 4.2059e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0510e+03; Test Loss: 2.4804e+00. (Time: 2.6s)Step 9600 of 1000000; Loss: 4.2011e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2561e+03; Test Loss: 2.4288e+00. (Time: 2.2s)Step 9800 of 1000000; Loss: 4.1966e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.6842e+01; Test Loss: 2.3786e+00. (Time: 2.4s)Step 10000 of 1000000; Loss: 4.1920e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0434e+03; Test Loss: 2.3295e+00. (Time: 2.5s)Step 10200 of 1000000; Loss: 4.1876e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2501e+03; Test Loss: 2.2813e+00. (Time: 2.8s)Step 10400 of 1000000; Loss: 4.1837e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.1345e+01; Test Loss: 2.2346e+00. (Time: 2.9s)Step 10600 of 1000000; Loss: 4.1795e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0361e+03; Test Loss: 2.1891e+00. (Time: 2.2s)Step 10800 of 1000000; Loss: 4.1755e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2450e+03; Test Loss: 2.1449e+00. (Time: 2.4s)Step 11000 of 1000000; Loss: 4.1715e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.6033e+01; Test Loss: 2.1020e+00. (Time: 2.3s)Step 11200 of 1000000; Loss: 4.1677e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0295e+03; Test Loss: 2.0594e+00. (Time: 2.4s)Step 11400 of 1000000; Loss: 4.1640e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2405e+03; Test Loss: 2.0183e+00. (Time: 3.2s)Step 11600 of 1000000; Loss: 4.1601e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.0563e+01; Test Loss: 1.9784e+00. (Time: 2.3s)Step 11800 of 1000000; Loss: 4.1562e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0236e+03; Test Loss: 1.9396e+00. (Time: 2.4s)Step 12000 of 1000000; Loss: 4.1524e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2364e+03; Test Loss: 1.9018e+00. (Time: 2.3s)Step 12200 of 1000000; Loss: 4.1485e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.6378e+01; Test Loss: 1.8654e+00. (Time: 2.5s)Step 12400 of 1000000; Loss: 4.1446e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0182e+03; Test Loss: 1.8295e+00. (Time: 3.2s)Step 12600 of 1000000; Loss: 4.1406e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2329e+03; Test Loss: 1.7943e+00. (Time: 2.3s)Step 12800 of 1000000; Loss: 4.1365e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.3383e+01; Test Loss: 1.7602e+00. (Time: 2.4s)Step 13000 of 1000000; Loss: 4.1321e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0137e+03; Test Loss: 1.7269e+00. (Time: 2.2s)Step 13200 of 1000000; Loss: 4.1274e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2295e+03; Test Loss: 1.6945e+00. (Time: 2.9s)Step 13400 of 1000000; Loss: 4.1233e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.0573e+01; Test Loss: 1.6634e+00. (Time: 3.0s)Step 13600 of 1000000; Loss: 4.1179e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0096e+03; Test Loss: 1.6331e+00. (Time: 2.4s)Step 13800 of 1000000; Loss: 4.1132e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2261e+03; Test Loss: 1.6030e+00. (Time: 2.3s)Step 14000 of 1000000; Loss: 4.1079e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.8206e+01; Test Loss: 1.5741e+00. (Time: 2.2s)Step 14200 of 1000000; Loss: 4.1026e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0048e+03; Test Loss: 1.5453e+00. (Time: 2.2s)Step 14400 of 1000000; Loss: 4.0969e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2226e+03; Test Loss: 1.5174e+00. (Time: 3.4s)Step 14600 of 1000000; Loss: 4.0908e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5722e+01; Test Loss: 1.4903e+00. (Time: 2.4s)Step 14800 of 1000000; Loss: 4.0848e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.9986e+02; Test Loss: 1.4645e+00. (Time: 2.3s)Step 15000 of 1000000; Loss: 4.0802e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2197e+03; Test Loss: 1.4396e+00. (Time: 2.1s)Step 15200 of 1000000; Loss: 4.0756e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2107e+01; Test Loss: 1.4170e+00. (Time: 2.2s)Step 15400 of 1000000; Loss: 4.0722e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.9564e+02; Test Loss: 1.3966e+00. (Time: 3.6s)Step 15600 of 1000000; Loss: 4.0689e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2169e+03; Test Loss: 1.3776e+00. (Time: 2.5s)Step 15800 of 1000000; Loss: 4.0658e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4468e+01; Test Loss: 1.3591e+00. (Time: 2.3s)Step 16000 of 1000000; Loss: 4.0626e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.9151e+02; Test Loss: 1.3412e+00. (Time: 2.2s)Step 16200 of 1000000; Loss: 4.0589e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2140e+03; Test Loss: 1.3242e+00. (Time: 2.2s)Step 16400 of 1000000; Loss: 4.0553e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3666e+01; Test Loss: 1.3080e+00. (Time: 3.5s)Step 16600 of 1000000; Loss: 4.0515e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.8702e+02; Test Loss: 1.2919e+00. (Time: 2.4s)Step 16800 of 1000000; Loss: 4.0474e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2108e+03; Test Loss: 1.2764e+00. (Time: 2.3s)Step 17000 of 1000000; Loss: 4.0434e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2943e+01; Test Loss: 1.2611e+00. (Time: 2.3s)Step 17200 of 1000000; Loss: 4.0395e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.8211e+02; Test Loss: 1.2458e+00. (Time: 2.1s)Step 17400 of 1000000; Loss: 4.0356e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2078e+03; Test Loss: 1.2305e+00. (Time: 3.4s)Step 17600 of 1000000; Loss: 4.0315e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2232e+01; Test Loss: 1.2149e+00. (Time: 2.5s)Step 17800 of 1000000; Loss: 4.0270e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.7686e+02; Test Loss: 1.1988e+00. (Time: 2.4s)Step 18000 of 1000000; Loss: 4.0217e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2045e+03; Test Loss: 1.1827e+00. (Time: 2.3s)Step 18200 of 1000000; Loss: 4.0166e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1493e+01; Test Loss: 1.1668e+00. (Time: 2.4s)Step 18400 of 1000000; Loss: 4.0115e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.7131e+02; Test Loss: 1.1509e+00. (Time: 3.4s)Step 18600 of 1000000; Loss: 4.0066e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2007e+03; Test Loss: 1.1357e+00. (Time: 2.4s)Step 18800 of 1000000; Loss: 4.0020e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0802e+01; Test Loss: 1.1208e+00. (Time: 2.1s)Step 19000 of 1000000; Loss: 3.9971e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6551e+02; Test Loss: 1.1057e+00. (Time: 2.2s)Step 19200 of 1000000; Loss: 3.9920e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1966e+03; Test Loss: 1.0904e+00. (Time: 2.3s)Step 19400 of 1000000; Loss: 3.9868e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0083e+01; Test Loss: 1.0758e+00. (Time: 2.7s)Step 19600 of 1000000; Loss: 3.9841e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6083e+02; Test Loss: 1.0611e+00. (Time: 2.9s)Step 19800 of 1000000; Loss: 3.9812e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1937e+03; Test Loss: 1.0462e+00. (Time: 2.2s)Step 20000 of 1000000; Loss: 3.9786e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9354e+01; Test Loss: 1.0316e+00. (Time: 2.3s)Step 20200 of 1000000; Loss: 3.9762e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5716e+02; Test Loss: 1.0169e+00. (Time: 2.1s)Step 20400 of 1000000; Loss: 3.9740e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1914e+03; Test Loss: 1.0021e+00. (Time: 2.4s)Step 20600 of 1000000; Loss: 3.9719e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8622e+01; Test Loss: 9.8763e-01. (Time: 3.7s)Step 20800 of 1000000; Loss: 3.9695e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5332e+02; Test Loss: 9.7316e-01. (Time: 2.4s)Step 21000 of 1000000; Loss: 3.9661e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1889e+03; Test Loss: 9.5816e-01. (Time: 2.2s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3900\n",
            "Running configuration: hidden_size=4, lr=0.0001, weight_decay=1e-05, batch_size=64\n",
            "Step 200 of 200; Loss: 3.2944e+03; Test Loss: 1.7245e+03. (Time: 2.0s)Step 200 of 1000000; Loss: 1.7403e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9238e+03; Test Loss: 1.6250e+03. (Time: 2.2s)Step 400 of 1000000; Loss: 1.6352e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4029e+03; Test Loss: 1.5562e+03. (Time: 3.1s)Step 600 of 1000000; Loss: 1.5637e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.8869e+02; Test Loss: 1.5000e+03. (Time: 2.1s)Step 800 of 1000000; Loss: 1.5062e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1291e+03; Test Loss: 1.4597e+03. (Time: 2.2s)Step 1000 of 1000000; Loss: 1.4639e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2159e+03; Test Loss: 1.4297e+03. (Time: 2.0s)Step 1200 of 1000000; Loss: 1.4330e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3498e+03; Test Loss: 1.4055e+03. (Time: 2.2s)Step 1400 of 1000000; Loss: 1.4083e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7363e+03; Test Loss: 1.3844e+03. (Time: 2.6s)Step 1600 of 1000000; Loss: 1.3869e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9483e+02; Test Loss: 1.3643e+03. (Time: 2.5s)Step 1800 of 1000000; Loss: 1.3667e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1004e+03; Test Loss: 1.3445e+03. (Time: 2.0s)Step 2000 of 1000000; Loss: 1.3470e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1774e+03; Test Loss: 1.3295e+03. (Time: 2.0s)Step 2200 of 1000000; Loss: 1.3311e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1154e+03; Test Loss: 1.3166e+03. (Time: 2.0s)Step 2400 of 1000000; Loss: 1.3181e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4691e+03; Test Loss: 1.3043e+03. (Time: 1.9s)Step 2600 of 1000000; Loss: 1.3058e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5451e+02; Test Loss: 1.2915e+03. (Time: 2.8s)Step 2800 of 1000000; Loss: 1.2931e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9788e+03; Test Loss: 1.2779e+03. (Time: 2.8s)Step 3000 of 1000000; Loss: 1.2796e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0693e+03; Test Loss: 1.2632e+03. (Time: 2.1s)Step 3200 of 1000000; Loss: 1.2650e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9476e+03; Test Loss: 1.2472e+03. (Time: 2.1s)Step 3400 of 1000000; Loss: 1.2492e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3032e+03; Test Loss: 1.2302e+03. (Time: 1.9s)Step 3600 of 1000000; Loss: 1.2324e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2384e+02; Test Loss: 1.2113e+03. (Time: 2.1s)Step 3800 of 1000000; Loss: 1.2138e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7148e+03; Test Loss: 1.1892e+03. (Time: 3.2s)Step 4000 of 1000000; Loss: 1.1920e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.8295e+03; Test Loss: 1.1653e+03. (Time: 2.2s)Step 4200 of 1000000; Loss: 1.1684e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7084e+03; Test Loss: 1.1387e+03. (Time: 2.2s)Step 4400 of 1000000; Loss: 1.1422e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0750e+03; Test Loss: 1.1079e+03. (Time: 2.0s)Step 4600 of 1000000; Loss: 1.1118e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+02; Test Loss: 1.0688e+03. (Time: 2.1s)Step 4800 of 1000000; Loss: 1.0749e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4398e+03; Test Loss: 9.4379e+02. (Time: 2.3s)Step 5000 of 1000000; Loss: 9.7250e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6700e+03; Test Loss: 8.9219e+02. (Time: 2.9s)Step 5200 of 1000000; Loss: 8.9345e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0596e+03; Test Loss: 8.8388e+02. (Time: 2.0s)Step 5400 of 1000000; Loss: 8.8461e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1261e+02; Test Loss: 8.7926e+02. (Time: 2.1s)Step 5600 of 1000000; Loss: 8.7973e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0562e+01; Test Loss: 8.7592e+02. (Time: 2.1s)Step 5800 of 1000000; Loss: 8.7629e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3733e+03; Test Loss: 8.7312e+02. (Time: 2.1s)Step 6000 of 1000000; Loss: 8.7346e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6258e+03; Test Loss: 8.7050e+02. (Time: 2.9s)Step 6200 of 1000000; Loss: 8.7081e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0100e+03; Test Loss: 8.6795e+02. (Time: 2.6s)Step 6400 of 1000000; Loss: 8.6826e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8380e+02; Test Loss: 8.6524e+02. (Time: 2.1s)Step 6600 of 1000000; Loss: 8.6558e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6615e+01; Test Loss: 8.6231e+02. (Time: 2.0s)Step 6800 of 1000000; Loss: 8.6269e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3217e+03; Test Loss: 8.5946e+02. (Time: 2.1s)Step 7000 of 1000000; Loss: 8.5979e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5869e+03; Test Loss: 8.5718e+02. (Time: 2.3s)Step 7200 of 1000000; Loss: 8.5744e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6488e+02; Test Loss: 8.5522e+02. (Time: 3.1s)Step 7400 of 1000000; Loss: 8.5545e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6152e+02; Test Loss: 8.5339e+02. (Time: 1.9s)Step 7600 of 1000000; Loss: 8.5362e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1089e+01; Test Loss: 8.5158e+02. (Time: 1.9s)Step 7800 of 1000000; Loss: 8.5181e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2754e+03; Test Loss: 8.4998e+02. (Time: 2.1s)Step 8000 of 1000000; Loss: 8.5017e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5541e+03; Test Loss: 8.4842e+02. (Time: 1.9s)Step 8200 of 1000000; Loss: 8.4861e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2000e+02; Test Loss: 8.4689e+02. (Time: 2.3s)Step 8400 of 1000000; Loss: 8.4707e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3914e+02; Test Loss: 8.4546e+02. (Time: 3.1s)Step 8600 of 1000000; Loss: 8.4563e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6793e+00; Test Loss: 8.4418e+02. (Time: 2.2s)Step 8800 of 1000000; Loss: 8.4433e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2362e+03; Test Loss: 8.4297e+02. (Time: 1.9s)Step 9000 of 1000000; Loss: 8.4311e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5248e+03; Test Loss: 8.4176e+02. (Time: 2.0s)Step 9200 of 1000000; Loss: 8.4191e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9143e+02; Test Loss: 8.4058e+02. (Time: 2.1s)Step 9400 of 1000000; Loss: 8.4072e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2187e+02; Test Loss: 8.3936e+02. (Time: 2.6s)Step 9600 of 1000000; Loss: 8.3950e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.7534e+00; Test Loss: 8.3826e+02. (Time: 2.6s)Step 9800 of 1000000; Loss: 8.3836e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2040e+03; Test Loss: 8.3718e+02. (Time: 2.0s)Step 10000 of 1000000; Loss: 8.3730e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5001e+03; Test Loss: 8.3611e+02. (Time: 2.0s)Step 10200 of 1000000; Loss: 8.3621e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.6944e+02; Test Loss: 8.3520e+02. (Time: 2.1s)Step 10400 of 1000000; Loss: 8.3532e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0506e+02; Test Loss: 8.3429e+02. (Time: 2.1s)Step 10600 of 1000000; Loss: 8.3439e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.7969e+00; Test Loss: 8.3346e+02. (Time: 3.3s)Step 10800 of 1000000; Loss: 8.3355e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1794e+03; Test Loss: 8.3265e+02. (Time: 2.1s)Step 11000 of 1000000; Loss: 8.3273e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4822e+03; Test Loss: 8.3176e+02. (Time: 2.1s)Step 11200 of 1000000; Loss: 8.3187e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.5182e+02; Test Loss: 8.3104e+02. (Time: 1.9s)Step 11400 of 1000000; Loss: 8.3112e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.1883e+01; Test Loss: 8.3036e+02. (Time: 2.0s)Step 11600 of 1000000; Loss: 8.3041e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.9204e+00; Test Loss: 8.2938e+02. (Time: 2.2s)Step 11800 of 1000000; Loss: 8.2948e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1592e+03; Test Loss: 8.2858e+02. (Time: 3.2s)Step 12000 of 1000000; Loss: 8.2869e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4670e+03; Test Loss: 8.2776e+02. (Time: 6.6s)Step 12200 of 1000000; Loss: 8.2786e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.3775e+02; Test Loss: 8.2688e+02. (Time: 2.0s)Step 12400 of 1000000; Loss: 8.2699e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.1774e+01; Test Loss: 8.2599e+02. (Time: 2.9s)Step 12600 of 1000000; Loss: 8.2611e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.0984e+00; Test Loss: 8.2495e+02. (Time: 2.4s)Step 12800 of 1000000; Loss: 8.2509e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1394e+03; Test Loss: 8.2392e+02. (Time: 2.0s)Step 13000 of 1000000; Loss: 8.2405e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4530e+03; Test Loss: 8.2286e+02. (Time: 2.0s)Step 13200 of 1000000; Loss: 8.2299e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.2601e+02; Test Loss: 8.2177e+02. (Time: 2.0s)Step 13400 of 1000000; Loss: 8.2189e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.2964e+01; Test Loss: 8.2070e+02. (Time: 2.0s)Step 13600 of 1000000; Loss: 8.2084e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3369e+00; Test Loss: 8.1935e+02. (Time: 2.7s)Step 13800 of 1000000; Loss: 8.1951e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1255e+03; Test Loss: 8.1793e+02. (Time: 2.3s)Step 14000 of 1000000; Loss: 8.1803e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4392e+03; Test Loss: 8.1675e+02. (Time: 2.1s)Step 14200 of 1000000; Loss: 8.1677e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.1491e+02; Test Loss: 8.1585e+02. (Time: 2.1s)Step 14400 of 1000000; Loss: 8.1592e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.0459e+01; Test Loss: 8.1524e+02. (Time: 2.1s)Step 14600 of 1000000; Loss: 8.1529e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.6350e+00; Test Loss: 8.1462e+02. (Time: 2.0s)Step 14800 of 1000000; Loss: 8.1470e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1005e+03; Test Loss: 8.1396e+02. (Time: 3.2s)Step 15000 of 1000000; Loss: 8.1404e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4274e+03; Test Loss: 8.1319e+02. (Time: 2.0s)Step 15200 of 1000000; Loss: 8.1328e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.0458e+02; Test Loss: 8.1245e+02. (Time: 2.1s)Step 15400 of 1000000; Loss: 8.1254e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2971e+01; Test Loss: 8.1167e+02. (Time: 2.0s)Step 15600 of 1000000; Loss: 8.1177e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.2424e+00; Test Loss: 8.1092e+02. (Time: 2.1s)Step 15800 of 1000000; Loss: 8.1101e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0786e+03; Test Loss: 8.1007e+02. (Time: 2.3s)Step 16000 of 1000000; Loss: 8.1019e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4134e+03; Test Loss: 8.0928e+02. (Time: 2.8s)Step 16200 of 1000000; Loss: 8.0937e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.9728e+02; Test Loss: 8.0843e+02. (Time: 2.0s)Step 16400 of 1000000; Loss: 8.0854e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.9068e+01; Test Loss: 8.0759e+02. (Time: 2.0s)Step 16600 of 1000000; Loss: 8.0769e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.8523e+00; Test Loss: 8.0644e+02. (Time: 2.1s)Step 16800 of 1000000; Loss: 8.0656e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0568e+03; Test Loss: 8.0544e+02. (Time: 2.0s)Step 17000 of 1000000; Loss: 8.0557e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3995e+03; Test Loss: 8.0429e+02. (Time: 2.7s)Step 17200 of 1000000; Loss: 8.0445e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.8971e+02; Test Loss: 8.0317e+02. (Time: 2.5s)Step 17400 of 1000000; Loss: 8.0331e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5230e+01; Test Loss: 8.0214e+02. (Time: 2.2s)Step 17600 of 1000000; Loss: 8.0227e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5334e+00; Test Loss: 8.0107e+02. (Time: 2.0s)Step 17800 of 1000000; Loss: 8.0119e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0355e+03; Test Loss: 8.0001e+02. (Time: 1.9s)Step 18000 of 1000000; Loss: 8.0013e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3832e+03; Test Loss: 7.9908e+02. (Time: 2.0s)Step 18200 of 1000000; Loss: 7.9920e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.8240e+02; Test Loss: 7.9819e+02. (Time: 2.7s)Step 18400 of 1000000; Loss: 7.9826e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1781e+01; Test Loss: 7.9748e+02. (Time: 2.5s)Step 18600 of 1000000; Loss: 7.9757e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2479e+00; Test Loss: 7.9686e+02. (Time: 2.0s)Step 18800 of 1000000; Loss: 7.9693e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0187e+03; Test Loss: 7.9629e+02. (Time: 2.0s)Step 19000 of 1000000; Loss: 7.9635e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3704e+03; Test Loss: 7.9577e+02. (Time: 2.0s)Step 19200 of 1000000; Loss: 7.9585e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.7567e+02; Test Loss: 7.9513e+02. (Time: 2.2s)Step 19400 of 1000000; Loss: 7.9520e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.8249e+01; Test Loss: 7.9458e+02. (Time: 3.2s)Step 19600 of 1000000; Loss: 7.9465e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9337e+00; Test Loss: 7.9400e+02. (Time: 2.1s)Step 19800 of 1000000; Loss: 7.9407e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0063e+03; Test Loss: 7.9350e+02. (Time: 2.2s)Step 20000 of 1000000; Loss: 7.9355e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3606e+03; Test Loss: 7.9311e+02. (Time: 2.0s)Step 20200 of 1000000; Loss: 7.9317e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.6937e+02; Test Loss: 7.9265e+02. (Time: 2.1s)Step 20400 of 1000000; Loss: 7.9270e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5074e+01; Test Loss: 7.9223e+02. (Time: 2.0s)Step 20600 of 1000000; Loss: 7.9229e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6526e+00; Test Loss: 7.9185e+02. (Time: 3.0s)Step 20800 of 1000000; Loss: 7.9189e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9951e+03; Test Loss: 7.9150e+02. (Time: 2.0s)Step 21000 of 1000000; Loss: 7.9153e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3532e+03; Test Loss: 7.9128e+02. (Time: 1.9s)Step 21200 of 1000000; Loss: 7.9126e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.6028e+02; Test Loss: 7.9123e+02. (Time: 2.1s)Step 21400 of 1000000; Loss: 7.9123e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2338e+01; Test Loss: 7.9123e+02. (Time: 2.1s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3757\n",
            "Running configuration: hidden_size=8, lr=0.001, weight_decay=0.001, batch_size=32\n",
            "Step 200 of 200; Loss: 1.6631e+03; Test Loss: 3.2623e+01. (Time: 3.3s)Step 200 of 1000000; Loss: 6.4720e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.8297e+02; Test Loss: 2.2850e+01. (Time: 2.5s)Step 400 of 1000000; Loss: 5.1850e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0004e+03; Test Loss: 5.7730e+00. (Time: 2.3s)Step 600 of 1000000; Loss: 4.2923e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2245e+03; Test Loss: 3.6460e+00. (Time: 2.5s)Step 800 of 1000000; Loss: 4.1306e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5485e+01; Test Loss: 2.7890e+00. (Time: 2.4s)Step 1000 of 1000000; Loss: 4.0530e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6331e+02; Test Loss: 1.0429e+00. (Time: 3.7s)Step 1200 of 1000000; Loss: 3.9717e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1900e+03; Test Loss: 1.0037e+00. (Time: 2.5s)Step 1400 of 1000000; Loss: 3.9489e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0318e+01; Test Loss: 9.7310e-01. (Time: 2.4s)Step 1600 of 1000000; Loss: 3.9203e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3618e+02; Test Loss: 9.4364e-01. (Time: 2.3s)Step 1800 of 1000000; Loss: 3.8869e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1676e+03; Test Loss: 9.4535e-01. (Time: 2.3s)Step 2000 of 1000000; Loss: 3.8562e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5758e+01; Test Loss: 9.2437e-01. (Time: 3.3s)Step 2200 of 1000000; Loss: 3.8373e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.1113e+02; Test Loss: 8.9224e-01. (Time: 2.7s)Step 2400 of 1000000; Loss: 3.8257e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1556e+03; Test Loss: 8.8876e-01. (Time: 2.6s)Step 2600 of 1000000; Loss: 3.8141e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2884e+01; Test Loss: 8.3957e-01. (Time: 2.3s)Step 2800 of 1000000; Loss: 3.8123e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0296e+02; Test Loss: 7.6596e-01. (Time: 2.4s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3733\n",
            "Running configuration: hidden_size=8, lr=0.001, weight_decay=0.001, batch_size=64\n",
            "Step 200 of 200; Loss: 3.3763e+03; Test Loss: 1.2768e+03. (Time: 3.3s)Step 200 of 1000000; Loss: 1.2992e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2804e+03; Test Loss: 9.9063e+02. (Time: 2.1s)Step 400 of 1000000; Loss: 1.0263e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7960e+02; Test Loss: 8.4413e+02. (Time: 2.3s)Step 600 of 1000000; Loss: 8.4986e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0018e+01; Test Loss: 8.1979e+02. (Time: 2.2s)Step 800 of 1000000; Loss: 8.2184e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0537e+03; Test Loss: 8.0439e+02. (Time: 2.2s)Step 1000 of 1000000; Loss: 8.0599e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3767e+03; Test Loss: 7.9206e+02. (Time: 2.9s)Step 1200 of 1000000; Loss: 7.9349e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.5968e+02; Test Loss: 7.8204e+02. (Time: 2.6s)Step 1400 of 1000000; Loss: 7.8297e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9984e+01; Test Loss: 7.7320e+02. (Time: 2.2s)Step 1600 of 1000000; Loss: 7.7448e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.2788e+00; Test Loss: 7.6928e+02. (Time: 2.3s)Step 1800 of 1000000; Loss: 7.6975e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9370e+03; Test Loss: 7.6629e+02. (Time: 2.3s)Step 2000 of 1000000; Loss: 7.6672e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2954e+03; Test Loss: 7.6439e+02. (Time: 2.4s)Step 2200 of 1000000; Loss: 7.6447e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1865e+02; Test Loss: 7.6324e+02. (Time: 3.2s)Step 2400 of 1000000; Loss: 7.6330e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.8868e+01; Test Loss: 7.6182e+02. (Time: 2.3s)Step 2600 of 1000000; Loss: 7.6195e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7528e+00; Test Loss: 7.6076e+02. (Time: 2.0s)Step 2800 of 1000000; Loss: 7.6094e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8899e+03; Test Loss: 7.5991e+02. (Time: 2.0s)Step 3000 of 1000000; Loss: 7.6030e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2743e+03; Test Loss: 7.5993e+02. (Time: 2.2s)Step 3200 of 1000000; Loss: 7.5987e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1325e+02; Test Loss: 7.5981e+02. (Time: 3.4s)Step 3400 of 1000000; Loss: 7.5959e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7747e+01; Test Loss: 7.6547e+02. (Time: 2.3s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3635\n",
            "Running configuration: hidden_size=8, lr=0.001, weight_decay=0.0001, batch_size=32\n",
            "Step 200 of 200; Loss: 1.6631e+03; Test Loss: 3.2623e+01. (Time: 2.4s)Step 200 of 1000000; Loss: 6.4720e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.8315e+02; Test Loss: 2.2861e+01. (Time: 2.4s)Step 400 of 1000000; Loss: 5.1850e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0004e+03; Test Loss: 5.7726e+00. (Time: 2.2s)Step 600 of 1000000; Loss: 4.2922e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2245e+03; Test Loss: 3.6467e+00. (Time: 3.5s)Step 800 of 1000000; Loss: 4.1306e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5631e+01; Test Loss: 2.7986e+00. (Time: 2.4s)Step 1000 of 1000000; Loss: 4.0529e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6333e+02; Test Loss: 1.0451e+00. (Time: 2.4s)Step 1200 of 1000000; Loss: 3.9713e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1899e+03; Test Loss: 1.0036e+00. (Time: 2.4s)Step 1400 of 1000000; Loss: 3.9485e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0310e+01; Test Loss: 9.7317e-01. (Time: 2.4s)Step 1600 of 1000000; Loss: 3.9199e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3611e+02; Test Loss: 9.4561e-01. (Time: 3.7s)Step 1800 of 1000000; Loss: 3.8862e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1674e+03; Test Loss: 9.5000e-01. (Time: 2.4s)Step 2000 of 1000000; Loss: 3.8563e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5720e+01; Test Loss: 9.2418e-01. (Time: 2.3s)Step 2200 of 1000000; Loss: 3.8370e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.1122e+02; Test Loss: 8.9408e-01. (Time: 2.4s)Step 2400 of 1000000; Loss: 3.8241e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1594e+03; Test Loss: 9.1146e-01. (Time: 2.3s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3758\n",
            "Running configuration: hidden_size=8, lr=0.001, weight_decay=0.0001, batch_size=64\n",
            "Step 200 of 200; Loss: 3.3763e+03; Test Loss: 1.2768e+03. (Time: 3.1s)Step 200 of 1000000; Loss: 1.2992e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2802e+03; Test Loss: 9.9059e+02. (Time: 2.1s)Step 400 of 1000000; Loss: 1.0263e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7942e+02; Test Loss: 8.4403e+02. (Time: 2.0s)Step 600 of 1000000; Loss: 8.4976e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0004e+01; Test Loss: 8.1973e+02. (Time: 2.0s)Step 800 of 1000000; Loss: 8.2177e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0536e+03; Test Loss: 8.0437e+02. (Time: 2.3s)Step 1000 of 1000000; Loss: 8.0597e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3766e+03; Test Loss: 7.9200e+02. (Time: 2.7s)Step 1200 of 1000000; Loss: 7.9344e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.5969e+02; Test Loss: 7.8207e+02. (Time: 2.8s)Step 1400 of 1000000; Loss: 7.8295e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.0142e+01; Test Loss: 7.7315e+02. (Time: 2.0s)Step 1600 of 1000000; Loss: 7.7443e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.2771e+00; Test Loss: 7.6925e+02. (Time: 2.0s)Step 1800 of 1000000; Loss: 7.6971e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9370e+03; Test Loss: 7.6624e+02. (Time: 2.1s)Step 2000 of 1000000; Loss: 7.6670e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2954e+03; Test Loss: 7.6438e+02. (Time: 2.1s)Step 2200 of 1000000; Loss: 7.6445e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1860e+02; Test Loss: 7.6321e+02. (Time: 3.4s)Step 2400 of 1000000; Loss: 7.6328e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.8846e+01; Test Loss: 7.6182e+02. (Time: 2.3s)Step 2600 of 1000000; Loss: 7.6195e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7499e+00; Test Loss: 7.6076e+02. (Time: 2.3s)Step 2800 of 1000000; Loss: 7.6097e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8897e+03; Test Loss: 7.6002e+02. (Time: 2.3s)Step 3000 of 1000000; Loss: 7.6033e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2742e+03; Test Loss: 7.5995e+02. (Time: 2.0s)Step 3200 of 1000000; Loss: 7.5987e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1334e+02; Test Loss: 7.5979e+02. (Time: 2.5s)Step 3400 of 1000000; Loss: 7.5953e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1538e+02; Test Loss: 8.8569e+02. (Time: 3.1s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.4206\n",
            "Running configuration: hidden_size=8, lr=0.001, weight_decay=1e-05, batch_size=32\n",
            "Step 200 of 200; Loss: 1.6631e+03; Test Loss: 3.2623e+01. (Time: 2.2s)Step 200 of 1000000; Loss: 6.4720e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.8311e+02; Test Loss: 2.2859e+01. (Time: 2.4s)Step 400 of 1000000; Loss: 5.1850e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0004e+03; Test Loss: 5.7729e+00. (Time: 2.3s)Step 600 of 1000000; Loss: 4.2923e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2245e+03; Test Loss: 3.6483e+00. (Time: 2.6s)Step 800 of 1000000; Loss: 4.1308e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5649e+01; Test Loss: 2.7983e+00. (Time: 3.2s)Step 1000 of 1000000; Loss: 4.0530e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6332e+02; Test Loss: 1.0477e+00. (Time: 2.3s)Step 1200 of 1000000; Loss: 3.9713e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1898e+03; Test Loss: 1.0039e+00. (Time: 2.5s)Step 1400 of 1000000; Loss: 3.9486e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0297e+01; Test Loss: 9.7230e-01. (Time: 2.3s)Step 1600 of 1000000; Loss: 3.9201e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3603e+02; Test Loss: 9.4455e-01. (Time: 2.8s)Step 1800 of 1000000; Loss: 3.8863e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1674e+03; Test Loss: 9.4882e-01. (Time: 3.3s)Step 2000 of 1000000; Loss: 3.8561e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5728e+01; Test Loss: 9.2371e-01. (Time: 2.5s)Step 2200 of 1000000; Loss: 3.8365e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.1136e+02; Test Loss: 8.9568e-01. (Time: 2.5s)Step 2400 of 1000000; Loss: 3.8238e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1594e+03; Test Loss: 9.1059e-01. (Time: 2.4s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3759\n",
            "Running configuration: hidden_size=8, lr=0.001, weight_decay=1e-05, batch_size=64\n",
            "Step 200 of 200; Loss: 3.3763e+03; Test Loss: 1.2768e+03. (Time: 2.6s)Step 200 of 1000000; Loss: 1.2992e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2803e+03; Test Loss: 9.9059e+02. (Time: 2.9s)Step 400 of 1000000; Loss: 1.0263e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7939e+02; Test Loss: 8.4401e+02. (Time: 2.2s)Step 600 of 1000000; Loss: 8.4974e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0002e+01; Test Loss: 8.1973e+02. (Time: 2.2s)Step 800 of 1000000; Loss: 8.2177e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0536e+03; Test Loss: 8.0436e+02. (Time: 2.1s)Step 1000 of 1000000; Loss: 8.0597e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3766e+03; Test Loss: 7.9203e+02. (Time: 2.2s)Step 1200 of 1000000; Loss: 7.9345e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.5966e+02; Test Loss: 7.8203e+02. (Time: 3.6s)Step 1400 of 1000000; Loss: 7.8294e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.0306e+01; Test Loss: 7.7320e+02. (Time: 2.2s)Step 1600 of 1000000; Loss: 7.7446e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.2740e+00; Test Loss: 7.6911e+02. (Time: 2.1s)Step 1800 of 1000000; Loss: 7.6956e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9372e+03; Test Loss: 7.6607e+02. (Time: 2.1s)Step 2000 of 1000000; Loss: 7.6647e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2955e+03; Test Loss: 7.6443e+02. (Time: 2.0s)Step 2200 of 1000000; Loss: 7.6452e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1858e+02; Test Loss: 7.6327e+02. (Time: 2.5s)Step 2400 of 1000000; Loss: 7.6332e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.8892e+01; Test Loss: 7.6189e+02. (Time: 3.0s)Step 2600 of 1000000; Loss: 7.6201e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7584e+00; Test Loss: 7.6072e+02. (Time: 2.2s)Step 2800 of 1000000; Loss: 7.6094e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8899e+03; Test Loss: 7.6003e+02. (Time: 2.3s)Step 3000 of 1000000; Loss: 7.6034e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2744e+03; Test Loss: 7.5997e+02. (Time: 2.2s)Step 3200 of 1000000; Loss: 7.5992e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1335e+02; Test Loss: 7.5981e+02. (Time: 2.2s)Step 3400 of 1000000; Loss: 7.5964e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5348e+01; Test Loss: 7.5949e+02. (Time: 3.3s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3606\n",
            "Running configuration: hidden_size=8, lr=0.0001, weight_decay=0.001, batch_size=32\n",
            "Step 200 of 200; Loss: 1.8166e+03; Test Loss: 7.4962e+01. (Time: 2.7s)Step 200 of 1000000; Loss: 8.4723e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1030e+03; Test Loss: 6.5260e+01. (Time: 2.2s)Step 400 of 1000000; Loss: 7.9388e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7115e+03; Test Loss: 5.7897e+01. (Time: 2.3s)Step 600 of 1000000; Loss: 7.5503e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7128e+03; Test Loss: 5.1805e+01. (Time: 2.7s)Step 800 of 1000000; Loss: 7.2423e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.9849e+02; Test Loss: 4.6958e+01. (Time: 3.4s)Step 1000 of 1000000; Loss: 7.0047e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6823e+03; Test Loss: 4.3108e+01. (Time: 2.3s)Step 1200 of 1000000; Loss: 6.8287e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6814e+03; Test Loss: 3.9788e+01. (Time: 2.3s)Step 1400 of 1000000; Loss: 6.6830e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.3103e+02; Test Loss: 3.6951e+01. (Time: 2.5s)Step 1600 of 1000000; Loss: 6.5599e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6702e+03; Test Loss: 3.4500e+01. (Time: 7.5s)Step 1800 of 1000000; Loss: 6.4516e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6582e+03; Test Loss: 3.2318e+01. (Time: 2.3s)Step 2000 of 1000000; Loss: 6.3487e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2060e+02; Test Loss: 3.0353e+01. (Time: 2.2s)Step 2200 of 1000000; Loss: 6.2445e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6088e+03; Test Loss: 2.8582e+01. (Time: 2.3s)Step 2400 of 1000000; Loss: 6.1158e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5336e+03; Test Loss: 2.7769e+01. (Time: 3.7s)Step 2600 of 1000000; Loss: 5.8859e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.6111e+02; Test Loss: 2.7008e+01. (Time: 2.3s)Step 2800 of 1000000; Loss: 5.6823e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2974e+03; Test Loss: 2.6327e+01. (Time: 2.3s)Step 3000 of 1000000; Loss: 5.5028e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3498e+03; Test Loss: 2.5675e+01. (Time: 2.4s)Step 3200 of 1000000; Loss: 5.3457e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1005e+02; Test Loss: 2.4625e+01. (Time: 2.2s)Step 3400 of 1000000; Loss: 5.2118e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1250e+03; Test Loss: 2.3073e+01. (Time: 3.9s)Step 3600 of 1000000; Loss: 5.1038e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2741e+03; Test Loss: 2.1584e+01. (Time: 2.3s)Step 3800 of 1000000; Loss: 5.0057e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2960e+02; Test Loss: 1.9998e+01. (Time: 2.5s)Step 4000 of 1000000; Loss: 4.9119e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0591e+03; Test Loss: 1.8267e+01. (Time: 2.4s)Step 4200 of 1000000; Loss: 4.8193e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2412e+03; Test Loss: 1.6433e+01. (Time: 2.3s)Step 4400 of 1000000; Loss: 4.7252e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4336e+02; Test Loss: 1.4569e+01. (Time: 3.5s)Step 4600 of 1000000; Loss: 4.6333e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0211e+03; Test Loss: 1.2590e+01. (Time: 2.3s)Step 4800 of 1000000; Loss: 4.5457e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2280e+03; Test Loss: 1.1074e+01. (Time: 2.4s)Step 5000 of 1000000; Loss: 4.4708e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6707e+02; Test Loss: 9.8910e+00. (Time: 2.4s)Step 5200 of 1000000; Loss: 4.4133e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0047e+03; Test Loss: 8.8457e+00. (Time: 2.3s)Step 5400 of 1000000; Loss: 4.3686e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2303e+03; Test Loss: 7.9852e+00. (Time: 3.3s)Step 5600 of 1000000; Loss: 4.3305e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2297e+02; Test Loss: 7.2528e+00. (Time: 2.3s)Step 5800 of 1000000; Loss: 4.2988e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0018e+03; Test Loss: 6.6076e+00. (Time: 2.3s)Step 6000 of 1000000; Loss: 4.2706e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2307e+03; Test Loss: 6.0780e+00. (Time: 2.4s)Step 6200 of 1000000; Loss: 4.2460e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6301e+01; Test Loss: 5.6407e+00. (Time: 2.7s)Step 6400 of 1000000; Loss: 4.2236e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.9719e+02; Test Loss: 5.2613e+00. (Time: 3.3s)Step 6600 of 1000000; Loss: 4.2041e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2275e+03; Test Loss: 4.9389e+00. (Time: 2.4s)Step 6800 of 1000000; Loss: 4.1865e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.0430e+01; Test Loss: 4.6671e+00. (Time: 2.5s)Step 7000 of 1000000; Loss: 4.1705e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.9141e+02; Test Loss: 4.4225e+00. (Time: 2.4s)Step 7200 of 1000000; Loss: 4.1563e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2228e+03; Test Loss: 4.2154e+00. (Time: 2.8s)Step 7400 of 1000000; Loss: 4.1423e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.0133e+01; Test Loss: 4.0313e+00. (Time: 3.3s)Step 7600 of 1000000; Loss: 4.1293e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.8452e+02; Test Loss: 3.8459e+00. (Time: 2.4s)Step 7800 of 1000000; Loss: 4.1170e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2175e+03; Test Loss: 3.6826e+00. (Time: 2.3s)Step 8000 of 1000000; Loss: 4.1052e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.2259e+01; Test Loss: 3.5264e+00. (Time: 2.3s)Step 8200 of 1000000; Loss: 4.0933e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.7754e+02; Test Loss: 3.3858e+00. (Time: 2.8s)Step 8400 of 1000000; Loss: 4.0818e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2120e+03; Test Loss: 3.2595e+00. (Time: 3.2s)Step 8600 of 1000000; Loss: 4.0703e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5660e+01; Test Loss: 3.1448e+00. (Time: 2.3s)Step 8800 of 1000000; Loss: 4.0590e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.7096e+02; Test Loss: 3.0248e+00. (Time: 2.3s)Step 9000 of 1000000; Loss: 4.0482e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2072e+03; Test Loss: 2.9155e+00. (Time: 2.3s)Step 9200 of 1000000; Loss: 4.0372e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.9913e+01; Test Loss: 2.8101e+00. (Time: 2.7s)Step 9400 of 1000000; Loss: 4.0264e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6537e+02; Test Loss: 2.7000e+00. (Time: 3.3s)Step 9600 of 1000000; Loss: 4.0161e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2028e+03; Test Loss: 2.6024e+00. (Time: 2.5s)Step 9800 of 1000000; Loss: 4.0056e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4732e+01; Test Loss: 2.5117e+00. (Time: 2.3s)Step 10000 of 1000000; Loss: 3.9961e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5996e+02; Test Loss: 2.4196e+00. (Time: 2.3s)Step 10200 of 1000000; Loss: 3.9871e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1982e+03; Test Loss: 2.3360e+00. (Time: 2.8s)Step 10400 of 1000000; Loss: 3.9786e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.0450e+01; Test Loss: 2.2594e+00. (Time: 3.4s)Step 10600 of 1000000; Loss: 3.9704e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5421e+02; Test Loss: 2.1793e+00. (Time: 2.6s)Step 10800 of 1000000; Loss: 3.9625e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1932e+03; Test Loss: 2.0968e+00. (Time: 2.4s)Step 11000 of 1000000; Loss: 3.9530e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6416e+01; Test Loss: 2.0188e+00. (Time: 2.4s)Step 11200 of 1000000; Loss: 3.9446e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.4878e+02; Test Loss: 1.9398e+00. (Time: 2.9s)Step 11400 of 1000000; Loss: 3.9365e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1885e+03; Test Loss: 1.8706e+00. (Time: 2.9s)Step 11600 of 1000000; Loss: 3.9291e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2687e+01; Test Loss: 1.8042e+00. (Time: 2.5s)Step 11800 of 1000000; Loss: 3.9211e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.4303e+02; Test Loss: 1.7432e+00. (Time: 2.3s)Step 12000 of 1000000; Loss: 3.9129e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1830e+03; Test Loss: 1.6895e+00. (Time: 2.2s)Step 12200 of 1000000; Loss: 3.9047e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9510e+01; Test Loss: 1.6332e+00. (Time: 3.1s)Step 12400 of 1000000; Loss: 3.8968e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3672e+02; Test Loss: 1.5529e+00. (Time: 2.7s)Step 12600 of 1000000; Loss: 3.8893e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1788e+03; Test Loss: 1.4936e+00. (Time: 2.2s)Step 12800 of 1000000; Loss: 3.8826e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5611e+01; Test Loss: 1.4298e+00. (Time: 2.4s)Step 13000 of 1000000; Loss: 3.8762e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3089e+02; Test Loss: 1.3673e+00. (Time: 2.4s)Step 13200 of 1000000; Loss: 3.8702e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1763e+03; Test Loss: 1.3277e+00. (Time: 2.7s)Step 13400 of 1000000; Loss: 3.8649e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2818e+01; Test Loss: 1.2882e+00. (Time: 3.1s)Step 13600 of 1000000; Loss: 3.8599e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2561e+02; Test Loss: 1.2456e+00. (Time: 2.4s)Step 13800 of 1000000; Loss: 3.8555e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1735e+03; Test Loss: 1.2199e+00. (Time: 2.6s)Step 14000 of 1000000; Loss: 3.8514e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0879e+01; Test Loss: 1.1917e+00. (Time: 2.4s)Step 14200 of 1000000; Loss: 3.8479e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2040e+02; Test Loss: 1.1580e+00. (Time: 3.2s)Step 14400 of 1000000; Loss: 3.8445e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1708e+03; Test Loss: 1.1399e+00. (Time: 2.9s)Step 14600 of 1000000; Loss: 3.8422e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9384e+01; Test Loss: 1.1242e+00. (Time: 2.3s)Step 14800 of 1000000; Loss: 3.8392e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.1527e+02; Test Loss: 1.1030e+00. (Time: 2.3s)Step 15000 of 1000000; Loss: 3.8369e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1679e+03; Test Loss: 1.0954e+00. (Time: 2.4s)Step 15200 of 1000000; Loss: 3.8349e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8352e+01; Test Loss: 1.0835e+00. (Time: 3.2s)Step 15400 of 1000000; Loss: 3.8326e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.1105e+02; Test Loss: 1.0666e+00. (Time: 2.8s)Step 15600 of 1000000; Loss: 3.8303e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1659e+03; Test Loss: 1.0641e+00. (Time: 2.5s)Step 15800 of 1000000; Loss: 3.8279e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7661e+01; Test Loss: 1.0587e+00. (Time: 2.6s)Step 16000 of 1000000; Loss: 3.8249e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0766e+02; Test Loss: 1.0439e+00. (Time: 2.5s)Step 16200 of 1000000; Loss: 3.8227e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1642e+03; Test Loss: 1.0407e+00. (Time: 3.4s)Step 16400 of 1000000; Loss: 3.8211e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7028e+01; Test Loss: 1.0340e+00. (Time: 2.6s)Step 16600 of 1000000; Loss: 3.8193e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0480e+02; Test Loss: 1.0211e+00. (Time: 2.4s)Step 16800 of 1000000; Loss: 3.8174e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1628e+03; Test Loss: 1.0183e+00. (Time: 2.4s)Step 17000 of 1000000; Loss: 3.8159e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6543e+01; Test Loss: 1.0134e+00. (Time: 2.4s)Step 17200 of 1000000; Loss: 3.8142e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0250e+02; Test Loss: 9.9959e-01. (Time: 3.4s)Step 17400 of 1000000; Loss: 3.8131e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1617e+03; Test Loss: 9.9627e-01. (Time: 2.6s)Step 17600 of 1000000; Loss: 3.8122e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6023e+01; Test Loss: 9.8874e-01. (Time: 2.3s)Step 17800 of 1000000; Loss: 3.8109e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0066e+02; Test Loss: 9.7534e-01. (Time: 2.3s)Step 18000 of 1000000; Loss: 3.8100e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1609e+03; Test Loss: 9.7364e-01. (Time: 2.6s)Step 18200 of 1000000; Loss: 3.8087e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5551e+01; Test Loss: 9.6611e-01. (Time: 3.6s)Step 18400 of 1000000; Loss: 3.8080e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9921e+02; Test Loss: 9.5174e-01. (Time: 2.3s)Step 18600 of 1000000; Loss: 3.8072e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1604e+03; Test Loss: 9.4892e-01. (Time: 2.4s)Step 18800 of 1000000; Loss: 3.8060e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5156e+01; Test Loss: 9.4438e-01. (Time: 2.3s)Step 19000 of 1000000; Loss: 3.8040e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9808e+02; Test Loss: 9.3281e-01. (Time: 2.3s)Step 19200 of 1000000; Loss: 3.8026e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1596e+03; Test Loss: 9.3040e-01. (Time: 3.6s)Step 19400 of 1000000; Loss: 3.8019e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4788e+01; Test Loss: 9.2350e-01. (Time: 2.5s)Step 19600 of 1000000; Loss: 3.8014e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9694e+02; Test Loss: 9.0995e-01. (Time: 2.5s)Step 19800 of 1000000; Loss: 3.8007e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1591e+03; Test Loss: 9.0807e-01. (Time: 2.3s)Step 20000 of 1000000; Loss: 3.7998e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4400e+01; Test Loss: 9.0163e-01. (Time: 2.3s)Step 20200 of 1000000; Loss: 3.7989e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9598e+02; Test Loss: 8.8763e-01. (Time: 3.6s)Step 20400 of 1000000; Loss: 3.7981e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1586e+03; Test Loss: 8.8503e-01. (Time: 2.4s)Step 20600 of 1000000; Loss: 3.7974e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3987e+01; Test Loss: 8.7986e-01. (Time: 2.3s)Step 20800 of 1000000; Loss: 3.7966e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9509e+02; Test Loss: 8.6733e-01. (Time: 2.4s)Step 21000 of 1000000; Loss: 3.7959e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1582e+03; Test Loss: 8.6618e-01. (Time: 2.2s)Step 21200 of 1000000; Loss: 3.7952e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3637e+01; Test Loss: 8.6102e-01. (Time: 3.5s)Step 21400 of 1000000; Loss: 3.7945e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9433e+02; Test Loss: 8.4880e-01. (Time: 2.4s)Step 21600 of 1000000; Loss: 3.7938e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1578e+03; Test Loss: 8.4797e-01. (Time: 2.3s)Step 21800 of 1000000; Loss: 3.7932e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3305e+01; Test Loss: 8.4324e-01. (Time: 2.4s)Step 22000 of 1000000; Loss: 3.7924e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9367e+02; Test Loss: 8.3395e-01. (Time: 2.4s)Step 22200 of 1000000; Loss: 3.7916e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1572e+03; Test Loss: 8.3434e-01. (Time: 3.7s)Step 22400 of 1000000; Loss: 3.7907e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3083e+01; Test Loss: 8.3126e-01. (Time: 2.4s)Step 22600 of 1000000; Loss: 3.7896e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9303e+02; Test Loss: 8.1907e-01. (Time: 2.3s)Step 22800 of 1000000; Loss: 3.7890e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1566e+03; Test Loss: 8.1616e-01. (Time: 2.4s)Step 23000 of 1000000; Loss: 3.7883e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2729e+01; Test Loss: 8.0945e-01. (Time: 2.6s)Step 23200 of 1000000; Loss: 3.7879e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9247e+02; Test Loss: 7.9973e-01. (Time: 3.7s)Step 23400 of 1000000; Loss: 3.7874e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1562e+03; Test Loss: 7.9822e-01. (Time: 2.4s)Step 23600 of 1000000; Loss: 3.7869e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2410e+01; Test Loss: 7.9302e-01. (Time: 2.5s)Step 23800 of 1000000; Loss: 3.7864e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9207e+02; Test Loss: 7.8168e-01. (Time: 2.4s)Step 24000 of 1000000; Loss: 3.7858e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1558e+03; Test Loss: 7.7809e-01. (Time: 2.7s)Step 24200 of 1000000; Loss: 3.7852e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2017e+01; Test Loss: 7.7259e-01. (Time: 3.6s)Step 24400 of 1000000; Loss: 3.7847e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9175e+02; Test Loss: 7.6398e-01. (Time: 2.5s)Step 24600 of 1000000; Loss: 3.7841e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1554e+03; Test Loss: 7.6284e-01. (Time: 2.6s)Step 24800 of 1000000; Loss: 3.7837e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1742e+01; Test Loss: 7.5654e-01. (Time: 2.5s)Step 25000 of 1000000; Loss: 3.7833e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9146e+02; Test Loss: 7.4647e-01. (Time: 3.1s)Step 25200 of 1000000; Loss: 3.7825e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1549e+03; Test Loss: 7.4397e-01. (Time: 2.9s)Step 25400 of 1000000; Loss: 3.7817e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1132e+01; Test Loss: 7.3969e-01. (Time: 2.4s)Step 25600 of 1000000; Loss: 3.7809e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9129e+02; Test Loss: 7.3150e-01. (Time: 2.4s)Step 25800 of 1000000; Loss: 3.7799e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1538e+03; Test Loss: 6.9354e-01. (Time: 2.4s)Step 26000 of 1000000; Loss: 3.7787e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0157e+01; Test Loss: 6.8307e-01. (Time: 2.8s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3718\n",
            "Running configuration: hidden_size=8, lr=0.0001, weight_decay=0.001, batch_size=64\n",
            "Step 200 of 200; Loss: 3.6120e+03; Test Loss: 1.6824e+03. (Time: 2.5s)Step 200 of 1000000; Loss: 1.6977e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7347e+03; Test Loss: 1.5819e+03. (Time: 2.2s)Step 400 of 1000000; Loss: 1.5926e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9638e+03; Test Loss: 1.5068e+03. (Time: 2.3s)Step 600 of 1000000; Loss: 1.5152e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1121e+02; Test Loss: 1.4464e+03. (Time: 2.2s)Step 800 of 1000000; Loss: 1.4532e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3869e+03; Test Loss: 1.4000e+03. (Time: 2.6s)Step 1000 of 1000000; Loss: 1.4050e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3994e+03; Test Loss: 1.3653e+03. (Time: 3.1s)Step 1200 of 1000000; Loss: 1.3691e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0964e+03; Test Loss: 1.3369e+03. (Time: 2.2s)Step 1400 of 1000000; Loss: 1.3401e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2619e+03; Test Loss: 1.3129e+03. (Time: 2.3s)Step 1600 of 1000000; Loss: 1.3157e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0873e+02; Test Loss: 1.2915e+03. (Time: 2.2s)Step 1800 of 1000000; Loss: 1.2940e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3351e+03; Test Loss: 1.2714e+03. (Time: 2.2s)Step 2000 of 1000000; Loss: 1.2739e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3350e+03; Test Loss: 1.2508e+03. (Time: 3.4s)Step 2200 of 1000000; Loss: 1.2535e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7954e+03; Test Loss: 1.2237e+03. (Time: 2.1s)Step 2400 of 1000000; Loss: 1.2279e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6423e+02; Test Loss: 1.1736e+03. (Time: 2.1s)Step 2600 of 1000000; Loss: 1.1792e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6563e+02; Test Loss: 1.1311e+03. (Time: 2.3s)Step 2800 of 1000000; Loss: 1.1361e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6301e+03; Test Loss: 1.0942e+03. (Time: 2.1s)Step 3000 of 1000000; Loss: 1.0984e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7022e+03; Test Loss: 1.0608e+03. (Time: 2.8s)Step 3200 of 1000000; Loss: 1.0647e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3933e+03; Test Loss: 1.0342e+03. (Time: 2.7s)Step 3400 of 1000000; Loss: 1.0372e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.4390e+02; Test Loss: 1.0119e+03. (Time: 2.2s)Step 3600 of 1000000; Loss: 1.0145e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2383e+02; Test Loss: 9.9162e+02. (Time: 2.1s)Step 3800 of 1000000; Loss: 9.9405e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2471e+03; Test Loss: 9.7202e+02. (Time: 2.2s)Step 4000 of 1000000; Loss: 9.7439e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5090e+03; Test Loss: 9.5263e+02. (Time: 2.4s)Step 4200 of 1000000; Loss: 9.5498e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1253e+03; Test Loss: 9.3367e+02. (Time: 3.3s)Step 4400 of 1000000; Loss: 9.3595e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4708e+02; Test Loss: 9.1558e+02. (Time: 2.3s)Step 4600 of 1000000; Loss: 9.1775e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.9535e+01; Test Loss: 8.9914e+02. (Time: 2.2s)Step 4800 of 1000000; Loss: 9.0101e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1357e+03; Test Loss: 8.8566e+02. (Time: 2.2s)Step 5000 of 1000000; Loss: 8.8708e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4534e+03; Test Loss: 8.7578e+02. (Time: 2.2s)Step 5200 of 1000000; Loss: 8.7685e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.4580e+02; Test Loss: 8.6770e+02. (Time: 3.5s)Step 5400 of 1000000; Loss: 8.6863e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4689e+02; Test Loss: 8.6113e+02. (Time: 2.2s)Step 5600 of 1000000; Loss: 8.6188e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9387e+01; Test Loss: 8.5525e+02. (Time: 2.2s)Step 5800 of 1000000; Loss: 8.5594e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1173e+03; Test Loss: 8.5008e+02. (Time: 2.2s)Step 6000 of 1000000; Loss: 8.5067e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4493e+03; Test Loss: 8.4544e+02. (Time: 2.3s)Step 6200 of 1000000; Loss: 8.4598e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.7642e+02; Test Loss: 8.4136e+02. (Time: 3.2s)Step 6400 of 1000000; Loss: 8.4183e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6792e+02; Test Loss: 8.3776e+02. (Time: 2.5s)Step 6600 of 1000000; Loss: 8.3819e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7025e+01; Test Loss: 8.3443e+02. (Time: 2.1s)Step 6800 of 1000000; Loss: 8.3481e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0958e+03; Test Loss: 8.3137e+02. (Time: 2.2s)Step 7000 of 1000000; Loss: 8.3172e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4321e+03; Test Loss: 8.2854e+02. (Time: 8.1s)Step 7200 of 1000000; Loss: 8.2888e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.3993e+02; Test Loss: 8.2588e+02. (Time: 2.2s)Step 7400 of 1000000; Loss: 8.2620e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3263e+02; Test Loss: 8.2338e+02. (Time: 2.2s)Step 7600 of 1000000; Loss: 8.2369e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1164e+01; Test Loss: 8.2091e+02. (Time: 2.4s)Step 7800 of 1000000; Loss: 8.2121e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0700e+03; Test Loss: 8.1839e+02. (Time: 2.5s)Step 8000 of 1000000; Loss: 8.1869e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4105e+03; Test Loss: 8.1579e+02. (Time: 3.2s)Step 8200 of 1000000; Loss: 8.1611e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.1367e+02; Test Loss: 8.1323e+02. (Time: 2.3s)Step 8400 of 1000000; Loss: 8.1354e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0907e+02; Test Loss: 8.1079e+02. (Time: 2.1s)Step 8600 of 1000000; Loss: 8.1109e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6964e+01; Test Loss: 8.0823e+02. (Time: 2.2s)Step 8800 of 1000000; Loss: 8.0856e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0452e+03; Test Loss: 8.0560e+02. (Time: 2.2s)Step 9000 of 1000000; Loss: 8.0591e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3891e+03; Test Loss: 8.0325e+02. (Time: 3.4s)Step 9200 of 1000000; Loss: 8.0354e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.9226e+02; Test Loss: 8.0101e+02. (Time: 2.2s)Step 9400 of 1000000; Loss: 8.0128e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9640e+01; Test Loss: 7.9880e+02. (Time: 2.5s)Step 9600 of 1000000; Loss: 7.9909e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3567e+01; Test Loss: 7.9652e+02. (Time: 2.2s)Step 9800 of 1000000; Loss: 7.9681e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0213e+03; Test Loss: 7.9407e+02. (Time: 2.1s)Step 10000 of 1000000; Loss: 7.9437e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3649e+03; Test Loss: 7.9175e+02. (Time: 3.1s)Step 10200 of 1000000; Loss: 7.9201e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.7308e+02; Test Loss: 7.8939e+02. (Time: 2.6s)Step 10400 of 1000000; Loss: 7.8967e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1523e+01; Test Loss: 7.8709e+02. (Time: 2.4s)Step 10600 of 1000000; Loss: 7.8735e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0294e+01; Test Loss: 7.8497e+02. (Time: 2.1s)Step 10800 of 1000000; Loss: 7.8521e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9934e+03; Test Loss: 7.8298e+02. (Time: 2.2s)Step 11000 of 1000000; Loss: 7.8327e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3401e+03; Test Loss: 7.8122e+02. (Time: 3.0s)Step 11200 of 1000000; Loss: 7.8140e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.5437e+02; Test Loss: 7.7939e+02. (Time: 2.9s)Step 11400 of 1000000; Loss: 7.7958e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.6234e+01; Test Loss: 7.7765e+02. (Time: 2.1s)Step 11600 of 1000000; Loss: 7.7786e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.6182e+00; Test Loss: 7.7580e+02. (Time: 2.4s)Step 11800 of 1000000; Loss: 7.7604e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9647e+03; Test Loss: 7.7423e+02. (Time: 2.4s)Step 12000 of 1000000; Loss: 7.7444e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3212e+03; Test Loss: 7.7272e+02. (Time: 2.9s)Step 12200 of 1000000; Loss: 7.7286e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.3936e+02; Test Loss: 7.7136e+02. (Time: 3.1s)Step 12400 of 1000000; Loss: 7.7149e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5755e+01; Test Loss: 7.7007e+02. (Time: 2.1s)Step 12600 of 1000000; Loss: 7.7020e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.8586e+00; Test Loss: 7.6894e+02. (Time: 2.4s)Step 12800 of 1000000; Loss: 7.6909e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9418e+03; Test Loss: 7.6780e+02. (Time: 2.3s)Step 13000 of 1000000; Loss: 7.6796e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3064e+03; Test Loss: 7.6666e+02. (Time: 2.5s)Step 13200 of 1000000; Loss: 7.6680e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.2909e+02; Test Loss: 7.6566e+02. (Time: 3.2s)Step 13400 of 1000000; Loss: 7.6575e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9971e+01; Test Loss: 7.6459e+02. (Time: 2.1s)Step 13600 of 1000000; Loss: 7.6470e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.8305e+00; Test Loss: 7.6366e+02. (Time: 2.3s)Step 13800 of 1000000; Loss: 7.6379e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9231e+03; Test Loss: 7.6288e+02. (Time: 2.1s)Step 14000 of 1000000; Loss: 7.6300e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2952e+03; Test Loss: 7.6225e+02. (Time: 2.1s)Step 14200 of 1000000; Loss: 7.6232e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.2241e+02; Test Loss: 7.6174e+02. (Time: 3.0s)Step 14400 of 1000000; Loss: 7.6173e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6132e+01; Test Loss: 7.6118e+02. (Time: 2.6s)Step 14600 of 1000000; Loss: 7.6123e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1290e+00; Test Loss: 7.6055e+02. (Time: 2.3s)Step 14800 of 1000000; Loss: 7.6064e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9100e+03; Test Loss: 7.6004e+02. (Time: 2.2s)Step 15000 of 1000000; Loss: 7.6015e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2886e+03; Test Loss: 7.5954e+02. (Time: 2.1s)Step 15200 of 1000000; Loss: 7.5959e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1769e+02; Test Loss: 7.5915e+02. (Time: 2.8s)Step 15400 of 1000000; Loss: 7.5915e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2676e+01; Test Loss: 7.5878e+02. (Time: 3.0s)Step 15600 of 1000000; Loss: 7.5881e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5354e+00; Test Loss: 7.5835e+02. (Time: 2.4s)Step 15800 of 1000000; Loss: 7.5842e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9010e+03; Test Loss: 7.5779e+02. (Time: 2.1s)Step 16000 of 1000000; Loss: 7.5789e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2830e+03; Test Loss: 7.5734e+02. (Time: 2.4s)Step 16200 of 1000000; Loss: 7.5737e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1466e+02; Test Loss: 7.5690e+02. (Time: 2.5s)Step 16400 of 1000000; Loss: 7.5691e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9144e+01; Test Loss: 7.5641e+02. (Time: 3.3s)Step 16600 of 1000000; Loss: 7.5645e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0190e+00; Test Loss: 7.5583e+02. (Time: 2.4s)Step 16800 of 1000000; Loss: 7.5595e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8949e+03; Test Loss: 7.5535e+02. (Time: 2.3s)Step 17000 of 1000000; Loss: 7.5542e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2788e+03; Test Loss: 7.5505e+02. (Time: 2.1s)Step 17200 of 1000000; Loss: 7.5509e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1068e+02; Test Loss: 7.5466e+02. (Time: 2.4s)Step 17400 of 1000000; Loss: 7.5468e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3624e+01; Test Loss: 7.5445e+02. (Time: 3.4s)Step 17600 of 1000000; Loss: 7.5442e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0876e+00; Test Loss: 7.5450e+02. (Time: 2.2s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3583\n",
            "Running configuration: hidden_size=8, lr=0.0001, weight_decay=0.0001, batch_size=32\n",
            "Step 200 of 200; Loss: 1.8166e+03; Test Loss: 7.4962e+01. (Time: 2.3s)Step 200 of 1000000; Loss: 8.4723e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1030e+03; Test Loss: 6.5260e+01. (Time: 2.3s)Step 400 of 1000000; Loss: 7.9388e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7115e+03; Test Loss: 5.7897e+01. (Time: 2.9s)Step 600 of 1000000; Loss: 7.5503e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7128e+03; Test Loss: 5.1805e+01. (Time: 3.1s)Step 800 of 1000000; Loss: 7.2423e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.9849e+02; Test Loss: 4.6958e+01. (Time: 2.4s)Step 1000 of 1000000; Loss: 7.0047e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6823e+03; Test Loss: 4.3108e+01. (Time: 2.4s)Step 1200 of 1000000; Loss: 6.8287e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6814e+03; Test Loss: 3.9788e+01. (Time: 2.4s)Step 1400 of 1000000; Loss: 6.6830e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.3103e+02; Test Loss: 3.6951e+01. (Time: 2.8s)Step 1600 of 1000000; Loss: 6.5599e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6702e+03; Test Loss: 3.4500e+01. (Time: 2.9s)Step 1800 of 1000000; Loss: 6.4516e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6582e+03; Test Loss: 3.2318e+01. (Time: 2.4s)Step 2000 of 1000000; Loss: 6.3487e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2060e+02; Test Loss: 3.0353e+01. (Time: 2.3s)Step 2200 of 1000000; Loss: 6.2445e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6088e+03; Test Loss: 2.8582e+01. (Time: 2.5s)Step 2400 of 1000000; Loss: 6.1158e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5336e+03; Test Loss: 2.7770e+01. (Time: 2.6s)Step 2600 of 1000000; Loss: 5.8859e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.6110e+02; Test Loss: 2.7008e+01. (Time: 3.2s)Step 2800 of 1000000; Loss: 5.6823e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2974e+03; Test Loss: 2.6327e+01. (Time: 2.5s)Step 3000 of 1000000; Loss: 5.5028e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3498e+03; Test Loss: 2.5675e+01. (Time: 2.4s)Step 3200 of 1000000; Loss: 5.3457e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1005e+02; Test Loss: 2.4625e+01. (Time: 2.5s)Step 3400 of 1000000; Loss: 5.2118e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1250e+03; Test Loss: 2.3073e+01. (Time: 3.0s)Step 3600 of 1000000; Loss: 5.1038e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2741e+03; Test Loss: 2.1583e+01. (Time: 3.0s)Step 3800 of 1000000; Loss: 5.0057e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2959e+02; Test Loss: 1.9997e+01. (Time: 2.3s)Step 4000 of 1000000; Loss: 4.9119e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0591e+03; Test Loss: 1.8267e+01. (Time: 2.3s)Step 4200 of 1000000; Loss: 4.8192e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2412e+03; Test Loss: 1.6433e+01. (Time: 2.6s)Step 4400 of 1000000; Loss: 4.7252e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4335e+02; Test Loss: 1.4568e+01. (Time: 3.2s)Step 4600 of 1000000; Loss: 4.6332e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0211e+03; Test Loss: 1.2589e+01. (Time: 3.0s)Step 4800 of 1000000; Loss: 4.5457e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2280e+03; Test Loss: 1.1073e+01. (Time: 2.4s)Step 5000 of 1000000; Loss: 4.4707e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6706e+02; Test Loss: 9.8907e+00. (Time: 2.4s)Step 5200 of 1000000; Loss: 4.4133e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0047e+03; Test Loss: 8.8454e+00. (Time: 2.4s)Step 5400 of 1000000; Loss: 4.3685e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2303e+03; Test Loss: 7.9849e+00. (Time: 3.1s)Step 5600 of 1000000; Loss: 4.3304e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2296e+02; Test Loss: 7.2524e+00. (Time: 2.6s)Step 5800 of 1000000; Loss: 4.2988e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0018e+03; Test Loss: 6.6073e+00. (Time: 2.2s)Step 6000 of 1000000; Loss: 4.2705e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2307e+03; Test Loss: 6.0779e+00. (Time: 2.3s)Step 6200 of 1000000; Loss: 4.2460e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6290e+01; Test Loss: 5.6402e+00. (Time: 2.4s)Step 6400 of 1000000; Loss: 4.2236e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.9719e+02; Test Loss: 5.2612e+00. (Time: 3.1s)Step 6600 of 1000000; Loss: 4.2041e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2275e+03; Test Loss: 4.9388e+00. (Time: 3.1s)Step 6800 of 1000000; Loss: 4.1865e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.0426e+01; Test Loss: 4.6668e+00. (Time: 2.4s)Step 7000 of 1000000; Loss: 4.1705e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.9141e+02; Test Loss: 4.4223e+00. (Time: 2.4s)Step 7200 of 1000000; Loss: 4.1562e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2228e+03; Test Loss: 4.2151e+00. (Time: 2.4s)Step 7400 of 1000000; Loss: 4.1423e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.0129e+01; Test Loss: 4.0309e+00. (Time: 3.0s)Step 7600 of 1000000; Loss: 4.1293e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.8452e+02; Test Loss: 3.8457e+00. (Time: 2.9s)Step 7800 of 1000000; Loss: 4.1170e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2175e+03; Test Loss: 3.6826e+00. (Time: 2.3s)Step 8000 of 1000000; Loss: 4.1052e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.2261e+01; Test Loss: 3.5264e+00. (Time: 2.4s)Step 8200 of 1000000; Loss: 4.0933e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.7754e+02; Test Loss: 3.3856e+00. (Time: 2.5s)Step 8400 of 1000000; Loss: 4.0817e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2120e+03; Test Loss: 3.2594e+00. (Time: 3.5s)Step 8600 of 1000000; Loss: 4.0703e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5659e+01; Test Loss: 3.1446e+00. (Time: 2.7s)Step 8800 of 1000000; Loss: 4.0590e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.7096e+02; Test Loss: 3.0246e+00. (Time: 2.4s)Step 9000 of 1000000; Loss: 4.0482e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2072e+03; Test Loss: 2.9153e+00. (Time: 2.4s)Step 9200 of 1000000; Loss: 4.0371e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.9911e+01; Test Loss: 2.8100e+00. (Time: 2.5s)Step 9400 of 1000000; Loss: 4.0264e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6537e+02; Test Loss: 2.7000e+00. (Time: 3.5s)Step 9600 of 1000000; Loss: 4.0161e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2028e+03; Test Loss: 2.6022e+00. (Time: 2.4s)Step 9800 of 1000000; Loss: 4.0056e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4730e+01; Test Loss: 2.5116e+00. (Time: 2.4s)Step 10000 of 1000000; Loss: 3.9961e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5996e+02; Test Loss: 2.4197e+00. (Time: 2.2s)Step 10200 of 1000000; Loss: 3.9871e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1982e+03; Test Loss: 2.3360e+00. (Time: 2.3s)Step 10400 of 1000000; Loss: 3.9786e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.0450e+01; Test Loss: 2.2594e+00. (Time: 3.4s)Step 10600 of 1000000; Loss: 3.9704e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5421e+02; Test Loss: 2.1792e+00. (Time: 2.7s)Step 10800 of 1000000; Loss: 3.9625e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1932e+03; Test Loss: 2.0968e+00. (Time: 2.3s)Step 11000 of 1000000; Loss: 3.9530e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6416e+01; Test Loss: 2.0188e+00. (Time: 2.4s)Step 11200 of 1000000; Loss: 3.9446e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.4878e+02; Test Loss: 1.9396e+00. (Time: 2.5s)Step 11400 of 1000000; Loss: 3.9365e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1885e+03; Test Loss: 1.8706e+00. (Time: 3.6s)Step 11600 of 1000000; Loss: 3.9291e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2689e+01; Test Loss: 1.8043e+00. (Time: 2.3s)Step 11800 of 1000000; Loss: 3.9210e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.4303e+02; Test Loss: 1.7429e+00. (Time: 2.5s)Step 12000 of 1000000; Loss: 3.9130e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1830e+03; Test Loss: 1.6895e+00. (Time: 2.4s)Step 12200 of 1000000; Loss: 3.9047e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9507e+01; Test Loss: 1.6331e+00. (Time: 2.3s)Step 12400 of 1000000; Loss: 3.8968e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3673e+02; Test Loss: 1.5527e+00. (Time: 3.8s)Step 12600 of 1000000; Loss: 3.8893e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1788e+03; Test Loss: 1.4935e+00. (Time: 2.3s)Step 12800 of 1000000; Loss: 3.8826e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5610e+01; Test Loss: 1.4298e+00. (Time: 2.3s)Step 13000 of 1000000; Loss: 3.8762e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3089e+02; Test Loss: 1.3672e+00. (Time: 2.6s)Step 13200 of 1000000; Loss: 3.8702e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1763e+03; Test Loss: 1.3278e+00. (Time: 2.5s)Step 13400 of 1000000; Loss: 3.8649e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2818e+01; Test Loss: 1.2882e+00. (Time: 3.6s)Step 13600 of 1000000; Loss: 3.8599e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2561e+02; Test Loss: 1.2456e+00. (Time: 2.3s)Step 13800 of 1000000; Loss: 3.8555e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1735e+03; Test Loss: 1.2200e+00. (Time: 2.3s)Step 14000 of 1000000; Loss: 3.8514e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0881e+01; Test Loss: 1.1918e+00. (Time: 2.4s)Step 14200 of 1000000; Loss: 3.8478e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2041e+02; Test Loss: 1.1581e+00. (Time: 2.4s)Step 14400 of 1000000; Loss: 3.8445e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1708e+03; Test Loss: 1.1399e+00. (Time: 3.4s)Step 14600 of 1000000; Loss: 3.8422e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9382e+01; Test Loss: 1.1242e+00. (Time: 2.6s)Step 14800 of 1000000; Loss: 3.8393e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.1526e+02; Test Loss: 1.1031e+00. (Time: 2.3s)Step 15000 of 1000000; Loss: 3.8369e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1679e+03; Test Loss: 1.0954e+00. (Time: 2.3s)Step 15200 of 1000000; Loss: 3.8349e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8353e+01; Test Loss: 1.0836e+00. (Time: 2.4s)Step 15400 of 1000000; Loss: 3.8326e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.1104e+02; Test Loss: 1.0667e+00. (Time: 3.5s)Step 15600 of 1000000; Loss: 3.8303e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1659e+03; Test Loss: 1.0639e+00. (Time: 2.5s)Step 15800 of 1000000; Loss: 3.8280e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7661e+01; Test Loss: 1.0588e+00. (Time: 2.4s)Step 16000 of 1000000; Loss: 3.8249e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0765e+02; Test Loss: 1.0440e+00. (Time: 2.4s)Step 16200 of 1000000; Loss: 3.8227e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1642e+03; Test Loss: 1.0407e+00. (Time: 3.0s)Step 16400 of 1000000; Loss: 3.8211e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7033e+01; Test Loss: 1.0343e+00. (Time: 3.2s)Step 16600 of 1000000; Loss: 3.8192e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0479e+02; Test Loss: 1.0211e+00. (Time: 2.5s)Step 16800 of 1000000; Loss: 3.8173e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1628e+03; Test Loss: 1.0183e+00. (Time: 2.5s)Step 17000 of 1000000; Loss: 3.8159e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6541e+01; Test Loss: 1.0132e+00. (Time: 2.6s)Step 17200 of 1000000; Loss: 3.8142e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0249e+02; Test Loss: 9.9952e-01. (Time: 3.6s)Step 17400 of 1000000; Loss: 3.8131e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1617e+03; Test Loss: 9.9619e-01. (Time: 2.6s)Step 17600 of 1000000; Loss: 3.8122e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6022e+01; Test Loss: 9.8868e-01. (Time: 2.5s)Step 17800 of 1000000; Loss: 3.8109e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0065e+02; Test Loss: 9.7519e-01. (Time: 2.5s)Step 18000 of 1000000; Loss: 3.8100e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1609e+03; Test Loss: 9.7357e-01. (Time: 2.4s)Step 18200 of 1000000; Loss: 3.8087e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5552e+01; Test Loss: 9.6615e-01. (Time: 3.8s)Step 18400 of 1000000; Loss: 3.8080e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9921e+02; Test Loss: 9.5173e-01. (Time: 2.5s)Step 18600 of 1000000; Loss: 3.8072e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1604e+03; Test Loss: 9.4896e-01. (Time: 2.5s)Step 18800 of 1000000; Loss: 3.8061e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5161e+01; Test Loss: 9.4466e-01. (Time: 2.6s)Step 19000 of 1000000; Loss: 3.8039e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9808e+02; Test Loss: 9.3283e-01. (Time: 2.6s)Step 19200 of 1000000; Loss: 3.8026e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1596e+03; Test Loss: 9.3035e-01. (Time: 3.3s)Step 19400 of 1000000; Loss: 3.8019e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4789e+01; Test Loss: 9.2356e-01. (Time: 2.4s)Step 19600 of 1000000; Loss: 3.8014e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9693e+02; Test Loss: 9.0988e-01. (Time: 2.4s)Step 19800 of 1000000; Loss: 3.8008e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1591e+03; Test Loss: 9.0802e-01. (Time: 2.6s)Step 20000 of 1000000; Loss: 3.7998e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4397e+01; Test Loss: 9.0152e-01. (Time: 2.7s)Step 20200 of 1000000; Loss: 3.7989e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9598e+02; Test Loss: 8.8759e-01. (Time: 3.0s)Step 20400 of 1000000; Loss: 3.7981e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1587e+03; Test Loss: 8.8509e-01. (Time: 2.3s)Step 20600 of 1000000; Loss: 3.7974e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3988e+01; Test Loss: 8.7995e-01. (Time: 2.5s)Step 20800 of 1000000; Loss: 3.7966e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9508e+02; Test Loss: 8.6750e-01. (Time: 2.5s)Step 21000 of 1000000; Loss: 3.7960e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1582e+03; Test Loss: 8.6626e-01. (Time: 2.8s)Step 21200 of 1000000; Loss: 3.7952e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3636e+01; Test Loss: 8.6101e-01. (Time: 3.0s)Step 21400 of 1000000; Loss: 3.7945e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9433e+02; Test Loss: 8.4895e-01. (Time: 2.4s)Step 21600 of 1000000; Loss: 3.7938e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1578e+03; Test Loss: 8.4815e-01. (Time: 2.4s)Step 21800 of 1000000; Loss: 3.7932e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3306e+01; Test Loss: 8.4340e-01. (Time: 8.5s)Step 22000 of 1000000; Loss: 3.7924e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9367e+02; Test Loss: 8.3405e-01. (Time: 2.5s)Step 22200 of 1000000; Loss: 3.7916e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1572e+03; Test Loss: 8.3449e-01. (Time: 2.3s)Step 22400 of 1000000; Loss: 3.7907e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3087e+01; Test Loss: 8.3140e-01. (Time: 2.5s)Step 22600 of 1000000; Loss: 3.7897e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9302e+02; Test Loss: 8.1941e-01. (Time: 3.0s)Step 22800 of 1000000; Loss: 3.7890e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1566e+03; Test Loss: 8.1605e-01. (Time: 2.8s)Step 23000 of 1000000; Loss: 3.7883e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2737e+01; Test Loss: 8.0988e-01. (Time: 2.3s)Step 23200 of 1000000; Loss: 3.7879e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9246e+02; Test Loss: 8.0011e-01. (Time: 2.3s)Step 23400 of 1000000; Loss: 3.7874e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1562e+03; Test Loss: 7.9858e-01. (Time: 2.4s)Step 23600 of 1000000; Loss: 3.7869e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2414e+01; Test Loss: 7.9316e-01. (Time: 2.9s)Step 23800 of 1000000; Loss: 3.7864e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9207e+02; Test Loss: 7.8169e-01. (Time: 3.0s)Step 24000 of 1000000; Loss: 3.7858e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1558e+03; Test Loss: 7.7836e-01. (Time: 2.3s)Step 24200 of 1000000; Loss: 3.7852e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2012e+01; Test Loss: 7.7249e-01. (Time: 2.6s)Step 24400 of 1000000; Loss: 3.7847e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9175e+02; Test Loss: 7.6402e-01. (Time: 2.3s)Step 24600 of 1000000; Loss: 3.7841e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1554e+03; Test Loss: 7.6266e-01. (Time: 3.5s)Step 24800 of 1000000; Loss: 3.7838e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1742e+01; Test Loss: 7.5656e-01. (Time: 2.5s)Step 25000 of 1000000; Loss: 3.7833e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9145e+02; Test Loss: 7.4659e-01. (Time: 2.5s)Step 25200 of 1000000; Loss: 3.7825e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1549e+03; Test Loss: 7.4466e-01. (Time: 2.6s)Step 25400 of 1000000; Loss: 3.7818e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1153e+01; Test Loss: 7.3960e-01. (Time: 2.3s)Step 25600 of 1000000; Loss: 3.7810e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9128e+02; Test Loss: 7.3160e-01. (Time: 3.6s)Step 25800 of 1000000; Loss: 3.7799e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1538e+03; Test Loss: 6.9787e-01. (Time: 2.5s)Step 26000 of 1000000; Loss: 3.7788e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0156e+01; Test Loss: 6.8322e-01. (Time: 2.3s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3718\n",
            "Running configuration: hidden_size=8, lr=0.0001, weight_decay=0.0001, batch_size=64\n",
            "Step 200 of 200; Loss: 3.6120e+03; Test Loss: 1.6824e+03. (Time: 2.3s)Step 200 of 1000000; Loss: 1.6977e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7347e+03; Test Loss: 1.5819e+03. (Time: 2.4s)Step 400 of 1000000; Loss: 1.5926e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9638e+03; Test Loss: 1.5068e+03. (Time: 3.7s)Step 600 of 1000000; Loss: 1.5152e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1121e+02; Test Loss: 1.4464e+03. (Time: 2.3s)Step 800 of 1000000; Loss: 1.4532e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3869e+03; Test Loss: 1.4000e+03. (Time: 2.3s)Step 1000 of 1000000; Loss: 1.4050e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3994e+03; Test Loss: 1.3653e+03. (Time: 2.1s)Step 1200 of 1000000; Loss: 1.3691e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0964e+03; Test Loss: 1.3369e+03. (Time: 2.1s)Step 1400 of 1000000; Loss: 1.3401e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2619e+03; Test Loss: 1.3129e+03. (Time: 3.5s)Step 1600 of 1000000; Loss: 1.3157e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0873e+02; Test Loss: 1.2915e+03. (Time: 2.4s)Step 1800 of 1000000; Loss: 1.2940e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3351e+03; Test Loss: 1.2714e+03. (Time: 2.5s)Step 2000 of 1000000; Loss: 1.2739e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3350e+03; Test Loss: 1.2508e+03. (Time: 2.1s)Step 2200 of 1000000; Loss: 1.2535e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7955e+03; Test Loss: 1.2237e+03. (Time: 2.3s)Step 2400 of 1000000; Loss: 1.2279e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6423e+02; Test Loss: 1.1736e+03. (Time: 2.9s)Step 2600 of 1000000; Loss: 1.1792e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6563e+02; Test Loss: 1.1311e+03. (Time: 2.5s)Step 2800 of 1000000; Loss: 1.1361e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6301e+03; Test Loss: 1.0942e+03. (Time: 2.1s)Step 3000 of 1000000; Loss: 1.0984e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7022e+03; Test Loss: 1.0608e+03. (Time: 2.2s)Step 3200 of 1000000; Loss: 1.0647e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3934e+03; Test Loss: 1.0342e+03. (Time: 2.1s)Step 3400 of 1000000; Loss: 1.0372e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.4391e+02; Test Loss: 1.0119e+03. (Time: 2.1s)Step 3600 of 1000000; Loss: 1.0145e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2383e+02; Test Loss: 9.9162e+02. (Time: 3.5s)Step 3800 of 1000000; Loss: 9.9405e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2471e+03; Test Loss: 9.7202e+02. (Time: 2.2s)Step 4000 of 1000000; Loss: 9.7439e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5090e+03; Test Loss: 9.5262e+02. (Time: 2.4s)Step 4200 of 1000000; Loss: 9.5498e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1253e+03; Test Loss: 9.3366e+02. (Time: 2.1s)Step 4400 of 1000000; Loss: 9.3595e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4706e+02; Test Loss: 9.1558e+02. (Time: 2.2s)Step 4600 of 1000000; Loss: 9.1775e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.9533e+01; Test Loss: 8.9913e+02. (Time: 3.7s)Step 4800 of 1000000; Loss: 9.0101e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1357e+03; Test Loss: 8.8566e+02. (Time: 2.1s)Step 5000 of 1000000; Loss: 8.8708e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4534e+03; Test Loss: 8.7578e+02. (Time: 2.4s)Step 5200 of 1000000; Loss: 8.7685e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.4579e+02; Test Loss: 8.6770e+02. (Time: 2.5s)Step 5400 of 1000000; Loss: 8.6863e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4688e+02; Test Loss: 8.6113e+02. (Time: 2.4s)Step 5600 of 1000000; Loss: 8.6188e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9385e+01; Test Loss: 8.5525e+02. (Time: 3.7s)Step 5800 of 1000000; Loss: 8.5594e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1173e+03; Test Loss: 8.5008e+02. (Time: 2.2s)Step 6000 of 1000000; Loss: 8.5067e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4493e+03; Test Loss: 8.4544e+02. (Time: 2.2s)Step 6200 of 1000000; Loss: 8.4597e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.7642e+02; Test Loss: 8.4136e+02. (Time: 2.2s)Step 6400 of 1000000; Loss: 8.4183e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6792e+02; Test Loss: 8.3775e+02. (Time: 2.3s)Step 6600 of 1000000; Loss: 8.3819e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7024e+01; Test Loss: 8.3443e+02. (Time: 3.5s)Step 6800 of 1000000; Loss: 8.3481e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0958e+03; Test Loss: 8.3137e+02. (Time: 2.6s)Step 7000 of 1000000; Loss: 8.3172e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4321e+03; Test Loss: 8.2854e+02. (Time: 2.3s)Step 7200 of 1000000; Loss: 8.2888e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.3993e+02; Test Loss: 8.2588e+02. (Time: 2.3s)Step 7400 of 1000000; Loss: 8.2620e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3263e+02; Test Loss: 8.2338e+02. (Time: 2.2s)Step 7600 of 1000000; Loss: 8.2369e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1163e+01; Test Loss: 8.2091e+02. (Time: 3.1s)Step 7800 of 1000000; Loss: 8.2121e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0700e+03; Test Loss: 8.1839e+02. (Time: 2.6s)Step 8000 of 1000000; Loss: 8.1869e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4105e+03; Test Loss: 8.1579e+02. (Time: 2.2s)Step 8200 of 1000000; Loss: 8.1611e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.1366e+02; Test Loss: 8.1323e+02. (Time: 2.4s)Step 8400 of 1000000; Loss: 8.1354e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0907e+02; Test Loss: 8.1078e+02. (Time: 2.3s)Step 8600 of 1000000; Loss: 8.1109e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6964e+01; Test Loss: 8.0823e+02. (Time: 2.8s)Step 8800 of 1000000; Loss: 8.0856e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0452e+03; Test Loss: 8.0561e+02. (Time: 3.1s)Step 9000 of 1000000; Loss: 8.0592e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3891e+03; Test Loss: 8.0325e+02. (Time: 2.5s)Step 9200 of 1000000; Loss: 8.0354e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.9226e+02; Test Loss: 8.0102e+02. (Time: 2.3s)Step 9400 of 1000000; Loss: 8.0129e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9637e+01; Test Loss: 7.9881e+02. (Time: 2.1s)Step 9600 of 1000000; Loss: 7.9909e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3567e+01; Test Loss: 7.9652e+02. (Time: 2.6s)Step 9800 of 1000000; Loss: 7.9681e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0213e+03; Test Loss: 7.9407e+02. (Time: 3.2s)Step 10000 of 1000000; Loss: 7.9437e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3649e+03; Test Loss: 7.9175e+02. (Time: 2.4s)Step 10200 of 1000000; Loss: 7.9201e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.7308e+02; Test Loss: 7.8939e+02. (Time: 2.4s)Step 10400 of 1000000; Loss: 7.8967e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1520e+01; Test Loss: 7.8709e+02. (Time: 2.3s)Step 10600 of 1000000; Loss: 7.8735e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0294e+01; Test Loss: 7.8497e+02. (Time: 2.6s)Step 10800 of 1000000; Loss: 7.8521e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9934e+03; Test Loss: 7.8298e+02. (Time: 3.1s)Step 11000 of 1000000; Loss: 7.8326e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3401e+03; Test Loss: 7.8122e+02. (Time: 2.2s)Step 11200 of 1000000; Loss: 7.8140e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.5437e+02; Test Loss: 7.7939e+02. (Time: 2.2s)Step 11400 of 1000000; Loss: 7.7958e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.6234e+01; Test Loss: 7.7765e+02. (Time: 2.1s)Step 11600 of 1000000; Loss: 7.7786e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.6179e+00; Test Loss: 7.7580e+02. (Time: 2.2s)Step 11800 of 1000000; Loss: 7.7604e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9647e+03; Test Loss: 7.7423e+02. (Time: 3.4s)Step 12000 of 1000000; Loss: 7.7444e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3212e+03; Test Loss: 7.7272e+02. (Time: 2.2s)Step 12200 of 1000000; Loss: 7.7286e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.3936e+02; Test Loss: 7.7137e+02. (Time: 2.4s)Step 12400 of 1000000; Loss: 7.7149e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5759e+01; Test Loss: 7.7007e+02. (Time: 2.3s)Step 12600 of 1000000; Loss: 7.7020e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.8584e+00; Test Loss: 7.6894e+02. (Time: 2.4s)Step 12800 of 1000000; Loss: 7.6909e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9418e+03; Test Loss: 7.6780e+02. (Time: 3.1s)Step 13000 of 1000000; Loss: 7.6796e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3064e+03; Test Loss: 7.6666e+02. (Time: 2.6s)Step 13200 of 1000000; Loss: 7.6680e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.2909e+02; Test Loss: 7.6566e+02. (Time: 2.3s)Step 13400 of 1000000; Loss: 7.6575e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9967e+01; Test Loss: 7.6459e+02. (Time: 2.3s)Step 13600 of 1000000; Loss: 7.6470e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.8297e+00; Test Loss: 7.6366e+02. (Time: 2.4s)Step 13800 of 1000000; Loss: 7.6379e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9231e+03; Test Loss: 7.6288e+02. (Time: 3.0s)Step 14000 of 1000000; Loss: 7.6300e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2952e+03; Test Loss: 7.6225e+02. (Time: 2.7s)Step 14200 of 1000000; Loss: 7.6232e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.2240e+02; Test Loss: 7.6174e+02. (Time: 2.2s)Step 14400 of 1000000; Loss: 7.6173e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6141e+01; Test Loss: 7.6117e+02. (Time: 2.2s)Step 14600 of 1000000; Loss: 7.6123e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1291e+00; Test Loss: 7.6052e+02. (Time: 2.1s)Step 14800 of 1000000; Loss: 7.6061e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9100e+03; Test Loss: 7.6006e+02. (Time: 2.3s)Step 15000 of 1000000; Loss: 7.6015e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2886e+03; Test Loss: 7.5954e+02. (Time: 3.4s)Step 15200 of 1000000; Loss: 7.5958e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1768e+02; Test Loss: 7.5916e+02. (Time: 2.4s)Step 15400 of 1000000; Loss: 7.5915e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2679e+01; Test Loss: 7.5878e+02. (Time: 2.3s)Step 15600 of 1000000; Loss: 7.5881e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5342e+00; Test Loss: 7.5835e+02. (Time: 2.2s)Step 15800 of 1000000; Loss: 7.5842e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9010e+03; Test Loss: 7.5779e+02. (Time: 2.3s)Step 16000 of 1000000; Loss: 7.5789e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2831e+03; Test Loss: 7.5733e+02. (Time: 3.4s)Step 16200 of 1000000; Loss: 7.5737e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1466e+02; Test Loss: 7.5690e+02. (Time: 2.2s)Step 16400 of 1000000; Loss: 7.5691e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9133e+01; Test Loss: 7.5639e+02. (Time: 2.2s)Step 16600 of 1000000; Loss: 7.5645e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0169e+00; Test Loss: 7.5579e+02. (Time: 2.4s)Step 16800 of 1000000; Loss: 7.5594e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8949e+03; Test Loss: 7.5534e+02. (Time: 2.2s)Step 17000 of 1000000; Loss: 7.5542e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2787e+03; Test Loss: 7.5505e+02. (Time: 3.2s)Step 17200 of 1000000; Loss: 7.5508e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1067e+02; Test Loss: 7.5465e+02. (Time: 2.6s)Step 17400 of 1000000; Loss: 7.5468e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3630e+01; Test Loss: 7.5445e+02. (Time: 2.4s)Step 17600 of 1000000; Loss: 7.5442e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0878e+00; Test Loss: 7.5449e+02. (Time: 2.2s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3583\n",
            "Running configuration: hidden_size=8, lr=0.0001, weight_decay=1e-05, batch_size=32\n",
            "Step 200 of 200; Loss: 1.8166e+03; Test Loss: 7.4962e+01. (Time: 2.5s)Step 200 of 1000000; Loss: 8.4723e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1030e+03; Test Loss: 6.5260e+01. (Time: 3.9s)Step 400 of 1000000; Loss: 7.9388e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7115e+03; Test Loss: 5.7897e+01. (Time: 2.5s)Step 600 of 1000000; Loss: 7.5503e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7128e+03; Test Loss: 5.1805e+01. (Time: 2.5s)Step 800 of 1000000; Loss: 7.2423e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.9849e+02; Test Loss: 4.6958e+01. (Time: 2.5s)Step 1000 of 1000000; Loss: 7.0047e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6823e+03; Test Loss: 4.3108e+01. (Time: 2.4s)Step 1200 of 1000000; Loss: 6.8287e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6814e+03; Test Loss: 3.9788e+01. (Time: 3.5s)Step 1400 of 1000000; Loss: 6.6830e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.3103e+02; Test Loss: 3.6951e+01. (Time: 2.3s)Step 1600 of 1000000; Loss: 6.5599e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6702e+03; Test Loss: 3.4500e+01. (Time: 2.4s)Step 1800 of 1000000; Loss: 6.4516e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6582e+03; Test Loss: 3.2318e+01. (Time: 2.4s)Step 2000 of 1000000; Loss: 6.3487e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2060e+02; Test Loss: 3.0353e+01. (Time: 2.8s)Step 2200 of 1000000; Loss: 6.2445e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6088e+03; Test Loss: 2.8582e+01. (Time: 3.2s)Step 2400 of 1000000; Loss: 6.1158e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5336e+03; Test Loss: 2.7769e+01. (Time: 2.4s)Step 2600 of 1000000; Loss: 5.8859e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.6110e+02; Test Loss: 2.7008e+01. (Time: 2.6s)Step 2800 of 1000000; Loss: 5.6823e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2974e+03; Test Loss: 2.6327e+01. (Time: 2.8s)Step 3000 of 1000000; Loss: 5.5028e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3498e+03; Test Loss: 2.5675e+01. (Time: 3.1s)Step 3200 of 1000000; Loss: 5.3457e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1005e+02; Test Loss: 2.4625e+01. (Time: 2.8s)Step 3400 of 1000000; Loss: 5.2118e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1250e+03; Test Loss: 2.3072e+01. (Time: 2.4s)Step 3600 of 1000000; Loss: 5.1038e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2741e+03; Test Loss: 2.1583e+01. (Time: 2.3s)Step 3800 of 1000000; Loss: 5.0057e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2958e+02; Test Loss: 1.9997e+01. (Time: 2.4s)Step 4000 of 1000000; Loss: 4.9119e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0591e+03; Test Loss: 1.8266e+01. (Time: 3.3s)Step 4200 of 1000000; Loss: 4.8192e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2412e+03; Test Loss: 1.6432e+01. (Time: 2.7s)Step 4400 of 1000000; Loss: 4.7252e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4334e+02; Test Loss: 1.4567e+01. (Time: 2.5s)Step 4600 of 1000000; Loss: 4.6332e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0211e+03; Test Loss: 1.2589e+01. (Time: 2.5s)Step 4800 of 1000000; Loss: 4.5457e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2280e+03; Test Loss: 1.1072e+01. (Time: 2.4s)Step 5000 of 1000000; Loss: 4.4707e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6705e+02; Test Loss: 9.8901e+00. (Time: 4.0s)Step 5200 of 1000000; Loss: 4.4133e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0047e+03; Test Loss: 8.8449e+00. (Time: 2.7s)Step 5400 of 1000000; Loss: 4.3685e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2303e+03; Test Loss: 7.9845e+00. (Time: 2.4s)Step 5600 of 1000000; Loss: 4.3304e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2296e+02; Test Loss: 7.2520e+00. (Time: 2.5s)Step 5800 of 1000000; Loss: 4.2988e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0018e+03; Test Loss: 6.6070e+00. (Time: 2.4s)Step 6000 of 1000000; Loss: 4.2705e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2307e+03; Test Loss: 6.0777e+00. (Time: 3.6s)Step 6200 of 1000000; Loss: 4.2459e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6286e+01; Test Loss: 5.6400e+00. (Time: 2.6s)Step 6400 of 1000000; Loss: 4.2236e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.9719e+02; Test Loss: 5.2611e+00. (Time: 2.6s)Step 6600 of 1000000; Loss: 4.2040e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2275e+03; Test Loss: 4.9387e+00. (Time: 2.4s)Step 6800 of 1000000; Loss: 4.1865e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.0426e+01; Test Loss: 4.6668e+00. (Time: 3.3s)Step 7000 of 1000000; Loss: 4.1705e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.9141e+02; Test Loss: 4.4223e+00. (Time: 3.0s)Step 7200 of 1000000; Loss: 4.1563e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2228e+03; Test Loss: 4.2152e+00. (Time: 2.3s)Step 7400 of 1000000; Loss: 4.1423e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.0130e+01; Test Loss: 4.0310e+00. (Time: 2.4s)Step 7600 of 1000000; Loss: 4.1293e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.8452e+02; Test Loss: 3.8458e+00. (Time: 2.4s)Step 7800 of 1000000; Loss: 4.1170e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2175e+03; Test Loss: 3.6826e+00. (Time: 3.1s)Step 8000 of 1000000; Loss: 4.1052e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.2260e+01; Test Loss: 3.5264e+00. (Time: 3.1s)Step 8200 of 1000000; Loss: 4.0933e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.7754e+02; Test Loss: 3.3857e+00. (Time: 2.6s)Step 8400 of 1000000; Loss: 4.0818e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2121e+03; Test Loss: 3.2594e+00. (Time: 2.3s)Step 8600 of 1000000; Loss: 4.0703e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5658e+01; Test Loss: 3.1446e+00. (Time: 2.5s)Step 8800 of 1000000; Loss: 4.0590e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.7096e+02; Test Loss: 3.0246e+00. (Time: 3.5s)Step 9000 of 1000000; Loss: 4.0482e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2072e+03; Test Loss: 2.9153e+00. (Time: 2.5s)Step 9200 of 1000000; Loss: 4.0371e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.9913e+01; Test Loss: 2.8101e+00. (Time: 2.5s)Step 9400 of 1000000; Loss: 4.0264e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6537e+02; Test Loss: 2.6999e+00. (Time: 2.5s)Step 9600 of 1000000; Loss: 4.0161e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2028e+03; Test Loss: 2.6023e+00. (Time: 2.4s)Step 9800 of 1000000; Loss: 4.0056e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4730e+01; Test Loss: 2.5116e+00. (Time: 4.0s)Step 10000 of 1000000; Loss: 3.9961e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5997e+02; Test Loss: 2.4196e+00. (Time: 2.3s)Step 10200 of 1000000; Loss: 3.9871e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1982e+03; Test Loss: 2.3359e+00. (Time: 2.5s)Step 10400 of 1000000; Loss: 3.9786e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.0450e+01; Test Loss: 2.2594e+00. (Time: 2.3s)Step 10600 of 1000000; Loss: 3.9704e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5421e+02; Test Loss: 2.1791e+00. (Time: 2.4s)Step 10800 of 1000000; Loss: 3.9625e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1932e+03; Test Loss: 2.0968e+00. (Time: 3.6s)Step 11000 of 1000000; Loss: 3.9530e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6416e+01; Test Loss: 2.0188e+00. (Time: 2.3s)Step 11200 of 1000000; Loss: 3.9446e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.4878e+02; Test Loss: 1.9399e+00. (Time: 8.2s)Step 11400 of 1000000; Loss: 3.9365e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1885e+03; Test Loss: 1.8705e+00. (Time: 3.2s)Step 11600 of 1000000; Loss: 3.9291e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2692e+01; Test Loss: 1.8045e+00. (Time: 2.4s)Step 11800 of 1000000; Loss: 3.9211e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.4303e+02; Test Loss: 1.7430e+00. (Time: 2.3s)Step 12000 of 1000000; Loss: 3.9130e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1830e+03; Test Loss: 1.6897e+00. (Time: 2.3s)Step 12200 of 1000000; Loss: 3.9047e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9508e+01; Test Loss: 1.6331e+00. (Time: 3.0s)Step 12400 of 1000000; Loss: 3.8968e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3673e+02; Test Loss: 1.5528e+00. (Time: 3.3s)Step 12600 of 1000000; Loss: 3.8893e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1788e+03; Test Loss: 1.4937e+00. (Time: 2.4s)Step 12800 of 1000000; Loss: 3.8826e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5612e+01; Test Loss: 1.4301e+00. (Time: 2.4s)Step 13000 of 1000000; Loss: 3.8762e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3089e+02; Test Loss: 1.3673e+00. (Time: 2.4s)Step 13200 of 1000000; Loss: 3.8701e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1763e+03; Test Loss: 1.3277e+00. (Time: 3.3s)Step 13400 of 1000000; Loss: 3.8649e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2817e+01; Test Loss: 1.2882e+00. (Time: 3.1s)Step 13600 of 1000000; Loss: 3.8599e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2561e+02; Test Loss: 1.2457e+00. (Time: 2.5s)Step 13800 of 1000000; Loss: 3.8555e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1735e+03; Test Loss: 1.2201e+00. (Time: 2.6s)Step 14000 of 1000000; Loss: 3.8514e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0882e+01; Test Loss: 1.1919e+00. (Time: 2.4s)Step 14200 of 1000000; Loss: 3.8479e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2042e+02; Test Loss: 1.1583e+00. (Time: 3.2s)Step 14400 of 1000000; Loss: 3.8445e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1708e+03; Test Loss: 1.1400e+00. (Time: 2.8s)Step 14600 of 1000000; Loss: 3.8422e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9380e+01; Test Loss: 1.1240e+00. (Time: 2.6s)Step 14800 of 1000000; Loss: 3.8393e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.1528e+02; Test Loss: 1.1026e+00. (Time: 2.3s)Step 15000 of 1000000; Loss: 3.8369e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1679e+03; Test Loss: 1.0954e+00. (Time: 2.3s)Step 15200 of 1000000; Loss: 3.8349e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8349e+01; Test Loss: 1.0834e+00. (Time: 3.5s)Step 15400 of 1000000; Loss: 3.8326e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.1105e+02; Test Loss: 1.0665e+00. (Time: 2.4s)Step 15600 of 1000000; Loss: 3.8303e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1659e+03; Test Loss: 1.0640e+00. (Time: 2.3s)Step 15800 of 1000000; Loss: 3.8279e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7660e+01; Test Loss: 1.0587e+00. (Time: 2.4s)Step 16000 of 1000000; Loss: 3.8249e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0765e+02; Test Loss: 1.0439e+00. (Time: 2.3s)Step 16200 of 1000000; Loss: 3.8227e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1642e+03; Test Loss: 1.0406e+00. (Time: 3.3s)Step 16400 of 1000000; Loss: 3.8211e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7027e+01; Test Loss: 1.0340e+00. (Time: 2.6s)Step 16600 of 1000000; Loss: 3.8192e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0479e+02; Test Loss: 1.0212e+00. (Time: 2.4s)Step 16800 of 1000000; Loss: 3.8174e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1628e+03; Test Loss: 1.0183e+00. (Time: 2.4s)Step 17000 of 1000000; Loss: 3.8160e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6543e+01; Test Loss: 1.0134e+00. (Time: 2.4s)Step 17200 of 1000000; Loss: 3.8142e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0249e+02; Test Loss: 9.9962e-01. (Time: 3.6s)Step 17400 of 1000000; Loss: 3.8131e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1617e+03; Test Loss: 9.9636e-01. (Time: 2.5s)Step 17600 of 1000000; Loss: 3.8122e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6022e+01; Test Loss: 9.8868e-01. (Time: 2.4s)Step 17800 of 1000000; Loss: 3.8109e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0065e+02; Test Loss: 9.7533e-01. (Time: 2.4s)Step 18000 of 1000000; Loss: 3.8100e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1609e+03; Test Loss: 9.7362e-01. (Time: 2.4s)Step 18200 of 1000000; Loss: 3.8087e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5555e+01; Test Loss: 9.6631e-01. (Time: 3.6s)Step 18400 of 1000000; Loss: 3.8080e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9921e+02; Test Loss: 9.5198e-01. (Time: 2.3s)Step 18600 of 1000000; Loss: 3.8072e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1604e+03; Test Loss: 9.4902e-01. (Time: 2.3s)Step 18800 of 1000000; Loss: 3.8060e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5164e+01; Test Loss: 9.4487e-01. (Time: 2.4s)Step 19000 of 1000000; Loss: 3.8039e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9807e+02; Test Loss: 9.3357e-01. (Time: 2.5s)Step 19200 of 1000000; Loss: 3.8026e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1596e+03; Test Loss: 9.3147e-01. (Time: 3.5s)Step 19400 of 1000000; Loss: 3.8019e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4808e+01; Test Loss: 9.2474e-01. (Time: 2.5s)Step 19600 of 1000000; Loss: 3.8015e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9692e+02; Test Loss: 9.1075e-01. (Time: 2.3s)Step 19800 of 1000000; Loss: 3.8008e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1591e+03; Test Loss: 9.0874e-01. (Time: 2.5s)Step 20000 of 1000000; Loss: 3.7999e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4407e+01; Test Loss: 9.0216e-01. (Time: 2.9s)Step 20200 of 1000000; Loss: 3.7990e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9598e+02; Test Loss: 8.8801e-01. (Time: 3.4s)Step 20400 of 1000000; Loss: 3.7982e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1587e+03; Test Loss: 8.8539e-01. (Time: 2.5s)Step 20600 of 1000000; Loss: 3.7974e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3994e+01; Test Loss: 8.8033e-01. (Time: 2.5s)Step 20800 of 1000000; Loss: 3.7967e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9508e+02; Test Loss: 8.6798e-01. (Time: 2.5s)Step 21000 of 1000000; Loss: 3.7960e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1582e+03; Test Loss: 8.6644e-01. (Time: 3.1s)Step 21200 of 1000000; Loss: 3.7953e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3643e+01; Test Loss: 8.6149e-01. (Time: 3.1s)Step 21400 of 1000000; Loss: 3.7945e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9432e+02; Test Loss: 8.4934e-01. (Time: 2.5s)Step 21600 of 1000000; Loss: 3.7939e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1578e+03; Test Loss: 8.4832e-01. (Time: 2.4s)Step 21800 of 1000000; Loss: 3.7933e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3311e+01; Test Loss: 8.4372e-01. (Time: 2.3s)Step 22000 of 1000000; Loss: 3.7924e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9366e+02; Test Loss: 8.3438e-01. (Time: 3.3s)Step 22200 of 1000000; Loss: 3.7916e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1572e+03; Test Loss: 8.3447e-01. (Time: 2.8s)Step 22400 of 1000000; Loss: 3.7907e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3100e+01; Test Loss: 8.3222e-01. (Time: 2.3s)Step 22600 of 1000000; Loss: 3.7896e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9301e+02; Test Loss: 8.2000e-01. (Time: 2.5s)Step 22800 of 1000000; Loss: 3.7890e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1567e+03; Test Loss: 8.1671e-01. (Time: 2.6s)Step 23000 of 1000000; Loss: 3.7883e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2734e+01; Test Loss: 8.0969e-01. (Time: 3.2s)Step 23200 of 1000000; Loss: 3.7879e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9247e+02; Test Loss: 7.9973e-01. (Time: 2.6s)Step 23400 of 1000000; Loss: 3.7874e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1562e+03; Test Loss: 7.9821e-01. (Time: 2.5s)Step 23600 of 1000000; Loss: 3.7869e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2411e+01; Test Loss: 7.9297e-01. (Time: 2.4s)Step 23800 of 1000000; Loss: 3.7864e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9207e+02; Test Loss: 7.8170e-01. (Time: 2.5s)Step 24000 of 1000000; Loss: 3.7858e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1558e+03; Test Loss: 7.7831e-01. (Time: 3.6s)Step 24200 of 1000000; Loss: 3.7852e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2017e+01; Test Loss: 7.7260e-01. (Time: 2.5s)Step 24400 of 1000000; Loss: 3.7847e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9175e+02; Test Loss: 7.6391e-01. (Time: 2.3s)Step 24600 of 1000000; Loss: 3.7841e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1555e+03; Test Loss: 7.6295e-01. (Time: 2.4s)Step 24800 of 1000000; Loss: 3.7838e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1750e+01; Test Loss: 7.5692e-01. (Time: 2.4s)Step 25000 of 1000000; Loss: 3.7833e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9144e+02; Test Loss: 7.4714e-01. (Time: 3.7s)Step 25200 of 1000000; Loss: 3.7826e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1549e+03; Test Loss: 7.4444e-01. (Time: 2.4s)Step 25400 of 1000000; Loss: 3.7818e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1163e+01; Test Loss: 7.3959e-01. (Time: 2.6s)Step 25600 of 1000000; Loss: 3.7810e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9127e+02; Test Loss: 7.3134e-01. (Time: 2.7s)Step 25800 of 1000000; Loss: 3.7799e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1538e+03; Test Loss: 7.0156e-01. (Time: 2.7s)Step 26000 of 1000000; Loss: 3.7789e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0158e+01; Test Loss: 6.8339e-01. (Time: 3.3s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3718\n",
            "Running configuration: hidden_size=8, lr=0.0001, weight_decay=1e-05, batch_size=64\n",
            "Step 200 of 200; Loss: 3.6120e+03; Test Loss: 1.6824e+03. (Time: 2.1s)Step 200 of 1000000; Loss: 1.6977e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7347e+03; Test Loss: 1.5819e+03. (Time: 2.1s)Step 400 of 1000000; Loss: 1.5926e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9638e+03; Test Loss: 1.5068e+03. (Time: 2.3s)Step 600 of 1000000; Loss: 1.5152e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1121e+02; Test Loss: 1.4464e+03. (Time: 2.1s)Step 800 of 1000000; Loss: 1.4532e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3869e+03; Test Loss: 1.4000e+03. (Time: 3.6s)Step 1000 of 1000000; Loss: 1.4050e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3994e+03; Test Loss: 1.3653e+03. (Time: 2.4s)Step 1200 of 1000000; Loss: 1.3691e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0964e+03; Test Loss: 1.3369e+03. (Time: 2.4s)Step 1400 of 1000000; Loss: 1.3401e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2619e+03; Test Loss: 1.3129e+03. (Time: 2.2s)Step 1600 of 1000000; Loss: 1.3157e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0873e+02; Test Loss: 1.2915e+03. (Time: 2.3s)Step 1800 of 1000000; Loss: 1.2940e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3351e+03; Test Loss: 1.2714e+03. (Time: 3.6s)Step 2000 of 1000000; Loss: 1.2739e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3350e+03; Test Loss: 1.2508e+03. (Time: 2.2s)Step 2200 of 1000000; Loss: 1.2535e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7955e+03; Test Loss: 1.2237e+03. (Time: 2.2s)Step 2400 of 1000000; Loss: 1.2279e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6423e+02; Test Loss: 1.1736e+03. (Time: 2.3s)Step 2600 of 1000000; Loss: 1.1792e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6563e+02; Test Loss: 1.1311e+03. (Time: 2.2s)Step 2800 of 1000000; Loss: 1.1361e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6301e+03; Test Loss: 1.0942e+03. (Time: 3.1s)Step 3000 of 1000000; Loss: 1.0984e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7022e+03; Test Loss: 1.0608e+03. (Time: 2.5s)Step 3200 of 1000000; Loss: 1.0647e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3934e+03; Test Loss: 1.0342e+03. (Time: 2.2s)Step 3400 of 1000000; Loss: 1.0372e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.4391e+02; Test Loss: 1.0119e+03. (Time: 2.2s)Step 3600 of 1000000; Loss: 1.0145e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2383e+02; Test Loss: 9.9162e+02. (Time: 2.1s)Step 3800 of 1000000; Loss: 9.9405e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2471e+03; Test Loss: 9.7202e+02. (Time: 2.5s)Step 4000 of 1000000; Loss: 9.7439e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5090e+03; Test Loss: 9.5262e+02. (Time: 3.3s)Step 4200 of 1000000; Loss: 9.5498e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1253e+03; Test Loss: 9.3366e+02. (Time: 2.2s)Step 4400 of 1000000; Loss: 9.3595e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4706e+02; Test Loss: 9.1558e+02. (Time: 2.1s)Step 4600 of 1000000; Loss: 9.1774e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.9533e+01; Test Loss: 8.9913e+02. (Time: 2.2s)Step 4800 of 1000000; Loss: 9.0101e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1357e+03; Test Loss: 8.8566e+02. (Time: 2.1s)Step 5000 of 1000000; Loss: 8.8708e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4534e+03; Test Loss: 8.7578e+02. (Time: 3.4s)Step 5200 of 1000000; Loss: 8.7685e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.4579e+02; Test Loss: 8.6770e+02. (Time: 2.4s)Step 5400 of 1000000; Loss: 8.6863e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4688e+02; Test Loss: 8.6113e+02. (Time: 2.4s)Step 5600 of 1000000; Loss: 8.6188e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9385e+01; Test Loss: 8.5525e+02. (Time: 2.3s)Step 5800 of 1000000; Loss: 8.5594e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1173e+03; Test Loss: 8.5008e+02. (Time: 2.4s)Step 6000 of 1000000; Loss: 8.5067e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4493e+03; Test Loss: 8.4544e+02. (Time: 3.0s)Step 6200 of 1000000; Loss: 8.4598e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.7642e+02; Test Loss: 8.4136e+02. (Time: 2.5s)Step 6400 of 1000000; Loss: 8.4183e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6792e+02; Test Loss: 8.3776e+02. (Time: 2.4s)Step 6600 of 1000000; Loss: 8.3819e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7024e+01; Test Loss: 8.3443e+02. (Time: 2.3s)Step 6800 of 1000000; Loss: 8.3481e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0958e+03; Test Loss: 8.3137e+02. (Time: 2.3s)Step 7000 of 1000000; Loss: 8.3172e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4321e+03; Test Loss: 8.2854e+02. (Time: 2.8s)Step 7200 of 1000000; Loss: 8.2888e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.3993e+02; Test Loss: 8.2588e+02. (Time: 2.9s)Step 7400 of 1000000; Loss: 8.2620e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3264e+02; Test Loss: 8.2339e+02. (Time: 2.5s)Step 7600 of 1000000; Loss: 8.2369e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1164e+01; Test Loss: 8.2091e+02. (Time: 2.2s)Step 7800 of 1000000; Loss: 8.2121e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0700e+03; Test Loss: 8.1839e+02. (Time: 2.1s)Step 8000 of 1000000; Loss: 8.1869e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4105e+03; Test Loss: 8.1579e+02. (Time: 2.2s)Step 8200 of 1000000; Loss: 8.1611e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.1367e+02; Test Loss: 8.1323e+02. (Time: 3.2s)Step 8400 of 1000000; Loss: 8.1354e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0908e+02; Test Loss: 8.1079e+02. (Time: 2.2s)Step 8600 of 1000000; Loss: 8.1109e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6964e+01; Test Loss: 8.0823e+02. (Time: 2.2s)Step 8800 of 1000000; Loss: 8.0856e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0452e+03; Test Loss: 8.0560e+02. (Time: 2.2s)Step 9000 of 1000000; Loss: 8.0591e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3891e+03; Test Loss: 8.0325e+02. (Time: 2.2s)Step 9200 of 1000000; Loss: 8.0354e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.9226e+02; Test Loss: 8.0101e+02. (Time: 3.2s)Step 9400 of 1000000; Loss: 8.0129e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9637e+01; Test Loss: 7.9880e+02. (Time: 2.3s)Step 9600 of 1000000; Loss: 7.9909e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3567e+01; Test Loss: 7.9652e+02. (Time: 2.3s)Step 9800 of 1000000; Loss: 7.9681e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0213e+03; Test Loss: 7.9407e+02. (Time: 2.4s)Step 10000 of 1000000; Loss: 7.9437e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3649e+03; Test Loss: 7.9175e+02. (Time: 2.4s)Step 10200 of 1000000; Loss: 7.9201e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.7308e+02; Test Loss: 7.8939e+02. (Time: 3.2s)Step 10400 of 1000000; Loss: 7.8967e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1518e+01; Test Loss: 7.8709e+02. (Time: 2.7s)Step 10600 of 1000000; Loss: 7.8735e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0295e+01; Test Loss: 7.8497e+02. (Time: 2.4s)Step 10800 of 1000000; Loss: 7.8521e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9934e+03; Test Loss: 7.8298e+02. (Time: 2.4s)Step 11000 of 1000000; Loss: 7.8327e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3401e+03; Test Loss: 7.8123e+02. (Time: 2.4s)Step 11200 of 1000000; Loss: 7.8140e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.5437e+02; Test Loss: 7.7939e+02. (Time: 3.1s)Step 11400 of 1000000; Loss: 7.7958e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.6242e+01; Test Loss: 7.7765e+02. (Time: 2.7s)Step 11600 of 1000000; Loss: 7.7786e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.6186e+00; Test Loss: 7.7580e+02. (Time: 2.4s)Step 11800 of 1000000; Loss: 7.7604e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9647e+03; Test Loss: 7.7424e+02. (Time: 2.2s)Step 12000 of 1000000; Loss: 7.7444e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3212e+03; Test Loss: 7.7273e+02. (Time: 2.2s)Step 12200 of 1000000; Loss: 7.7287e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.3937e+02; Test Loss: 7.7137e+02. (Time: 2.4s)Step 12400 of 1000000; Loss: 7.7149e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5769e+01; Test Loss: 7.7007e+02. (Time: 3.5s)Step 12600 of 1000000; Loss: 7.7020e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.8586e+00; Test Loss: 7.6894e+02. (Time: 2.3s)Step 12800 of 1000000; Loss: 7.6909e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9418e+03; Test Loss: 7.6779e+02. (Time: 2.1s)Step 13000 of 1000000; Loss: 7.6795e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3064e+03; Test Loss: 7.6666e+02. (Time: 2.1s)Step 13200 of 1000000; Loss: 7.6679e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.2910e+02; Test Loss: 7.6566e+02. (Time: 2.4s)Step 13400 of 1000000; Loss: 7.6575e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9987e+01; Test Loss: 7.6458e+02. (Time: 3.4s)Step 13600 of 1000000; Loss: 7.6468e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.8363e+00; Test Loss: 7.6364e+02. (Time: 2.3s)Step 13800 of 1000000; Loss: 7.6376e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9231e+03; Test Loss: 7.6286e+02. (Time: 2.3s)Step 14000 of 1000000; Loss: 7.6298e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2952e+03; Test Loss: 7.6225e+02. (Time: 2.2s)Step 14200 of 1000000; Loss: 7.6232e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.2238e+02; Test Loss: 7.6174e+02. (Time: 2.1s)Step 14400 of 1000000; Loss: 7.6174e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6155e+01; Test Loss: 7.6114e+02. (Time: 3.2s)Step 14600 of 1000000; Loss: 7.6120e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1302e+00; Test Loss: 7.6047e+02. (Time: 2.5s)Step 14800 of 1000000; Loss: 7.6057e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9099e+03; Test Loss: 7.5998e+02. (Time: 2.3s)Step 15000 of 1000000; Loss: 7.6008e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2886e+03; Test Loss: 7.5949e+02. (Time: 2.2s)Step 15200 of 1000000; Loss: 7.5955e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1768e+02; Test Loss: 7.5911e+02. (Time: 2.4s)Step 15400 of 1000000; Loss: 7.5910e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2632e+01; Test Loss: 7.5873e+02. (Time: 3.0s)Step 15600 of 1000000; Loss: 7.5878e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5315e+00; Test Loss: 7.5833e+02. (Time: 2.7s)Step 15800 of 1000000; Loss: 7.5840e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9010e+03; Test Loss: 7.5776e+02. (Time: 2.3s)Step 16000 of 1000000; Loss: 7.5786e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2830e+03; Test Loss: 7.5731e+02. (Time: 2.1s)Step 16200 of 1000000; Loss: 7.5734e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1462e+02; Test Loss: 7.5688e+02. (Time: 2.1s)Step 16400 of 1000000; Loss: 7.5689e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9075e+01; Test Loss: 7.5636e+02. (Time: 2.2s)Step 16600 of 1000000; Loss: 7.5641e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0139e+00; Test Loss: 7.5582e+02. (Time: 3.2s)Step 16800 of 1000000; Loss: 7.5588e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8949e+03; Test Loss: 7.5529e+02. (Time: 2.3s)Step 17000 of 1000000; Loss: 7.5539e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2786e+03; Test Loss: 7.5501e+02. (Time: 2.4s)Step 17200 of 1000000; Loss: 7.5504e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1053e+02; Test Loss: 7.5461e+02. (Time: 2.2s)Step 17400 of 1000000; Loss: 7.5462e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3609e+01; Test Loss: 7.5448e+02. (Time: 2.3s)Step 17600 of 1000000; Loss: 7.5446e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0931e+00; Test Loss: 7.5453e+02. (Time: 3.5s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3583\n",
            "Running configuration: hidden_size=16, lr=0.001, weight_decay=0.001, batch_size=32\n",
            "Step 200 of 200; Loss: 1.5045e+03; Test Loss: 2.1460e+01. (Time: 3.2s)Step 200 of 1000000; Loss: 5.6825e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.8917e+01; Test Loss: 1.8962e+00. (Time: 3.1s)Step 400 of 1000000; Loss: 4.2598e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5796e+02; Test Loss: 1.4176e+00. (Time: 3.8s)Step 600 of 1000000; Loss: 3.9894e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2013e+03; Test Loss: 9.3934e+00. (Time: 4.3s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.4178\n",
            "Running configuration: hidden_size=16, lr=0.001, weight_decay=0.001, batch_size=64\n",
            "Step 200 of 200; Loss: 3.0907e+03; Test Loss: 1.1148e+03. (Time: 3.3s)Step 200 of 1000000; Loss: 1.1459e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9175e+02; Test Loss: 8.2853e+02. (Time: 3.2s)Step 400 of 1000000; Loss: 8.4496e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1751e+01; Test Loss: 7.9835e+02. (Time: 4.4s)Step 600 of 1000000; Loss: 7.9961e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.5771e+00; Test Loss: 7.9044e+02. (Time: 3.1s)Step 800 of 1000000; Loss: 7.9145e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9893e+03; Test Loss: 7.8207e+02. (Time: 3.0s)Step 1000 of 1000000; Loss: 7.8316e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3152e+03; Test Loss: 7.7291e+02. (Time: 9.5s)Step 1200 of 1000000; Loss: 7.7392e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.2777e+02; Test Loss: 7.6571e+02. (Time: 3.1s)Step 1400 of 1000000; Loss: 7.6648e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.0196e+01; Test Loss: 7.6054e+02. (Time: 3.1s)Step 1600 of 1000000; Loss: 7.6121e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6846e+02; Test Loss: 1.1317e+03. (Time: 4.5s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.5374\n",
            "Running configuration: hidden_size=16, lr=0.001, weight_decay=0.0001, batch_size=32\n",
            "Step 200 of 200; Loss: 1.5045e+03; Test Loss: 2.1460e+01. (Time: 3.4s)Step 200 of 1000000; Loss: 5.6825e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.9063e+01; Test Loss: 1.8993e+00. (Time: 3.5s)Step 400 of 1000000; Loss: 4.2603e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5814e+02; Test Loss: 1.4146e+00. (Time: 4.5s)Step 600 of 1000000; Loss: 3.9901e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2345e+03; Test Loss: 2.3033e+01. (Time: 3.4s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.4668\n",
            "Running configuration: hidden_size=16, lr=0.001, weight_decay=0.0001, batch_size=64\n",
            "Step 200 of 200; Loss: 3.0907e+03; Test Loss: 1.1148e+03. (Time: 3.0s)Step 200 of 1000000; Loss: 1.1459e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9173e+02; Test Loss: 8.2862e+02. (Time: 3.5s)Step 400 of 1000000; Loss: 8.4510e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1425e+01; Test Loss: 7.9832e+02. (Time: 4.1s)Step 600 of 1000000; Loss: 7.9962e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9239e+01; Test Loss: 7.9528e+02. (Time: 3.1s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3776\n",
            "Running configuration: hidden_size=16, lr=0.001, weight_decay=1e-05, batch_size=32\n",
            "Step 200 of 200; Loss: 1.5045e+03; Test Loss: 2.1460e+01. (Time: 3.3s)Step 200 of 1000000; Loss: 5.6825e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.8903e+01; Test Loss: 1.8975e+00. (Time: 4.6s)Step 400 of 1000000; Loss: 4.2601e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5804e+02; Test Loss: 1.4150e+00. (Time: 3.2s)Step 600 of 1000000; Loss: 3.9897e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1931e+03; Test Loss: 1.4862e+01. (Time: 3.4s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.4304\n",
            "Running configuration: hidden_size=16, lr=0.001, weight_decay=1e-05, batch_size=64\n",
            "Step 200 of 200; Loss: 3.0907e+03; Test Loss: 1.1148e+03. (Time: 3.6s)Step 200 of 1000000; Loss: 1.1459e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9114e+02; Test Loss: 8.2830e+02. (Time: 3.9s)Step 400 of 1000000; Loss: 8.4484e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1302e+01; Test Loss: 7.9822e+02. (Time: 3.1s)Step 600 of 1000000; Loss: 7.9953e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0117e+01; Test Loss: 8.2760e+02. (Time: 3.3s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3930\n",
            "Running configuration: hidden_size=16, lr=0.0001, weight_decay=0.001, batch_size=32\n",
            "Step 200 of 200; Loss: 2.8811e+03; Test Loss: 1.7406e+02. (Time: 4.6s)Step 200 of 1000000; Loss: 1.6016e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0137e+03; Test Loss: 1.1861e+02. (Time: 3.3s)Step 400 of 1000000; Loss: 1.1642e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8661e+03; Test Loss: 8.8010e+01. (Time: 3.4s)Step 600 of 1000000; Loss: 9.4511e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7590e+03; Test Loss: 6.6442e+01. (Time: 3.3s)Step 800 of 1000000; Loss: 8.0498e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.2064e+02; Test Loss: 4.8159e+01. (Time: 4.6s)Step 1000 of 1000000; Loss: 6.9866e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4918e+03; Test Loss: 3.2252e+01. (Time: 3.5s)Step 1200 of 1000000; Loss: 6.1501e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5322e+03; Test Loss: 2.5446e+01. (Time: 3.1s)Step 1400 of 1000000; Loss: 5.7847e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1574e+02; Test Loss: 2.4173e+01. (Time: 3.8s)Step 1600 of 1000000; Loss: 5.6563e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3495e+03; Test Loss: 2.3484e+01. (Time: 4.1s)Step 1800 of 1000000; Loss: 5.5559e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4439e+03; Test Loss: 2.2898e+01. (Time: 3.2s)Step 2000 of 1000000; Loss: 5.4719e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.8353e+02; Test Loss: 2.2097e+01. (Time: 3.2s)Step 2200 of 1000000; Loss: 5.3943e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2603e+03; Test Loss: 2.0935e+01. (Time: 4.6s)Step 2400 of 1000000; Loss: 5.3104e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3928e+03; Test Loss: 1.5038e+01. (Time: 3.3s)Step 2600 of 1000000; Loss: 5.1174e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.6386e+01; Test Loss: 2.4606e+00. (Time: 3.1s)Step 2800 of 1000000; Loss: 4.4790e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1364e+03; Test Loss: 2.3485e+00. (Time: 3.1s)Step 3000 of 1000000; Loss: 4.3590e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2767e+03; Test Loss: 2.2409e+00. (Time: 4.7s)Step 3200 of 1000000; Loss: 4.2667e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.3374e+01; Test Loss: 2.1208e+00. (Time: 3.4s)Step 3400 of 1000000; Loss: 4.1944e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0238e+03; Test Loss: 2.0003e+00. (Time: 3.5s)Step 3600 of 1000000; Loss: 4.1392e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2268e+03; Test Loss: 1.8858e+00. (Time: 4.3s)Step 3800 of 1000000; Loss: 4.1034e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.3024e+01; Test Loss: 1.7771e+00. (Time: 3.4s)Step 4000 of 1000000; Loss: 4.0777e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.7706e+02; Test Loss: 1.6858e+00. (Time: 3.4s)Step 4200 of 1000000; Loss: 4.0578e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2065e+03; Test Loss: 1.6105e+00. (Time: 3.3s)Step 4400 of 1000000; Loss: 4.0410e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2182e+01; Test Loss: 1.5624e+00. (Time: 4.9s)Step 4600 of 1000000; Loss: 4.0255e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5925e+02; Test Loss: 1.5166e+00. (Time: 3.4s)Step 4800 of 1000000; Loss: 4.0113e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1944e+03; Test Loss: 1.4642e+00. (Time: 3.2s)Step 5000 of 1000000; Loss: 4.0001e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.3330e+01; Test Loss: 1.4184e+00. (Time: 3.8s)Step 5200 of 1000000; Loss: 3.9894e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5004e+02; Test Loss: 1.3753e+00. (Time: 4.0s)Step 5400 of 1000000; Loss: 3.9779e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1874e+03; Test Loss: 1.3391e+00. (Time: 3.3s)Step 5600 of 1000000; Loss: 3.9654e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6871e+01; Test Loss: 1.3070e+00. (Time: 3.2s)Step 5800 of 1000000; Loss: 3.9535e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.4439e+02; Test Loss: 1.2745e+00. (Time: 4.9s)Step 6000 of 1000000; Loss: 3.9425e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1807e+03; Test Loss: 1.2467e+00. (Time: 3.2s)Step 6200 of 1000000; Loss: 3.9327e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1890e+01; Test Loss: 1.2204e+00. (Time: 3.4s)Step 6400 of 1000000; Loss: 3.9228e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3831e+02; Test Loss: 1.1997e+00. (Time: 3.4s)Step 6600 of 1000000; Loss: 3.9122e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1751e+03; Test Loss: 1.1832e+00. (Time: 4.5s)Step 6800 of 1000000; Loss: 3.9017e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.8027e+01; Test Loss: 1.1693e+00. (Time: 3.2s)Step 7000 of 1000000; Loss: 3.8910e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3201e+02; Test Loss: 1.1523e+00. (Time: 3.2s)Step 7200 of 1000000; Loss: 3.8797e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1693e+03; Test Loss: 1.1368e+00. (Time: 4.1s)Step 7400 of 1000000; Loss: 3.8680e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4534e+01; Test Loss: 1.1176e+00. (Time: 3.8s)Step 7600 of 1000000; Loss: 3.8592e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2604e+02; Test Loss: 1.0991e+00. (Time: 3.4s)Step 7800 of 1000000; Loss: 3.8515e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1632e+03; Test Loss: 1.0806e+00. (Time: 3.4s)Step 8000 of 1000000; Loss: 3.8432e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1645e+01; Test Loss: 1.0596e+00. (Time: 4.8s)Step 8200 of 1000000; Loss: 3.8346e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2059e+02; Test Loss: 1.0401e+00. (Time: 3.5s)Step 8400 of 1000000; Loss: 3.8265e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1574e+03; Test Loss: 1.0284e+00. (Time: 3.1s)Step 8600 of 1000000; Loss: 3.8180e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9424e+01; Test Loss: 1.0214e+00. (Time: 3.5s)Step 8800 of 1000000; Loss: 3.8099e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.1493e+02; Test Loss: 1.0130e+00. (Time: 4.4s)Step 9000 of 1000000; Loss: 3.8034e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1515e+03; Test Loss: 1.0060e+00. (Time: 3.1s)Step 9200 of 1000000; Loss: 3.7974e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8083e+01; Test Loss: 1.0055e+00. (Time: 3.3s)Step 9400 of 1000000; Loss: 3.7907e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0873e+02; Test Loss: 1.0013e+00. (Time: 4.0s)Step 9600 of 1000000; Loss: 3.7848e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1464e+03; Test Loss: 1.0017e+00. (Time: 3.8s)Step 9800 of 1000000; Loss: 3.7804e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7445e+01; Test Loss: 1.0010e+00. (Time: 3.4s)Step 10000 of 1000000; Loss: 3.7771e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0353e+02; Test Loss: 9.9544e-01. (Time: 3.3s)Step 10200 of 1000000; Loss: 3.7762e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1426e+03; Test Loss: 9.9448e-01. (Time: 4.9s)Step 10400 of 1000000; Loss: 3.7761e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7555e+01; Test Loss: 9.9515e-01. (Time: 3.4s)Step 10600 of 1000000; Loss: 3.7749e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9924e+02; Test Loss: 9.8968e-01. (Time: 3.3s)Step 10800 of 1000000; Loss: 3.7746e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1399e+03; Test Loss: 9.8885e-01. (Time: 4.0s)Step 11000 of 1000000; Loss: 3.7744e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7306e+01; Test Loss: 9.8963e-01. (Time: 4.0s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3712\n",
            "Running configuration: hidden_size=16, lr=0.0001, weight_decay=0.001, batch_size=64\n",
            "Step 200 of 200; Loss: 5.6729e+03; Test Loss: 3.1008e+03. (Time: 3.1s)Step 200 of 1000000; Loss: 3.2515e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.3876e+03; Test Loss: 2.3067e+03. (Time: 3.1s)Step 400 of 1000000; Loss: 2.3766e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0893e+03; Test Loss: 1.8920e+03. (Time: 4.5s)Step 600 of 1000000; Loss: 1.9331e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1680e+02; Test Loss: 1.6208e+03. (Time: 3.3s)Step 800 of 1000000; Loss: 1.6498e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2390e+03; Test Loss: 1.4135e+03. (Time: 3.1s)Step 1000 of 1000000; Loss: 1.4366e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1848e+03; Test Loss: 1.2459e+03. (Time: 3.2s)Step 1200 of 1000000; Loss: 1.2635e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6452e+03; Test Loss: 1.1623e+03. (Time: 4.6s)Step 1400 of 1000000; Loss: 1.1678e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.3166e+02; Test Loss: 1.1346e+03. (Time: 3.2s)Step 1600 of 1000000; Loss: 1.1374e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4636e+02; Test Loss: 1.1137e+03. (Time: 3.0s)Step 1800 of 1000000; Loss: 1.1161e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7209e+03; Test Loss: 1.0964e+03. (Time: 3.9s)Step 2000 of 1000000; Loss: 1.0984e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9005e+03; Test Loss: 1.0797e+03. (Time: 3.7s)Step 2200 of 1000000; Loss: 1.0817e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3758e+03; Test Loss: 1.0476e+03. (Time: 3.1s)Step 2400 of 1000000; Loss: 1.0588e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7521e+02; Test Loss: 9.0668e+02. (Time: 3.1s)Step 2600 of 1000000; Loss: 9.1135e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4748e+01; Test Loss: 8.8083e+02. (Time: 4.2s)Step 2800 of 1000000; Loss: 8.8355e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3191e+03; Test Loss: 8.6049e+02. (Time: 3.5s)Step 3000 of 1000000; Loss: 8.6281e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5432e+03; Test Loss: 8.4387e+02. (Time: 3.2s)Step 3200 of 1000000; Loss: 8.4579e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9958e+02; Test Loss: 8.3113e+02. (Time: 3.3s)Step 3400 of 1000000; Loss: 8.3238e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2423e+02; Test Loss: 8.2282e+02. (Time: 4.6s)Step 3600 of 1000000; Loss: 8.2384e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6679e+01; Test Loss: 8.1764e+02. (Time: 3.1s)Step 3800 of 1000000; Loss: 8.1814e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0883e+03; Test Loss: 8.1398e+02. (Time: 3.3s)Step 4000 of 1000000; Loss: 8.1442e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4163e+03; Test Loss: 8.1062e+02. (Time: 3.2s)Step 4200 of 1000000; Loss: 8.1103e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.2914e+02; Test Loss: 8.0755e+02. (Time: 4.3s)Step 4400 of 1000000; Loss: 8.0790e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0001e+01; Test Loss: 8.0463e+02. (Time: 3.2s)Step 4600 of 1000000; Loss: 8.0497e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2002e+01; Test Loss: 8.0183e+02. (Time: 3.2s)Step 4800 of 1000000; Loss: 8.0217e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0376e+03; Test Loss: 7.9913e+02. (Time: 3.7s)Step 5000 of 1000000; Loss: 7.9950e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3757e+03; Test Loss: 7.9646e+02. (Time: 3.9s)Step 5200 of 1000000; Loss: 7.9671e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.8806e+02; Test Loss: 7.9434e+02. (Time: 3.3s)Step 5400 of 1000000; Loss: 7.9462e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.9976e+01; Test Loss: 7.9193e+02. (Time: 3.0s)Step 5600 of 1000000; Loss: 7.9229e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.6549e+00; Test Loss: 7.8967e+02. (Time: 4.1s)Step 5800 of 1000000; Loss: 7.8991e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0103e+03; Test Loss: 7.8756e+02. (Time: 3.3s)Step 6000 of 1000000; Loss: 7.8783e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3472e+03; Test Loss: 7.8557e+02. (Time: 3.2s)Step 6200 of 1000000; Loss: 7.8581e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.6275e+02; Test Loss: 7.8347e+02. (Time: 3.3s)Step 6400 of 1000000; Loss: 7.8372e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.7878e+01; Test Loss: 7.8129e+02. (Time: 4.5s)Step 6600 of 1000000; Loss: 7.8155e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.5498e+00; Test Loss: 7.7899e+02. (Time: 3.2s)Step 6800 of 1000000; Loss: 7.7928e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9824e+03; Test Loss: 7.7680e+02. (Time: 3.3s)Step 7000 of 1000000; Loss: 7.7707e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3205e+03; Test Loss: 7.7448e+02. (Time: 3.5s)Step 7200 of 1000000; Loss: 7.7480e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.4672e+02; Test Loss: 7.7210e+02. (Time: 4.0s)Step 7400 of 1000000; Loss: 7.7237e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.8107e+01; Test Loss: 7.6989e+02. (Time: 3.4s)Step 7600 of 1000000; Loss: 7.7014e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.5473e+00; Test Loss: 7.6782e+02. (Time: 3.0s)Step 7800 of 1000000; Loss: 7.6805e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9526e+03; Test Loss: 7.6576e+02. (Time: 3.8s)Step 8000 of 1000000; Loss: 7.6602e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2907e+03; Test Loss: 7.6364e+02. (Time: 3.5s)Step 8200 of 1000000; Loss: 7.6390e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.3068e+02; Test Loss: 7.6189e+02. (Time: 3.0s)Step 8400 of 1000000; Loss: 7.6212e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.0645e+01; Test Loss: 7.6064e+02. (Time: 3.5s)Step 8600 of 1000000; Loss: 7.6080e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5413e+00; Test Loss: 7.5949e+02. (Time: 4.4s)Step 8800 of 1000000; Loss: 7.5960e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9255e+03; Test Loss: 7.5856e+02. (Time: 3.2s)Step 9000 of 1000000; Loss: 7.5868e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2673e+03; Test Loss: 7.5805e+02. (Time: 3.2s)Step 9200 of 1000000; Loss: 7.5803e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1618e+02; Test Loss: 7.5733e+02. (Time: 3.1s)Step 9400 of 1000000; Loss: 7.5739e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.7467e+01; Test Loss: 7.5668e+02. (Time: 4.5s)Step 9600 of 1000000; Loss: 7.5674e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.9026e+00; Test Loss: 7.5610e+02. (Time: 3.5s)Step 9800 of 1000000; Loss: 7.5614e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9045e+03; Test Loss: 7.5574e+02. (Time: 3.1s)Step 10000 of 1000000; Loss: 7.5581e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2510e+03; Test Loss: 7.5557e+02. (Time: 3.3s)Step 10200 of 1000000; Loss: 7.5558e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1114e+02; Test Loss: 7.5546e+02. (Time: 4.4s)Step 10400 of 1000000; Loss: 7.5545e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6552e+01; Test Loss: 7.5524e+02. (Time: 3.1s)Step 10600 of 1000000; Loss: 7.5528e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5095e+00; Test Loss: 7.5505e+02. (Time: 3.4s)Step 10800 of 1000000; Loss: 7.5504e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8865e+03; Test Loss: 7.5493e+02. (Time: 3.8s)Step 11000 of 1000000; Loss: 7.5497e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2370e+03; Test Loss: 7.5453e+02. (Time: 3.6s)Step 11200 of 1000000; Loss: 7.5460e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.0735e+02; Test Loss: 7.5442e+02. (Time: 3.2s)Step 11400 of 1000000; Loss: 7.5441e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5746e+01; Test Loss: 7.5426e+02. (Time: 3.2s)Step 11600 of 1000000; Loss: 7.5427e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1256e+00; Test Loss: 7.5404e+02. (Time: 4.2s)Step 11800 of 1000000; Loss: 7.5405e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8714e+03; Test Loss: 7.5385e+02. (Time: 3.3s)Step 12000 of 1000000; Loss: 7.5388e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2270e+03; Test Loss: 7.5369e+02. (Time: 3.2s)Step 12200 of 1000000; Loss: 7.5369e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.0380e+02; Test Loss: 7.5384e+02. (Time: 3.2s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3579\n",
            "Running configuration: hidden_size=16, lr=0.0001, weight_decay=0.0001, batch_size=32\n",
            "Step 200 of 200; Loss: 2.8811e+03; Test Loss: 1.7406e+02. (Time: 4.2s)Step 200 of 1000000; Loss: 1.6016e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0137e+03; Test Loss: 1.1861e+02. (Time: 3.2s)Step 400 of 1000000; Loss: 1.1642e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8661e+03; Test Loss: 8.8010e+01. (Time: 3.3s)Step 600 of 1000000; Loss: 9.4511e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7590e+03; Test Loss: 6.6442e+01. (Time: 4.5s)Step 800 of 1000000; Loss: 8.0498e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.2064e+02; Test Loss: 4.8159e+01. (Time: 3.5s)Step 1000 of 1000000; Loss: 6.9866e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4918e+03; Test Loss: 3.2252e+01. (Time: 3.5s)Step 1200 of 1000000; Loss: 6.1501e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5322e+03; Test Loss: 2.5446e+01. (Time: 3.2s)Step 1400 of 1000000; Loss: 5.7847e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1574e+02; Test Loss: 2.4173e+01. (Time: 4.8s)Step 1600 of 1000000; Loss: 5.6563e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3495e+03; Test Loss: 2.3484e+01. (Time: 3.1s)Step 1800 of 1000000; Loss: 5.5559e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4439e+03; Test Loss: 2.2898e+01. (Time: 3.1s)Step 2000 of 1000000; Loss: 5.4719e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.8349e+02; Test Loss: 2.2096e+01. (Time: 3.9s)Step 2200 of 1000000; Loss: 5.3943e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2602e+03; Test Loss: 2.0935e+01. (Time: 4.1s)Step 2400 of 1000000; Loss: 5.3103e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3935e+03; Test Loss: 1.4421e+01. (Time: 3.3s)Step 2600 of 1000000; Loss: 5.1091e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.6108e+01; Test Loss: 2.4505e+00. (Time: 3.2s)Step 2800 of 1000000; Loss: 4.4774e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1360e+03; Test Loss: 2.3493e+00. (Time: 4.6s)Step 3000 of 1000000; Loss: 4.3584e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2765e+03; Test Loss: 2.2411e+00. (Time: 3.2s)Step 3200 of 1000000; Loss: 4.2663e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.3447e+01; Test Loss: 2.1206e+00. (Time: 3.4s)Step 3400 of 1000000; Loss: 4.1941e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0236e+03; Test Loss: 2.0003e+00. (Time: 3.2s)Step 3600 of 1000000; Loss: 4.1391e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2268e+03; Test Loss: 1.8855e+00. (Time: 4.5s)Step 3800 of 1000000; Loss: 4.1034e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.3103e+01; Test Loss: 1.7768e+00. (Time: 3.3s)Step 4000 of 1000000; Loss: 4.0777e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.7703e+02; Test Loss: 1.6857e+00. (Time: 3.3s)Step 4200 of 1000000; Loss: 4.0578e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2065e+03; Test Loss: 1.6106e+00. (Time: 3.9s)Step 4400 of 1000000; Loss: 4.0411e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2262e+01; Test Loss: 1.5626e+00. (Time: 3.9s)Step 4600 of 1000000; Loss: 4.0256e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5923e+02; Test Loss: 1.5167e+00. (Time: 3.4s)Step 4800 of 1000000; Loss: 4.0114e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1945e+03; Test Loss: 1.4644e+00. (Time: 3.3s)Step 5000 of 1000000; Loss: 4.0003e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.3403e+01; Test Loss: 1.4187e+00. (Time: 4.6s)Step 5200 of 1000000; Loss: 3.9895e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5007e+02; Test Loss: 1.3755e+00. (Time: 3.4s)Step 5400 of 1000000; Loss: 3.9779e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1874e+03; Test Loss: 1.3392e+00. (Time: 3.1s)Step 5600 of 1000000; Loss: 3.9655e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6919e+01; Test Loss: 1.3069e+00. (Time: 3.1s)Step 5800 of 1000000; Loss: 3.9536e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.4443e+02; Test Loss: 1.2753e+00. (Time: 4.7s)Step 6000 of 1000000; Loss: 3.9426e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1807e+03; Test Loss: 1.2470e+00. (Time: 3.4s)Step 6200 of 1000000; Loss: 3.9327e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1945e+01; Test Loss: 1.2211e+00. (Time: 3.2s)Step 6400 of 1000000; Loss: 3.9227e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3834e+02; Test Loss: 1.2001e+00. (Time: 4.2s)Step 6600 of 1000000; Loss: 3.9122e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1752e+03; Test Loss: 1.1830e+00. (Time: 3.7s)Step 6800 of 1000000; Loss: 3.9017e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.8077e+01; Test Loss: 1.1695e+00. (Time: 3.4s)Step 7000 of 1000000; Loss: 3.8911e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3203e+02; Test Loss: 1.1530e+00. (Time: 3.4s)Step 7200 of 1000000; Loss: 3.8795e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1693e+03; Test Loss: 1.1372e+00. (Time: 4.8s)Step 7400 of 1000000; Loss: 3.8677e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4558e+01; Test Loss: 1.1177e+00. (Time: 3.4s)Step 7600 of 1000000; Loss: 3.8589e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2602e+02; Test Loss: 1.0992e+00. (Time: 3.4s)Step 7800 of 1000000; Loss: 3.8514e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1633e+03; Test Loss: 1.0812e+00. (Time: 4.1s)Step 8000 of 1000000; Loss: 3.8429e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1679e+01; Test Loss: 1.0599e+00. (Time: 3.5s)Step 8200 of 1000000; Loss: 3.8345e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2052e+02; Test Loss: 1.0409e+00. (Time: 3.3s)Step 8400 of 1000000; Loss: 3.8264e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1573e+03; Test Loss: 1.0290e+00. (Time: 9.4s)Step 8600 of 1000000; Loss: 3.8178e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9466e+01; Test Loss: 1.0226e+00. (Time: 3.3s)Step 8800 of 1000000; Loss: 3.8098e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.1493e+02; Test Loss: 1.0133e+00. (Time: 3.4s)Step 9000 of 1000000; Loss: 3.8036e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1514e+03; Test Loss: 1.0077e+00. (Time: 4.8s)Step 9200 of 1000000; Loss: 3.7976e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8113e+01; Test Loss: 1.0064e+00. (Time: 3.3s)Step 9400 of 1000000; Loss: 3.7905e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0870e+02; Test Loss: 1.0020e+00. (Time: 3.2s)Step 9600 of 1000000; Loss: 3.7847e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1463e+03; Test Loss: 1.0027e+00. (Time: 4.2s)Step 9800 of 1000000; Loss: 3.7803e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7468e+01; Test Loss: 1.0005e+00. (Time: 3.7s)Step 10000 of 1000000; Loss: 3.7771e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0357e+02; Test Loss: 9.9538e-01. (Time: 3.3s)Step 10200 of 1000000; Loss: 3.7759e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1427e+03; Test Loss: 9.9426e-01. (Time: 3.3s)Step 10400 of 1000000; Loss: 3.7756e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7585e+01; Test Loss: 9.9441e-01. (Time: 4.7s)Step 10600 of 1000000; Loss: 3.7746e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9931e+02; Test Loss: 9.8983e-01. (Time: 3.3s)Step 10800 of 1000000; Loss: 3.7744e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1401e+03; Test Loss: 9.8814e-01. (Time: 3.3s)Step 11000 of 1000000; Loss: 3.7740e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7332e+01; Test Loss: 9.8955e-01. (Time: 3.1s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3712\n",
            "Running configuration: hidden_size=16, lr=0.0001, weight_decay=0.0001, batch_size=64\n",
            "Step 200 of 200; Loss: 5.6729e+03; Test Loss: 3.1008e+03. (Time: 4.3s)Step 200 of 1000000; Loss: 3.2515e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.3876e+03; Test Loss: 2.3067e+03. (Time: 3.2s)Step 400 of 1000000; Loss: 2.3766e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0893e+03; Test Loss: 1.8920e+03. (Time: 3.0s)Step 600 of 1000000; Loss: 1.9331e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1680e+02; Test Loss: 1.6208e+03. (Time: 4.0s)Step 800 of 1000000; Loss: 1.6498e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2390e+03; Test Loss: 1.4135e+03. (Time: 3.7s)Step 1000 of 1000000; Loss: 1.4366e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1848e+03; Test Loss: 1.2459e+03. (Time: 3.2s)Step 1200 of 1000000; Loss: 1.2635e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6452e+03; Test Loss: 1.1623e+03. (Time: 3.3s)Step 1400 of 1000000; Loss: 1.1678e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.3166e+02; Test Loss: 1.1346e+03. (Time: 4.5s)Step 1600 of 1000000; Loss: 1.1374e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4636e+02; Test Loss: 1.1137e+03. (Time: 3.2s)Step 1800 of 1000000; Loss: 1.1161e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7209e+03; Test Loss: 1.0964e+03. (Time: 3.2s)Step 2000 of 1000000; Loss: 1.0984e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9005e+03; Test Loss: 1.0797e+03. (Time: 3.3s)Step 2200 of 1000000; Loss: 1.0817e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3752e+03; Test Loss: 1.0473e+03. (Time: 4.4s)Step 2400 of 1000000; Loss: 1.0587e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7468e+02; Test Loss: 9.0658e+02. (Time: 3.3s)Step 2600 of 1000000; Loss: 9.1119e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4734e+01; Test Loss: 8.8080e+02. (Time: 3.1s)Step 2800 of 1000000; Loss: 8.8352e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3190e+03; Test Loss: 8.6029e+02. (Time: 3.8s)Step 3000 of 1000000; Loss: 8.6261e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5431e+03; Test Loss: 8.4384e+02. (Time: 3.8s)Step 3200 of 1000000; Loss: 8.4575e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9957e+02; Test Loss: 8.3113e+02. (Time: 3.2s)Step 3400 of 1000000; Loss: 8.3238e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2417e+02; Test Loss: 8.2281e+02. (Time: 3.0s)Step 3600 of 1000000; Loss: 8.2381e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6675e+01; Test Loss: 8.1766e+02. (Time: 4.2s)Step 3800 of 1000000; Loss: 8.1815e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0883e+03; Test Loss: 8.1400e+02. (Time: 3.4s)Step 4000 of 1000000; Loss: 8.1444e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4163e+03; Test Loss: 8.1065e+02. (Time: 3.3s)Step 4200 of 1000000; Loss: 8.1106e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.2916e+02; Test Loss: 8.0757e+02. (Time: 3.2s)Step 4400 of 1000000; Loss: 8.0793e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0019e+01; Test Loss: 8.0466e+02. (Time: 4.6s)Step 4600 of 1000000; Loss: 8.0499e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2002e+01; Test Loss: 8.0185e+02. (Time: 3.1s)Step 4800 of 1000000; Loss: 8.0219e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0376e+03; Test Loss: 7.9914e+02. (Time: 3.3s)Step 5000 of 1000000; Loss: 7.9951e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3757e+03; Test Loss: 7.9648e+02. (Time: 3.5s)Step 5200 of 1000000; Loss: 7.9673e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.8804e+02; Test Loss: 7.9436e+02. (Time: 4.2s)Step 5400 of 1000000; Loss: 7.9464e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.0006e+01; Test Loss: 7.9191e+02. (Time: 3.0s)Step 5600 of 1000000; Loss: 7.9227e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.6545e+00; Test Loss: 7.8966e+02. (Time: 3.2s)Step 5800 of 1000000; Loss: 7.8990e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0102e+03; Test Loss: 7.8756e+02. (Time: 3.7s)Step 6000 of 1000000; Loss: 7.8783e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3472e+03; Test Loss: 7.8557e+02. (Time: 4.1s)Step 6200 of 1000000; Loss: 7.8581e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.6277e+02; Test Loss: 7.8350e+02. (Time: 3.4s)Step 6400 of 1000000; Loss: 7.8376e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.7923e+01; Test Loss: 7.8129e+02. (Time: 3.2s)Step 6600 of 1000000; Loss: 7.8156e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.5524e+00; Test Loss: 7.7899e+02. (Time: 4.0s)Step 6800 of 1000000; Loss: 7.7929e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9824e+03; Test Loss: 7.7682e+02. (Time: 3.4s)Step 7000 of 1000000; Loss: 7.7709e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3204e+03; Test Loss: 7.7449e+02. (Time: 3.1s)Step 7200 of 1000000; Loss: 7.7481e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.4675e+02; Test Loss: 7.7211e+02. (Time: 3.1s)Step 7400 of 1000000; Loss: 7.7238e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.8142e+01; Test Loss: 7.6993e+02. (Time: 4.5s)Step 7600 of 1000000; Loss: 7.7018e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.5499e+00; Test Loss: 7.6778e+02. (Time: 3.1s)Step 7800 of 1000000; Loss: 7.6803e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9526e+03; Test Loss: 7.6573e+02. (Time: 3.2s)Step 8000 of 1000000; Loss: 7.6599e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2907e+03; Test Loss: 7.6362e+02. (Time: 3.4s)Step 8200 of 1000000; Loss: 7.6389e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.3072e+02; Test Loss: 7.6194e+02. (Time: 4.4s)Step 8400 of 1000000; Loss: 7.6214e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.0663e+01; Test Loss: 7.6063e+02. (Time: 3.3s)Step 8600 of 1000000; Loss: 7.6076e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5448e+00; Test Loss: 7.5952e+02. (Time: 3.1s)Step 8800 of 1000000; Loss: 7.5964e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9254e+03; Test Loss: 7.5858e+02. (Time: 4.1s)Step 9000 of 1000000; Loss: 7.5871e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2672e+03; Test Loss: 7.5804e+02. (Time: 3.6s)Step 9200 of 1000000; Loss: 7.5809e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.3036e+02; Test Loss: 7.6651e+02. (Time: 3.3s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3640\n",
            "Running configuration: hidden_size=16, lr=0.0001, weight_decay=1e-05, batch_size=32\n",
            "Step 200 of 200; Loss: 2.8811e+03; Test Loss: 1.7406e+02. (Time: 3.1s)Step 200 of 1000000; Loss: 1.6016e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0137e+03; Test Loss: 1.1861e+02. (Time: 4.5s)Step 400 of 1000000; Loss: 1.1642e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8661e+03; Test Loss: 8.8010e+01. (Time: 3.4s)Step 600 of 1000000; Loss: 9.4511e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7590e+03; Test Loss: 6.6442e+01. (Time: 3.2s)Step 800 of 1000000; Loss: 8.0498e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.2064e+02; Test Loss: 4.8159e+01. (Time: 3.9s)Step 1000 of 1000000; Loss: 6.9866e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4918e+03; Test Loss: 3.2252e+01. (Time: 4.1s)Step 1200 of 1000000; Loss: 6.1501e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5322e+03; Test Loss: 2.5446e+01. (Time: 3.2s)Step 1400 of 1000000; Loss: 5.7847e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1574e+02; Test Loss: 2.4173e+01. (Time: 3.1s)Step 1600 of 1000000; Loss: 5.6563e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3495e+03; Test Loss: 2.3484e+01. (Time: 4.0s)Step 1800 of 1000000; Loss: 5.5559e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4439e+03; Test Loss: 2.2898e+01. (Time: 3.6s)Step 2000 of 1000000; Loss: 5.4719e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.8349e+02; Test Loss: 2.2096e+01. (Time: 3.3s)Step 2200 of 1000000; Loss: 5.3943e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2602e+03; Test Loss: 2.0935e+01. (Time: 3.4s)Step 2400 of 1000000; Loss: 5.3103e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3935e+03; Test Loss: 1.4388e+01. (Time: 5.0s)Step 2600 of 1000000; Loss: 5.1089e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.6121e+01; Test Loss: 2.4500e+00. (Time: 3.2s)Step 2800 of 1000000; Loss: 4.4772e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1361e+03; Test Loss: 2.3488e+00. (Time: 3.3s)Step 3000 of 1000000; Loss: 4.3583e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2764e+03; Test Loss: 2.2408e+00. (Time: 4.0s)Step 3200 of 1000000; Loss: 4.2662e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.3444e+01; Test Loss: 2.1204e+00. (Time: 4.1s)Step 3400 of 1000000; Loss: 4.1941e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0236e+03; Test Loss: 2.0002e+00. (Time: 3.3s)Step 3600 of 1000000; Loss: 4.1391e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2267e+03; Test Loss: 1.8855e+00. (Time: 3.4s)Step 3800 of 1000000; Loss: 4.1034e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.3093e+01; Test Loss: 1.7768e+00. (Time: 4.8s)Step 4000 of 1000000; Loss: 4.0777e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.7702e+02; Test Loss: 1.6857e+00. (Time: 3.2s)Step 4200 of 1000000; Loss: 4.0578e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2064e+03; Test Loss: 1.6106e+00. (Time: 3.3s)Step 4400 of 1000000; Loss: 4.0411e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2244e+01; Test Loss: 1.5626e+00. (Time: 3.5s)Step 4600 of 1000000; Loss: 4.0256e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5924e+02; Test Loss: 1.5166e+00. (Time: 4.1s)Step 4800 of 1000000; Loss: 4.0114e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1944e+03; Test Loss: 1.4644e+00. (Time: 3.3s)Step 5000 of 1000000; Loss: 4.0002e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.3383e+01; Test Loss: 1.4186e+00. (Time: 3.3s)Step 5200 of 1000000; Loss: 3.9895e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5009e+02; Test Loss: 1.3753e+00. (Time: 4.4s)Step 5400 of 1000000; Loss: 3.9779e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1874e+03; Test Loss: 1.3394e+00. (Time: 3.3s)Step 5600 of 1000000; Loss: 3.9656e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6913e+01; Test Loss: 1.3071e+00. (Time: 3.1s)Step 5800 of 1000000; Loss: 3.9536e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.4442e+02; Test Loss: 1.2751e+00. (Time: 3.3s)Step 6000 of 1000000; Loss: 3.9425e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1807e+03; Test Loss: 1.2470e+00. (Time: 4.5s)Step 6200 of 1000000; Loss: 3.9328e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1953e+01; Test Loss: 1.2218e+00. (Time: 3.3s)Step 6400 of 1000000; Loss: 3.9227e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3831e+02; Test Loss: 1.2005e+00. (Time: 3.1s)Step 6600 of 1000000; Loss: 3.9123e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1751e+03; Test Loss: 1.1836e+00. (Time: 3.7s)Step 6800 of 1000000; Loss: 3.9018e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.8096e+01; Test Loss: 1.1700e+00. (Time: 4.0s)Step 7000 of 1000000; Loss: 3.8912e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3203e+02; Test Loss: 1.1537e+00. (Time: 3.2s)Step 7200 of 1000000; Loss: 3.8795e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1693e+03; Test Loss: 1.1377e+00. (Time: 3.3s)Step 7400 of 1000000; Loss: 3.8677e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4569e+01; Test Loss: 1.1182e+00. (Time: 4.5s)Step 7600 of 1000000; Loss: 3.8589e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2600e+02; Test Loss: 1.0997e+00. (Time: 3.3s)Step 7800 of 1000000; Loss: 3.8513e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1633e+03; Test Loss: 1.0816e+00. (Time: 3.3s)Step 8000 of 1000000; Loss: 3.8430e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1683e+01; Test Loss: 1.0599e+00. (Time: 3.4s)Step 8200 of 1000000; Loss: 3.8345e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2054e+02; Test Loss: 1.0404e+00. (Time: 4.6s)Step 8400 of 1000000; Loss: 3.8264e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1574e+03; Test Loss: 1.0288e+00. (Time: 3.3s)Step 8600 of 1000000; Loss: 3.8179e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9428e+01; Test Loss: 1.0219e+00. (Time: 3.2s)Step 8800 of 1000000; Loss: 3.8098e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.1499e+02; Test Loss: 1.0131e+00. (Time: 4.0s)Step 9000 of 1000000; Loss: 3.8034e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1514e+03; Test Loss: 1.0075e+00. (Time: 3.7s)Step 9200 of 1000000; Loss: 3.7975e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8112e+01; Test Loss: 1.0062e+00. (Time: 3.4s)Step 9400 of 1000000; Loss: 3.7906e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0869e+02; Test Loss: 1.0015e+00. (Time: 3.3s)Step 9600 of 1000000; Loss: 3.7849e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1464e+03; Test Loss: 1.0017e+00. (Time: 4.6s)Step 9800 of 1000000; Loss: 3.7802e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7486e+01; Test Loss: 1.0011e+00. (Time: 3.4s)Step 10000 of 1000000; Loss: 3.7769e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0352e+02; Test Loss: 9.9521e-01. (Time: 3.2s)Step 10200 of 1000000; Loss: 3.7756e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1428e+03; Test Loss: 9.9359e-01. (Time: 4.0s)Step 10400 of 1000000; Loss: 3.7755e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7591e+01; Test Loss: 9.9483e-01. (Time: 4.0s)Step 10600 of 1000000; Loss: 3.7744e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9916e+02; Test Loss: 9.8985e-01. (Time: 3.1s)Step 10800 of 1000000; Loss: 3.7739e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1401e+03; Test Loss: 9.8532e-01. (Time: 3.0s)Step 11000 of 1000000; Loss: 3.7744e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7256e+01; Test Loss: 9.8621e-01. (Time: 4.1s)Step 11200 of 1000000; Loss: 3.7738e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9563e+02; Test Loss: 9.8396e-01. (Time: 3.5s)Step 11400 of 1000000; Loss: 3.7730e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1376e+03; Test Loss: 9.8643e-01. (Time: 3.3s)Step 11600 of 1000000; Loss: 3.7727e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7160e+01; Test Loss: 9.8846e-01. (Time: 3.4s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3710\n",
            "Running configuration: hidden_size=16, lr=0.0001, weight_decay=1e-05, batch_size=64\n",
            "Step 200 of 200; Loss: 5.6729e+03; Test Loss: 3.1008e+03. (Time: 4.7s)Step 200 of 1000000; Loss: 3.2515e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.3876e+03; Test Loss: 2.3067e+03. (Time: 3.0s)Step 400 of 1000000; Loss: 2.3766e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0893e+03; Test Loss: 1.8920e+03. (Time: 3.3s)Step 600 of 1000000; Loss: 1.9331e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.1680e+02; Test Loss: 1.6208e+03. (Time: 3.7s)Step 800 of 1000000; Loss: 1.6498e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2390e+03; Test Loss: 1.4135e+03. (Time: 4.0s)Step 1000 of 1000000; Loss: 1.4366e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1848e+03; Test Loss: 1.2459e+03. (Time: 3.1s)Step 1200 of 1000000; Loss: 1.2635e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6452e+03; Test Loss: 1.1623e+03. (Time: 3.3s)Step 1400 of 1000000; Loss: 1.1678e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.3166e+02; Test Loss: 1.1346e+03. (Time: 4.1s)Step 1600 of 1000000; Loss: 1.1374e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4636e+02; Test Loss: 1.1137e+03. (Time: 3.3s)Step 1800 of 1000000; Loss: 1.1161e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7209e+03; Test Loss: 1.0964e+03. (Time: 3.0s)Step 2000 of 1000000; Loss: 1.0984e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9005e+03; Test Loss: 1.0797e+03. (Time: 3.1s)Step 2200 of 1000000; Loss: 1.0817e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3748e+03; Test Loss: 1.0472e+03. (Time: 3.9s)Step 2400 of 1000000; Loss: 1.0586e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7473e+02; Test Loss: 9.0653e+02. (Time: 3.2s)Step 2600 of 1000000; Loss: 9.1114e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4735e+01; Test Loss: 8.8077e+02. (Time: 3.1s)Step 2800 of 1000000; Loss: 8.8349e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3189e+03; Test Loss: 8.6026e+02. (Time: 3.0s)Step 3000 of 1000000; Loss: 8.6258e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5430e+03; Test Loss: 8.4382e+02. (Time: 4.1s)Step 3200 of 1000000; Loss: 8.4573e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9952e+02; Test Loss: 8.3111e+02. (Time: 3.5s)Step 3400 of 1000000; Loss: 8.3236e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2416e+02; Test Loss: 8.2279e+02. (Time: 3.2s)Step 3600 of 1000000; Loss: 8.2379e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6673e+01; Test Loss: 8.1766e+02. (Time: 3.1s)Step 3800 of 1000000; Loss: 8.1815e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0882e+03; Test Loss: 8.1400e+02. (Time: 4.4s)Step 4000 of 1000000; Loss: 8.1443e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4163e+03; Test Loss: 8.1065e+02. (Time: 3.2s)Step 4200 of 1000000; Loss: 8.1105e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.2914e+02; Test Loss: 8.0757e+02. (Time: 3.2s)Step 4400 of 1000000; Loss: 8.0793e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0014e+01; Test Loss: 8.0466e+02. (Time: 3.5s)Step 4600 of 1000000; Loss: 8.0499e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2000e+01; Test Loss: 8.0184e+02. (Time: 4.1s)Step 4800 of 1000000; Loss: 8.0218e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0376e+03; Test Loss: 7.9912e+02. (Time: 3.4s)Step 5000 of 1000000; Loss: 7.9949e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3757e+03; Test Loss: 7.9647e+02. (Time: 3.2s)Step 5200 of 1000000; Loss: 7.9673e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.8803e+02; Test Loss: 7.9436e+02. (Time: 4.2s)Step 5400 of 1000000; Loss: 7.9464e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.9997e+01; Test Loss: 7.9190e+02. (Time: 3.5s)Step 5600 of 1000000; Loss: 7.9227e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.6534e+00; Test Loss: 7.8964e+02. (Time: 3.2s)Step 5800 of 1000000; Loss: 7.8988e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0102e+03; Test Loss: 7.8753e+02. (Time: 3.1s)Step 6000 of 1000000; Loss: 7.8780e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3472e+03; Test Loss: 7.8557e+02. (Time: 4.7s)Step 6200 of 1000000; Loss: 7.8581e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.6277e+02; Test Loss: 7.8351e+02. (Time: 3.3s)Step 6400 of 1000000; Loss: 7.8376e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.7921e+01; Test Loss: 7.8128e+02. (Time: 3.2s)Step 6600 of 1000000; Loss: 7.8155e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.5574e+00; Test Loss: 7.7901e+02. (Time: 3.5s)Step 6800 of 1000000; Loss: 7.7930e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9824e+03; Test Loss: 7.7683e+02. (Time: 4.3s)Step 7000 of 1000000; Loss: 7.7710e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3204e+03; Test Loss: 7.7450e+02. (Time: 3.4s)Step 7200 of 1000000; Loss: 7.7481e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.4676e+02; Test Loss: 7.7211e+02. (Time: 3.3s)Step 7400 of 1000000; Loss: 7.7238e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.8150e+01; Test Loss: 7.6993e+02. (Time: 3.9s)Step 7600 of 1000000; Loss: 7.7018e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.5494e+00; Test Loss: 7.6777e+02. (Time: 3.8s)Step 7800 of 1000000; Loss: 7.6803e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9525e+03; Test Loss: 7.6574e+02. (Time: 3.0s)Step 8000 of 1000000; Loss: 7.6599e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2907e+03; Test Loss: 7.6364e+02. (Time: 3.3s)Step 8200 of 1000000; Loss: 7.6389e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.3076e+02; Test Loss: 7.6196e+02. (Time: 4.6s)Step 8400 of 1000000; Loss: 7.6215e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.0716e+01; Test Loss: 7.6064e+02. (Time: 3.0s)Step 8600 of 1000000; Loss: 7.6077e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5518e+00; Test Loss: 7.5955e+02. (Time: 3.2s)Step 8800 of 1000000; Loss: 7.5968e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9253e+03; Test Loss: 7.5861e+02. (Time: 3.1s)Step 9000 of 1000000; Loss: 7.5872e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2674e+03; Test Loss: 7.5809e+02. (Time: 4.6s)Step 9200 of 1000000; Loss: 7.5806e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1633e+02; Test Loss: 7.5734e+02. (Time: 3.1s)Step 9400 of 1000000; Loss: 7.5742e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.7595e+01; Test Loss: 7.5678e+02. (Time: 3.3s)Step 9600 of 1000000; Loss: 7.5682e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.9198e+00; Test Loss: 7.5614e+02. (Time: 3.4s)Step 9800 of 1000000; Loss: 7.5622e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9043e+03; Test Loss: 7.5574e+02. (Time: 4.1s)Step 10000 of 1000000; Loss: 7.5578e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2509e+03; Test Loss: 7.5553e+02. (Time: 3.3s)Step 10200 of 1000000; Loss: 7.5554e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1128e+02; Test Loss: 7.5536e+02. (Time: 3.1s)Step 10400 of 1000000; Loss: 7.5538e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6680e+01; Test Loss: 7.5507e+02. (Time: 4.0s)Step 10600 of 1000000; Loss: 7.5510e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5154e+00; Test Loss: 7.5487e+02. (Time: 3.9s)Step 10800 of 1000000; Loss: 7.5490e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8866e+03; Test Loss: 7.5479e+02. (Time: 3.2s)Step 11000 of 1000000; Loss: 7.5482e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2369e+03; Test Loss: 7.5445e+02. (Time: 3.1s)Step 11200 of 1000000; Loss: 7.5450e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.0738e+02; Test Loss: 7.5418e+02. (Time: 4.6s)Step 11400 of 1000000; Loss: 7.5421e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5808e+01; Test Loss: 7.5419e+02. (Time: 3.1s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3581\n",
            "Running configuration: hidden_size=32, lr=0.001, weight_decay=0.001, batch_size=32\n",
            "Step 200 of 200; Loss: 1.1887e+03; Test Loss: 3.2287e+00. (Time: 2.6s)Step 200 of 1000000; Loss: 4.0026e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7563e+01; Test Loss: 1.0125e+00. (Time: 2.6s)Step 400 of 1000000; Loss: 3.8302e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9340e+02; Test Loss: 8.9928e-01. (Time: 3.7s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3908\n",
            "Running configuration: hidden_size=32, lr=0.001, weight_decay=0.001, batch_size=64\n",
            "Step 200 of 200; Loss: 2.3373e+03; Test Loss: 7.8152e+02. (Time: 2.6s)Step 200 of 1000000; Loss: 8.0438e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.2294e+02; Test Loss: 7.5864e+02. (Time: 2.7s)Step 400 of 1000000; Loss: 7.5890e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9763e+01; Test Loss: 7.4864e+02. (Time: 9.0s)Step 600 of 1000000; Loss: 7.4998e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6621e+00; Test Loss: 7.4086e+02. (Time: 2.7s)Step 800 of 1000000; Loss: 7.3912e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7937e+03; Test Loss: 7.3335e+02. (Time: 2.6s)Step 1000 of 1000000; Loss: 7.3377e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1806e+03; Test Loss: 7.3162e+02. (Time: 2.8s)Step 1200 of 1000000; Loss: 7.3410e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.5167e+02; Test Loss: 7.3828e+02. (Time: 4.2s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3506\n",
            "Running configuration: hidden_size=32, lr=0.001, weight_decay=0.0001, batch_size=32\n",
            "Step 200 of 200; Loss: 1.3814e+03; Test Loss: 1.9179e+01. (Time: 2.7s)Step 200 of 1000000; Loss: 5.1944e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.9031e+01; Test Loss: 3.4910e+00. (Time: 2.6s)Step 400 of 1000000; Loss: 4.2392e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.5945e+02; Test Loss: 1.7699e+00. (Time: 2.5s)Step 600 of 1000000; Loss: 3.9426e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1593e+03; Test Loss: 1.3444e+00. (Time: 3.5s)Step 800 of 1000000; Loss: 3.8988e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4779e+01; Test Loss: 1.1127e+00. (Time: 3.0s)Step 1000 of 1000000; Loss: 3.8606e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9502e+02; Test Loss: 9.7729e-01. (Time: 2.7s)Step 1200 of 1000000; Loss: 3.8339e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1430e+03; Test Loss: 8.3584e-01. (Time: 2.8s)Step 1400 of 1000000; Loss: 3.8188e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1227e+01; Test Loss: 7.6668e-01. (Time: 2.5s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3749\n",
            "Running configuration: hidden_size=32, lr=0.001, weight_decay=0.0001, batch_size=64\n",
            "Step 200 of 200; Loss: 2.3216e+03; Test Loss: 7.7453e+02. (Time: 3.6s)Step 200 of 1000000; Loss: 7.9133e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.1824e+02; Test Loss: 7.5930e+02. (Time: 2.8s)Step 400 of 1000000; Loss: 7.6092e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0439e+01; Test Loss: 7.4897e+02. (Time: 2.7s)Step 600 of 1000000; Loss: 7.5099e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5959e+00; Test Loss: 7.4320e+02. (Time: 2.6s)Step 800 of 1000000; Loss: 7.4457e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7933e+03; Test Loss: 7.3946e+02. (Time: 3.9s)Step 1000 of 1000000; Loss: 7.3854e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1647e+03; Test Loss: 7.3802e+02. (Time: 2.6s)Step 1200 of 1000000; Loss: 7.3729e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.6149e+02; Test Loss: 7.3383e+02. (Time: 2.6s)Step 1400 of 1000000; Loss: 7.3792e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3472e+01; Test Loss: 7.3354e+02. (Time: 2.6s)Step 1600 of 1000000; Loss: 7.3297e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9352e+00; Test Loss: 7.5873e+02. (Time: 2.9s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3603\n",
            "Running configuration: hidden_size=32, lr=0.001, weight_decay=1e-05, batch_size=32\n",
            "Step 200 of 200; Loss: 1.3763e+03; Test Loss: 1.7261e+01. (Time: 3.5s)Step 200 of 1000000; Loss: 4.9892e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2333e+01; Test Loss: 2.0005e+00. (Time: 2.6s)Step 400 of 1000000; Loss: 4.1146e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.4459e+02; Test Loss: 1.5854e+00. (Time: 2.8s)Step 600 of 1000000; Loss: 3.9278e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1552e+03; Test Loss: 1.3028e+00. (Time: 2.5s)Step 800 of 1000000; Loss: 3.8628e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4253e+01; Test Loss: 1.0547e+00. (Time: 4.0s)Step 1000 of 1000000; Loss: 3.8170e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.9346e+02; Test Loss: 8.9833e-01. (Time: 2.6s)Step 1200 of 1000000; Loss: 3.7907e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1345e+03; Test Loss: 8.3006e-01. (Time: 2.6s)Step 1400 of 1000000; Loss: 3.7681e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0837e+01; Test Loss: 7.8464e-01. (Time: 2.6s)Step 1600 of 1000000; Loss: 3.7527e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.8303e+02; Test Loss: 6.2674e-01. (Time: 3.5s)Step 1800 of 1000000; Loss: 3.7500e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1309e+03; Test Loss: 6.3343e-01. (Time: 3.3s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3686\n",
            "Running configuration: hidden_size=32, lr=0.001, weight_decay=1e-05, batch_size=64\n",
            "Step 200 of 200; Loss: 2.3256e+03; Test Loss: 7.7585e+02. (Time: 2.7s)Step 200 of 1000000; Loss: 7.9608e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.3193e+02; Test Loss: 7.5521e+02. (Time: 2.6s)Step 400 of 1000000; Loss: 7.5904e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1194e+01; Test Loss: 7.4971e+02. (Time: 2.7s)Step 600 of 1000000; Loss: 7.4962e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4744e+00; Test Loss: 7.4312e+02. (Time: 3.8s)Step 800 of 1000000; Loss: 7.4284e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8171e+03; Test Loss: 7.3561e+02. (Time: 2.7s)Step 1000 of 1000000; Loss: 7.3836e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1717e+03; Test Loss: 7.3413e+02. (Time: 2.7s)Step 1200 of 1000000; Loss: 7.3598e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.6212e+02; Test Loss: 7.3435e+02. (Time: 2.6s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3487\n",
            "Running configuration: hidden_size=32, lr=0.0001, weight_decay=0.001, batch_size=32\n",
            "Step 200 of 200; Loss: 1.6373e+03; Test Loss: 2.0701e+01. (Time: 3.8s)Step 200 of 1000000; Loss: 5.8665e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0909e+02; Test Loss: 1.8180e+01. (Time: 2.7s)Step 400 of 1000000; Loss: 5.4404e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3325e+03; Test Loss: 1.5424e+01. (Time: 2.5s)Step 600 of 1000000; Loss: 5.1292e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3561e+03; Test Loss: 1.2132e+01. (Time: 2.7s)Step 800 of 1000000; Loss: 4.7856e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3353e+02; Test Loss: 8.2909e+00. (Time: 3.0s)Step 1000 of 1000000; Loss: 4.3950e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0221e+03; Test Loss: 6.2216e+00. (Time: 3.6s)Step 1200 of 1000000; Loss: 4.1375e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1938e+03; Test Loss: 4.9017e+00. (Time: 2.7s)Step 1400 of 1000000; Loss: 4.0096e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.2910e+01; Test Loss: 1.5727e+00. (Time: 2.8s)Step 1600 of 1000000; Loss: 3.8889e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3968e+02; Test Loss: 1.4714e+00. (Time: 2.7s)Step 1800 of 1000000; Loss: 3.8735e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1691e+03; Test Loss: 1.4176e+00. (Time: 3.8s)Step 2000 of 1000000; Loss: 3.8658e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0024e+01; Test Loss: 1.3600e+00. (Time: 2.7s)Step 2200 of 1000000; Loss: 3.8466e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.1790e+02; Test Loss: 1.3064e+00. (Time: 2.8s)Step 2400 of 1000000; Loss: 3.8317e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1620e+03; Test Loss: 1.2493e+00. (Time: 2.6s)Step 2600 of 1000000; Loss: 3.8222e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3738e+01; Test Loss: 1.1989e+00. (Time: 3.3s)Step 2800 of 1000000; Loss: 3.8201e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0415e+02; Test Loss: 1.1611e+00. (Time: 3.1s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3751\n",
            "Running configuration: hidden_size=32, lr=0.0001, weight_decay=0.001, batch_size=64\n",
            "Step 200 of 200; Loss: 3.3057e+03; Test Loss: 1.1622e+03. (Time: 2.7s)Step 200 of 1000000; Loss: 1.1741e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4330e+03; Test Loss: 1.0800e+03. (Time: 2.7s)Step 400 of 1000000; Loss: 1.0881e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.1898e+02; Test Loss: 1.0167e+03. (Time: 3.0s)Step 600 of 1000000; Loss: 1.0248e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.6141e+01; Test Loss: 9.4082e+02. (Time: 3.7s)Step 800 of 1000000; Loss: 9.5190e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2803e+03; Test Loss: 8.6260e+02. (Time: 2.5s)Step 1000 of 1000000; Loss: 8.7048e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4273e+03; Test Loss: 8.1696e+02. (Time: 2.6s)Step 1200 of 1000000; Loss: 8.2117e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.2634e+02; Test Loss: 7.9369e+02. (Time: 2.7s)Step 1400 of 1000000; Loss: 7.9421e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6653e+01; Test Loss: 7.7953e+02. (Time: 3.7s)Step 1600 of 1000000; Loss: 7.8076e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.1912e+00; Test Loss: 7.7102e+02. (Time: 3.0s)Step 1800 of 1000000; Loss: 7.7150e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9674e+03; Test Loss: 7.6939e+02. (Time: 2.6s)Step 2000 of 1000000; Loss: 7.6988e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3021e+03; Test Loss: 7.6724e+02. (Time: 2.6s)Step 2200 of 1000000; Loss: 7.6635e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.5107e+02; Test Loss: 7.6635e+02. (Time: 2.7s)Step 2400 of 1000000; Loss: 7.6583e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.9358e+01; Test Loss: 7.6514e+02. (Time: 4.0s)Step 2600 of 1000000; Loss: 7.6574e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.8972e+00; Test Loss: 7.6253e+02. (Time: 2.7s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3621\n",
            "Running configuration: hidden_size=32, lr=0.0001, weight_decay=0.0001, batch_size=32\n",
            "Step 200 of 200; Loss: 1.6373e+03; Test Loss: 2.0700e+01. (Time: 2.6s)Step 200 of 1000000; Loss: 5.8664e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0905e+02; Test Loss: 1.8178e+01. (Time: 2.6s)Step 400 of 1000000; Loss: 5.4404e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3325e+03; Test Loss: 1.5417e+01. (Time: 4.0s)Step 600 of 1000000; Loss: 5.1290e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3565e+03; Test Loss: 1.1979e+01. (Time: 2.7s)Step 800 of 1000000; Loss: 4.7828e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3238e+02; Test Loss: 8.2198e+00. (Time: 2.7s)Step 1000 of 1000000; Loss: 4.3847e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0196e+03; Test Loss: 6.1190e+00. (Time: 2.8s)Step 1200 of 1000000; Loss: 4.1273e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1926e+03; Test Loss: 4.6548e+00. (Time: 3.3s)Step 1400 of 1000000; Loss: 4.0055e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.2426e+01; Test Loss: 1.5694e+00. (Time: 3.2s)Step 1600 of 1000000; Loss: 3.8867e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3916e+02; Test Loss: 1.4824e+00. (Time: 2.7s)Step 1800 of 1000000; Loss: 3.8719e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1692e+03; Test Loss: 1.4147e+00. (Time: 2.5s)Step 2000 of 1000000; Loss: 3.8582e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9922e+01; Test Loss: 1.3527e+00. (Time: 2.7s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3773\n",
            "Running configuration: hidden_size=32, lr=0.0001, weight_decay=0.0001, batch_size=64\n",
            "Step 200 of 200; Loss: 3.3057e+03; Test Loss: 1.1622e+03. (Time: 3.8s)Step 200 of 1000000; Loss: 1.1741e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4330e+03; Test Loss: 1.0800e+03. (Time: 2.8s)Step 400 of 1000000; Loss: 1.0881e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.1912e+02; Test Loss: 1.0167e+03. (Time: 2.7s)Step 600 of 1000000; Loss: 1.0248e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.5686e+01; Test Loss: 9.4041e+02. (Time: 2.7s)Step 800 of 1000000; Loss: 9.5149e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2778e+03; Test Loss: 8.6183e+02. (Time: 3.8s)Step 1000 of 1000000; Loss: 8.6969e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4263e+03; Test Loss: 8.1615e+02. (Time: 2.7s)Step 1200 of 1000000; Loss: 8.2048e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.2548e+02; Test Loss: 7.9381e+02. (Time: 2.7s)Step 1400 of 1000000; Loss: 7.9432e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.0910e+01; Test Loss: 7.7429e+02. (Time: 2.6s)Step 1600 of 1000000; Loss: 7.7588e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3894e+00; Test Loss: 7.7155e+02. (Time: 3.0s)Step 1800 of 1000000; Loss: 7.7192e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9688e+03; Test Loss: 7.7015e+02. (Time: 3.7s)Step 2000 of 1000000; Loss: 7.7079e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3025e+03; Test Loss: 7.6687e+02. (Time: 2.6s)Step 2200 of 1000000; Loss: 7.6655e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.5187e+02; Test Loss: 7.6587e+02. (Time: 2.7s)Step 2400 of 1000000; Loss: 7.6547e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.9703e+01; Test Loss: 7.6301e+02. (Time: 2.6s)Step 2600 of 1000000; Loss: 7.6458e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.9557e+00; Test Loss: 7.6444e+02. (Time: 3.8s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3630\n",
            "Running configuration: hidden_size=32, lr=0.0001, weight_decay=1e-05, batch_size=32\n",
            "Step 200 of 200; Loss: 1.6373e+03; Test Loss: 2.0698e+01. (Time: 2.6s)Step 200 of 1000000; Loss: 5.8663e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0900e+02; Test Loss: 1.8175e+01. (Time: 2.6s)Step 400 of 1000000; Loss: 5.4403e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3325e+03; Test Loss: 1.5412e+01. (Time: 2.5s)Step 600 of 1000000; Loss: 5.1289e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3567e+03; Test Loss: 1.1854e+01. (Time: 3.1s)Step 800 of 1000000; Loss: 4.7807e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3206e+02; Test Loss: 8.1998e+00. (Time: 3.4s)Step 1000 of 1000000; Loss: 4.3810e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0189e+03; Test Loss: 6.1049e+00. (Time: 2.6s)Step 1200 of 1000000; Loss: 4.1246e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1923e+03; Test Loss: 4.6338e+00. (Time: 2.5s)Step 1400 of 1000000; Loss: 4.0054e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.2018e+01; Test Loss: 1.5668e+00. (Time: 2.9s)Step 1600 of 1000000; Loss: 3.8858e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3929e+02; Test Loss: 1.4673e+00. (Time: 4.0s)Step 1800 of 1000000; Loss: 3.8716e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1690e+03; Test Loss: 1.4147e+00. (Time: 2.6s)Step 2000 of 1000000; Loss: 3.8660e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9944e+01; Test Loss: 1.3658e+00. (Time: 2.7s)Step 2200 of 1000000; Loss: 3.8572e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.1875e+02; Test Loss: 1.3107e+00. (Time: 2.7s)Step 2400 of 1000000; Loss: 3.8379e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1623e+03; Test Loss: 1.2541e+00. (Time: 3.1s)Step 2600 of 1000000; Loss: 3.8283e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3798e+01; Test Loss: 1.2041e+00. (Time: 3.4s)Step 2800 of 1000000; Loss: 3.8221e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.0301e+02; Test Loss: 1.1726e+00. (Time: 2.6s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3766\n",
            "Running configuration: hidden_size=32, lr=0.0001, weight_decay=1e-05, batch_size=64\n",
            "Step 200 of 200; Loss: 3.3057e+03; Test Loss: 1.1622e+03. (Time: 2.8s)Step 200 of 1000000; Loss: 1.1741e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4330e+03; Test Loss: 1.0800e+03. (Time: 2.8s)Step 400 of 1000000; Loss: 1.0881e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.1913e+02; Test Loss: 1.0167e+03. (Time: 4.0s)Step 600 of 1000000; Loss: 1.0248e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.5422e+01; Test Loss: 9.4004e+02. (Time: 2.8s)Step 800 of 1000000; Loss: 9.5109e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2758e+03; Test Loss: 8.6138e+02. (Time: 3.0s)Step 1000 of 1000000; Loss: 8.6922e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4257e+03; Test Loss: 8.1569e+02. (Time: 2.8s)Step 1200 of 1000000; Loss: 8.2007e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.2522e+02; Test Loss: 7.9400e+02. (Time: 3.9s)Step 1400 of 1000000; Loss: 7.9444e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.1558e+01; Test Loss: 7.7505e+02. (Time: 2.6s)Step 1600 of 1000000; Loss: 7.7521e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.4341e+00; Test Loss: 7.7269e+02. (Time: 2.7s)Step 1800 of 1000000; Loss: 7.7293e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9713e+03; Test Loss: 7.6987e+02. (Time: 2.6s)Step 2000 of 1000000; Loss: 7.7097e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3027e+03; Test Loss: 7.6743e+02. (Time: 3.0s)Step 2200 of 1000000; Loss: 7.6733e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.5159e+02; Test Loss: 7.6506e+02. (Time: 3.6s)Step 2400 of 1000000; Loss: 7.6465e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.9983e+01; Test Loss: 7.6325e+02. (Time: 2.7s)Step 2600 of 1000000; Loss: 7.6358e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.0164e+00; Test Loss: 7.6233e+02. (Time: 2.9s)Step 2800 of 1000000; Loss: 7.6281e+02. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9123e+03; Test Loss: 7.6337e+02. (Time: 2.7s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3625\n",
            "    hidden_size      lr  weight_decay  batch_size  avg_val_nll\n",
            "0             4  0.0010       0.00100          32     0.399959\n",
            "1             4  0.0010       0.00100          64     0.356652\n",
            "2             4  0.0010       0.00010          32     0.384748\n",
            "3             4  0.0010       0.00010          64     0.403082\n",
            "4             4  0.0010       0.00001          32     0.377664\n",
            "5             4  0.0010       0.00001          64     0.364952\n",
            "6             4  0.0001       0.00100          32     0.389951\n",
            "7             4  0.0001       0.00100          64     0.375713\n",
            "8             4  0.0001       0.00010          32     0.389553\n",
            "9             4  0.0001       0.00010          64     0.375531\n",
            "10            4  0.0001       0.00001          32     0.390012\n",
            "11            4  0.0001       0.00001          64     0.375703\n",
            "12            8  0.0010       0.00100          32     0.373275\n",
            "13            8  0.0010       0.00100          64     0.363468\n",
            "14            8  0.0010       0.00010          32     0.375798\n",
            "15            8  0.0010       0.00010          64     0.420555\n",
            "16            8  0.0010       0.00001          32     0.375909\n",
            "17            8  0.0010       0.00001          64     0.360633\n",
            "18            8  0.0001       0.00100          32     0.371786\n",
            "19            8  0.0001       0.00100          64     0.358261\n",
            "20            8  0.0001       0.00010          32     0.371784\n",
            "21            8  0.0001       0.00010          64     0.358256\n",
            "22            8  0.0001       0.00001          32     0.371785\n",
            "23            8  0.0001       0.00001          64     0.358277\n",
            "24           16  0.0010       0.00100          32     0.417845\n",
            "25           16  0.0010       0.00100          64     0.537365\n",
            "26           16  0.0010       0.00010          32     0.466773\n",
            "27           16  0.0010       0.00010          64     0.377628\n",
            "28           16  0.0010       0.00001          32     0.430365\n",
            "29           16  0.0010       0.00001          64     0.392974\n",
            "30           16  0.0001       0.00100          32     0.371164\n",
            "31           16  0.0001       0.00100          64     0.357949\n",
            "32           16  0.0001       0.00010          32     0.371150\n",
            "33           16  0.0001       0.00010          64     0.363966\n",
            "34           16  0.0001       0.00001          32     0.371005\n",
            "35           16  0.0001       0.00001          64     0.358116\n",
            "36           32  0.0010       0.00100          32     0.390831\n",
            "37           32  0.0010       0.00100          64     0.350560\n",
            "38           32  0.0010       0.00010          32     0.374856\n",
            "39           32  0.0010       0.00010          64     0.360271\n",
            "40           32  0.0010       0.00001          32     0.368557\n",
            "41           32  0.0010       0.00001          64     0.348696\n",
            "42           32  0.0001       0.00100          32     0.375099\n",
            "43           32  0.0001       0.00100          64     0.362073\n",
            "44           32  0.0001       0.00010          32     0.377265\n",
            "45           32  0.0001       0.00010          64     0.362980\n",
            "46           32  0.0001       0.00001          32     0.376613\n",
            "47           32  0.0001       0.00001          64     0.362476\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'hidden_size': [2, 4, 8, 16, 32, 64],\n",
        "    'lr': [1e-3, 1e-4],\n",
        "    'weight_decay': [1e-3, 1e-4, 1e-5],\n",
        "    'batch_size': [32, 64],\n",
        "}\n",
        "\n",
        "# Prepare to collect results\n",
        "output_path = '../Results/monkey_grid_search_results.csv'\n",
        "# 如果文件不存在，就写入 header；否则 append 时不写 header\n",
        "write_header = not os.path.exists(output_path)\n",
        "\n",
        "# Loop through all combinations\n",
        "for hs, lr, wd, bs in itertools.product(\n",
        "        param_grid['hidden_size'],\n",
        "        param_grid['lr'],\n",
        "        param_grid['weight_decay'],\n",
        "        param_grid['batch_size'],\n",
        "):\n",
        "    print(f\"Running configuration: hidden_size={hs}, lr={lr}, weight_decay={wd}, batch_size={bs}\")\n",
        "    dataset_m_train, dataset_m_test, dataset_m_validate = format_into_datasets_multi_monkey(\n",
        "        xs_list, ys_list,\n",
        "        dataset_constructor=rnn_utils.DatasetRNN,\n",
        "        n_train_sessions=267,    # 全体一共要选 200 个 session 做训练\n",
        "        n_test_sessions= 35,     # 全体 40 做测试\n",
        "        n_validate_sessions=35,  # 全体 40 做验证\n",
        "        batch_size=bs,\n",
        "        random_seed=42\n",
        "    )\n",
        "    n_hidden = hs\n",
        "    def make_vanilla_rnn():\n",
        "        model = hk.DeepRNN(\n",
        "            [hk.VanillaRNN(n_hidden), hk.Linear(output_size=2)]\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    # # set n_step_max for each lr\n",
        "    # if lr == 1e-3:\n",
        "    #     n_step_max = 10000\n",
        "    # elif lr == 1e-4:\n",
        "    #     n_step_max = 30000\n",
        "\n",
        "    # Fit the model    \n",
        "    t0 = time.time()\n",
        "    vanillaRNN_params, _, all_losses = rnn_utils.fit_model(\n",
        "        model_fun=make_vanilla_rnn,\n",
        "        dataset_train=dataset_m_train,\n",
        "        dataset_test=dataset_m_validate,\n",
        "        loss_fun='categorical',\n",
        "        optimizer=optax.chain(\n",
        "            optax.add_decayed_weights(wd),  # L2 regularization\n",
        "            optax.adam(learning_rate=lr)    # Adam optimizer\n",
        "        ),\n",
        "        n_steps_per_call=200,\n",
        "        n_steps_max=100000,\n",
        "        return_all_losses=True,\n",
        "        early_stop_step=400,\n",
        "        if_early_stop=True\n",
        "    )\n",
        "\n",
        "    # Extract validation losses (assuming all_losses is a dict with 'test' key)\n",
        "    avg_nll = compute_negative_log_likelihood(dataset_m_validate, make_vanilla_rnn, vanillaRNN_params)\n",
        "\n",
        "    # Record the result\n",
        "    result = {\n",
        "        'hidden_size': hs,\n",
        "        'lr': lr,\n",
        "        'weight_decay': wd,\n",
        "        'batch_size': bs,\n",
        "        'avg_val_nll': avg_nll,\n",
        "        'time': time.time()-t0,\n",
        "    }\n",
        "\n",
        "    pd.DataFrame([result]).to_csv(\n",
        "        output_path,\n",
        "        mode='a',            # 追加而非覆盖\n",
        "        index=False,\n",
        "        header=write_header  # 只有第一次写入 header\n",
        "    )\n",
        "    write_header = False   # header 只写一次"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrMRyuyLTqPS"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('../Results/monkey_grid_search_results.csv')\n",
        "\n",
        "# 打印完整的 DataFrame\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# select the best hyperparams\n",
        "# 1. 找到 'avg_val_nll' 最小值所在的行索引\n",
        "best_idx = df['avg_val_nll'].idxmin()\n",
        "# 2. 取出该行\n",
        "best_row = df.loc[best_idx]\n",
        "# 3. 如果你只想要超参（不包括 avg_val_nll 本身），可以这样：\n",
        "best_params = best_row[['hidden_size', 'lr', 'weight_decay', 'batch_size']].to_dict()\n",
        "print(\"最优参数：\", best_params)\n",
        "print(\"对应最小 avg_val_nll:\", best_row['avg_val_nll'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etPOqCE6RYcz"
      },
      "source": [
        "#### Now for outer loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yKvCVON0OWM",
        "outputId": "734f181f-c36d-401f-f31c-db60f18566ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 0:\n",
            "  Monkey 0: train=76, val=10, test=10\n",
            "  Monkey 1: train=74, val=10, test=10\n",
            "  Monkey 2: train=117, val=15, test=15\n",
            "Fold 1:\n",
            "  Monkey 0: train=76, val=10, test=10\n",
            "  Monkey 1: train=74, val=10, test=10\n",
            "  Monkey 2: train=117, val=15, test=15\n",
            "Fold 2:\n",
            "  Monkey 0: train=76, val=10, test=10\n",
            "  Monkey 1: train=74, val=10, test=10\n",
            "  Monkey 2: train=117, val=15, test=15\n",
            "Fold 3:\n",
            "  Monkey 0: train=76, val=10, test=10\n",
            "  Monkey 1: train=74, val=10, test=10\n",
            "  Monkey 2: train=117, val=15, test=15\n",
            "Fold 4:\n",
            "  Monkey 0: train=76, val=10, test=10\n",
            "  Monkey 1: train=76, val=9, test=9\n",
            "  Monkey 2: train=117, val=15, test=15\n",
            "Fold 5:\n",
            "  Monkey 0: train=76, val=10, test=10\n",
            "  Monkey 1: train=76, val=9, test=9\n",
            "  Monkey 2: train=117, val=15, test=15\n",
            "Fold 6:\n",
            "  Monkey 0: train=78, val=9, test=9\n",
            "  Monkey 1: train=76, val=9, test=9\n",
            "  Monkey 2: train=117, val=15, test=15\n",
            "Fold 7:\n",
            "  Monkey 0: train=78, val=9, test=9\n",
            "  Monkey 1: train=76, val=9, test=9\n",
            "  Monkey 2: train=119, val=14, test=14\n",
            "Fold 8:\n",
            "  Monkey 0: train=78, val=9, test=9\n",
            "  Monkey 1: train=76, val=9, test=9\n",
            "  Monkey 2: train=119, val=14, test=14\n",
            "Fold 9:\n",
            "  Monkey 0: train=78, val=9, test=9\n",
            "  Monkey 1: train=76, val=9, test=9\n",
            "  Monkey 2: train=119, val=14, test=14\n",
            "=== Fold 0 ===\n",
            "Step 500 of 500; Loss: 3.3527e+03; Test Loss: 1.3474e+03. (Time: 4.8s)Step 500 of 1000000; Loss: 1.3485e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.6985e+03; Test Loss: 1.3233e+03. (Time: 5.5s)Step 1000 of 1000000; Loss: 1.3245e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.4860e+02; Test Loss: 1.2962e+03. (Time: 4.4s)Step 1500 of 1000000; Loss: 1.2976e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1732e+02; Test Loss: 1.2672e+03. (Time: 4.4s)Step 2000 of 1000000; Loss: 1.2686e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.2130e+03; Test Loss: 1.2377e+03. (Time: 5.4s)Step 2500 of 1000000; Loss: 1.2391e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.0908e+03; Test Loss: 1.2129e+03. (Time: 4.5s)Step 3000 of 1000000; Loss: 1.2139e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.4713e+03; Test Loss: 1.1922e+03. (Time: 4.6s)Step 3500 of 1000000; Loss: 1.1932e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.0438e+02; Test Loss: 1.1743e+03. (Time: 5.4s)Step 4000 of 1000000; Loss: 1.1751e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.8368e+01; Test Loss: 1.1591e+03. (Time: 4.6s)Step 4500 of 1000000; Loss: 1.1599e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.9463e+03; Test Loss: 1.1444e+03. (Time: 5.7s)Step 5000 of 1000000; Loss: 1.1452e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.8899e+03; Test Loss: 1.1299e+03. (Time: 4.4s)Step 5500 of 1000000; Loss: 1.1306e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.3431e+03; Test Loss: 1.1151e+03. (Time: 4.4s)Step 6000 of 1000000; Loss: 1.1158e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.0965e+02; Test Loss: 1.0996e+03. (Time: 5.6s)Step 6500 of 1000000; Loss: 1.1004e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.1473e+01; Test Loss: 1.0842e+03. (Time: 4.4s)Step 7000 of 1000000; Loss: 1.0849e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.7443e+03; Test Loss: 1.0689e+03. (Time: 4.6s)Step 7500 of 1000000; Loss: 1.0696e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.7239e+03; Test Loss: 1.0531e+03. (Time: 5.3s)Step 8000 of 1000000; Loss: 1.0539e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2133e+03; Test Loss: 1.0372e+03. (Time: 4.3s)Step 8500 of 1000000; Loss: 1.0380e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.0767e+02; Test Loss: 1.0211e+03. (Time: 5.2s)Step 9000 of 1000000; Loss: 1.0219e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.2799e+01; Test Loss: 1.0052e+03. (Time: 4.9s)Step 9500 of 1000000; Loss: 1.0059e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5558e+03; Test Loss: 9.8982e+02. (Time: 4.3s)Step 10000 of 1000000; Loss: 9.9055e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5773e+03; Test Loss: 9.7477e+02. (Time: 5.7s)Step 10500 of 1000000; Loss: 9.7551e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0851e+03; Test Loss: 9.6012e+02. (Time: 4.3s)Step 11000 of 1000000; Loss: 9.6083e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.9213e+02; Test Loss: 9.4608e+02. (Time: 4.4s)Step 11500 of 1000000; Loss: 9.4677e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.4744e+01; Test Loss: 9.3174e+02. (Time: 5.4s)Step 12000 of 1000000; Loss: 9.3243e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3937e+03; Test Loss: 9.1735e+02. (Time: 4.5s)Step 12500 of 1000000; Loss: 9.1804e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.4530e+03; Test Loss: 9.0298e+02. (Time: 5.4s)Step 13000 of 1000000; Loss: 9.0369e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.7682e+02; Test Loss: 8.8900e+02. (Time: 4.6s)Step 13500 of 1000000; Loss: 8.8969e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9459e+02; Test Loss: 8.7569e+02. (Time: 4.9s)Step 14000 of 1000000; Loss: 8.7634e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.0855e+01; Test Loss: 8.6307e+02. (Time: 6.1s)Step 14500 of 1000000; Loss: 8.6367e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2489e+03; Test Loss: 8.5006e+02. (Time: 4.8s)Step 15000 of 1000000; Loss: 8.5068e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3428e+03; Test Loss: 8.3795e+02. (Time: 5.8s)Step 15500 of 1000000; Loss: 8.3851e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.8940e+02; Test Loss: 8.2789e+02. (Time: 4.6s)Step 16000 of 1000000; Loss: 8.2830e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1885e+02; Test Loss: 8.1902e+02. (Time: 4.5s)Step 16500 of 1000000; Loss: 8.1940e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0584e+01; Test Loss: 8.1097e+02. (Time: 6.7s)Step 17000 of 1000000; Loss: 8.1146e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1414e+03; Test Loss: 8.0088e+02. (Time: 4.7s)Step 17500 of 1000000; Loss: 8.0116e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2659e+03; Test Loss: 7.9603e+02. (Time: 5.3s)Step 18000 of 1000000; Loss: 7.9628e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.4327e+02; Test Loss: 7.9251e+02. (Time: 5.3s)Step 18500 of 1000000; Loss: 7.9269e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.9654e+01; Test Loss: 7.9019e+02. (Time: 4.6s)Step 19000 of 1000000; Loss: 7.9027e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.5083e+01; Test Loss: 7.8807e+02. (Time: 6.0s)Step 19500 of 1000000; Loss: 7.8812e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0980e+03; Test Loss: 7.8635e+02. (Time: 5.1s)Step 20000 of 1000000; Loss: 7.8640e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2395e+03; Test Loss: 7.8510e+02. (Time: 5.9s)Step 20500 of 1000000; Loss: 7.8518e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.2288e+02; Test Loss: 7.8397e+02. (Time: 4.9s)Step 21000 of 1000000; Loss: 7.8403e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.3233e+01; Test Loss: 7.8274e+02. (Time: 4.6s)Step 21500 of 1000000; Loss: 7.8280e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2634e+01; Test Loss: 7.8188e+02. (Time: 5.8s)Step 22000 of 1000000; Loss: 7.8189e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0758e+03; Test Loss: 7.8117e+02. (Time: 4.9s)Step 22500 of 1000000; Loss: 7.8122e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2278e+03; Test Loss: 7.8039e+02. (Time: 5.2s)Step 23000 of 1000000; Loss: 7.8044e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.1101e+02; Test Loss: 7.7961e+02. (Time: 5.2s)Step 23500 of 1000000; Loss: 7.7965e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.4051e+01; Test Loss: 7.7879e+02. (Time: 4.7s)Step 24000 of 1000000; Loss: 7.7887e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0921e+01; Test Loss: 7.7766e+02. (Time: 5.6s)Step 24500 of 1000000; Loss: 7.7769e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0555e+03; Test Loss: 7.7640e+02. (Time: 4.6s)Step 25000 of 1000000; Loss: 7.7644e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2177e+03; Test Loss: 7.7555e+02. (Time: 5.3s)Step 25500 of 1000000; Loss: 7.7559e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.0225e+02; Test Loss: 7.7508e+02. (Time: 5.6s)Step 26000 of 1000000; Loss: 7.7510e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.6692e+01; Test Loss: 7.7474e+02. (Time: 4.5s)Step 26500 of 1000000; Loss: 7.7476e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.6222e+00; Test Loss: 7.7425e+02. (Time: 5.9s)Step 27000 of 1000000; Loss: 7.7425e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0379e+03; Test Loss: 7.7375e+02. (Time: 4.5s)Step 27500 of 1000000; Loss: 7.7379e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2053e+03; Test Loss: 7.7358e+02. (Time: 4.5s)Step 28000 of 1000000; Loss: 7.7361e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.9513e+02; Test Loss: 7.7312e+02. (Time: 5.9s)Step 28500 of 1000000; Loss: 7.7317e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.2105e+01; Test Loss: 7.7273e+02. (Time: 4.5s)Step 29000 of 1000000; Loss: 7.7275e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.7415e+00; Test Loss: 7.7215e+02. (Time: 5.2s)Step 29500 of 1000000; Loss: 7.7215e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0223e+03; Test Loss: 7.7149e+02. (Time: 4.8s)Step 30000 of 1000000; Loss: 7.7158e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1949e+03; Test Loss: 7.7095e+02. (Time: 4.4s)Step 30500 of 1000000; Loss: 7.7103e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.8958e+02; Test Loss: 7.7011e+02. (Time: 5.5s)Step 31000 of 1000000; Loss: 7.7019e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.7773e+01; Test Loss: 7.6926e+02. (Time: 4.5s)Step 31500 of 1000000; Loss: 7.6933e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.8855e+00; Test Loss: 7.6813e+02. (Time: 4.4s)Step 32000 of 1000000; Loss: 7.6813e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0052e+03; Test Loss: 7.6691e+02. (Time: 5.6s)Step 32500 of 1000000; Loss: 7.6697e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1852e+03; Test Loss: 7.6591e+02. (Time: 4.5s)Step 33000 of 1000000; Loss: 7.6599e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.8399e+02; Test Loss: 7.6526e+02. (Time: 5.2s)Step 33500 of 1000000; Loss: 7.6528e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.3509e+01; Test Loss: 7.6504e+02. (Time: 4.9s)Step 34000 of 1000000; Loss: 7.6504e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.3665e+00; Test Loss: 7.6461e+02. (Time: 4.6s)Step 34500 of 1000000; Loss: 7.6457e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9914e+03; Test Loss: 7.6414e+02. (Time: 5.6s)Step 35000 of 1000000; Loss: 7.6418e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1751e+03; Test Loss: 7.6381e+02. (Time: 4.5s)Step 35500 of 1000000; Loss: 7.6389e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.8073e+02; Test Loss: 7.6377e+02. (Time: 4.5s)Step 36000 of 1000000; Loss: 7.6379e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.0838e+01; Test Loss: 7.6353e+02. (Time: 5.8s)Step 36500 of 1000000; Loss: 7.6354e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.0228e+00; Test Loss: 7.6322e+02. (Time: 5.1s)Step 37000 of 1000000; Loss: 7.6322e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9799e+03; Test Loss: 7.6295e+02. (Time: 6.6s)Step 37500 of 1000000; Loss: 7.6298e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1665e+03; Test Loss: 7.6279e+02. (Time: 4.6s)Step 38000 of 1000000; Loss: 7.6281e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.7757e+02; Test Loss: 7.6280e+02. (Time: 4.5s)Step 38500 of 1000000; Loss: 7.6280e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.8825e+01; Test Loss: 7.6291e+02. (Time: 5.6s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.2872\n",
            "Fold 0 test Avg NLL: 0.2872\n",
            "\n",
            "=== Fold 1 ===\n",
            "Step 500 of 500; Loss: 3.3922e+03; Test Loss: 1.2918e+03. (Time: 4.5s)Step 500 of 1000000; Loss: 1.2931e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.6873e+03; Test Loss: 1.2647e+03. (Time: 5.6s)Step 1000 of 1000000; Loss: 1.2660e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.6697e+02; Test Loss: 1.2359e+03. (Time: 4.4s)Step 1500 of 1000000; Loss: 1.2374e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1250e+02; Test Loss: 1.2055e+03. (Time: 4.4s)Step 2000 of 1000000; Loss: 1.2070e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1405e+03; Test Loss: 1.1740e+03. (Time: 5.5s)Step 2500 of 1000000; Loss: 1.1755e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1158e+03; Test Loss: 1.1476e+03. (Time: 4.4s)Step 3000 of 1000000; Loss: 1.1487e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.4719e+03; Test Loss: 1.1254e+03. (Time: 4.8s)Step 3500 of 1000000; Loss: 1.1265e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.1950e+02; Test Loss: 1.1061e+03. (Time: 5.3s)Step 4000 of 1000000; Loss: 1.1070e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.4298e+01; Test Loss: 1.0894e+03. (Time: 4.6s)Step 4500 of 1000000; Loss: 1.0901e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.8798e+03; Test Loss: 1.0734e+03. (Time: 5.5s)Step 5000 of 1000000; Loss: 1.0741e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.9084e+03; Test Loss: 1.0578e+03. (Time: 4.5s)Step 5500 of 1000000; Loss: 1.0585e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.3498e+03; Test Loss: 1.0416e+03. (Time: 4.4s)Step 6000 of 1000000; Loss: 1.0425e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.2335e+02; Test Loss: 1.0251e+03. (Time: 5.6s)Step 6500 of 1000000; Loss: 1.0260e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.8160e+01; Test Loss: 1.0087e+03. (Time: 4.4s)Step 7000 of 1000000; Loss: 1.0095e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.6818e+03; Test Loss: 9.9252e+02. (Time: 4.8s)Step 7500 of 1000000; Loss: 9.9332e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.7385e+03; Test Loss: 9.7625e+02. (Time: 5.1s)Step 8000 of 1000000; Loss: 9.7705e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2241e+03; Test Loss: 9.6012e+02. (Time: 4.4s)Step 8500 of 1000000; Loss: 9.6089e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.1213e+02; Test Loss: 9.4400e+02. (Time: 5.6s)Step 9000 of 1000000; Loss: 9.4480e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.8459e+01; Test Loss: 9.2789e+02. (Time: 4.9s)Step 9500 of 1000000; Loss: 9.2866e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.4991e+03; Test Loss: 9.1230e+02. (Time: 4.6s)Step 10000 of 1000000; Loss: 9.1305e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5899e+03; Test Loss: 8.9700e+02. (Time: 6.1s)Step 10500 of 1000000; Loss: 8.9774e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0991e+03; Test Loss: 8.8222e+02. (Time: 4.5s)Step 11000 of 1000000; Loss: 8.8294e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.8999e+02; Test Loss: 8.6742e+02. (Time: 5.4s)Step 11500 of 1000000; Loss: 8.6814e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.7881e+01; Test Loss: 8.5290e+02. (Time: 4.8s)Step 12000 of 1000000; Loss: 8.5362e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3291e+03; Test Loss: 8.3852e+02. (Time: 4.6s)Step 12500 of 1000000; Loss: 8.3922e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.4490e+03; Test Loss: 8.2425e+02. (Time: 6.0s)Step 13000 of 1000000; Loss: 8.2491e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.9407e+02; Test Loss: 8.1093e+02. (Time: 5.1s)Step 13500 of 1000000; Loss: 8.1157e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0247e+02; Test Loss: 7.9810e+02. (Time: 5.3s)Step 14000 of 1000000; Loss: 7.9871e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3922e+01; Test Loss: 7.8534e+02. (Time: 5.0s)Step 14500 of 1000000; Loss: 7.8597e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1782e+03; Test Loss: 7.7168e+02. (Time: 4.5s)Step 15000 of 1000000; Loss: 7.7224e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3271e+03; Test Loss: 7.6333e+02. (Time: 5.8s)Step 15500 of 1000000; Loss: 7.6368e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.0206e+02; Test Loss: 7.5705e+02. (Time: 4.4s)Step 16000 of 1000000; Loss: 7.5733e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1862e+02; Test Loss: 7.5200e+02. (Time: 4.7s)Step 16500 of 1000000; Loss: 7.5222e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.6169e+01; Test Loss: 7.4814e+02. (Time: 5.7s)Step 17000 of 1000000; Loss: 7.4831e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0802e+03; Test Loss: 7.4498e+02. (Time: 4.5s)Step 17500 of 1000000; Loss: 7.4513e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2568e+03; Test Loss: 7.4227e+02. (Time: 5.6s)Step 18000 of 1000000; Loss: 7.4240e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.6409e+02; Test Loss: 7.4005e+02. (Time: 4.5s)Step 18500 of 1000000; Loss: 7.4015e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.6488e+01; Test Loss: 7.3834e+02. (Time: 4.4s)Step 19000 of 1000000; Loss: 7.3842e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.4154e+01; Test Loss: 7.3679e+02. (Time: 5.5s)Step 19500 of 1000000; Loss: 7.3686e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0399e+03; Test Loss: 7.3552e+02. (Time: 4.4s)Step 20000 of 1000000; Loss: 7.3558e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2258e+03; Test Loss: 7.3441e+02. (Time: 4.6s)Step 20500 of 1000000; Loss: 7.3447e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.4291e+02; Test Loss: 7.3311e+02. (Time: 5.4s)Step 21000 of 1000000; Loss: 7.3317e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.6100e+01; Test Loss: 7.3194e+02. (Time: 4.4s)Step 21500 of 1000000; Loss: 7.3200e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2437e+01; Test Loss: 7.3092e+02. (Time: 5.4s)Step 22000 of 1000000; Loss: 7.3097e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0161e+03; Test Loss: 7.2983e+02. (Time: 4.8s)Step 22500 of 1000000; Loss: 7.2990e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2069e+03; Test Loss: 7.2879e+02. (Time: 4.8s)Step 23000 of 1000000; Loss: 7.2883e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.2853e+02; Test Loss: 7.2819e+02. (Time: 5.5s)Step 23500 of 1000000; Loss: 7.2820e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.4415e+01; Test Loss: 7.2770e+02. (Time: 4.4s)Step 24000 of 1000000; Loss: 7.2774e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1264e+01; Test Loss: 7.2690e+02. (Time: 4.7s)Step 24500 of 1000000; Loss: 7.2696e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9917e+03; Test Loss: 7.2595e+02. (Time: 5.2s)Step 25000 of 1000000; Loss: 7.2601e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1926e+03; Test Loss: 7.2514e+02. (Time: 4.5s)Step 25500 of 1000000; Loss: 7.2518e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.1752e+02; Test Loss: 7.2430e+02. (Time: 5.8s)Step 26000 of 1000000; Loss: 7.2434e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.7463e+01; Test Loss: 7.2337e+02. (Time: 4.6s)Step 26500 of 1000000; Loss: 7.2343e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0078e+01; Test Loss: 7.2237e+02. (Time: 4.4s)Step 27000 of 1000000; Loss: 7.2241e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9776e+03; Test Loss: 7.2158e+02. (Time: 5.6s)Step 27500 of 1000000; Loss: 7.2162e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1810e+03; Test Loss: 7.2068e+02. (Time: 4.5s)Step 28000 of 1000000; Loss: 7.2071e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.1027e+02; Test Loss: 7.1976e+02. (Time: 5.0s)Step 28500 of 1000000; Loss: 7.1980e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.2875e+01; Test Loss: 7.1886e+02. (Time: 5.2s)Step 29000 of 1000000; Loss: 7.1890e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.0595e+00; Test Loss: 7.1805e+02. (Time: 4.5s)Step 29500 of 1000000; Loss: 7.1808e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9623e+03; Test Loss: 7.1741e+02. (Time: 5.6s)Step 30000 of 1000000; Loss: 7.1745e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1715e+03; Test Loss: 7.1673e+02. (Time: 4.4s)Step 30500 of 1000000; Loss: 7.1676e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.0408e+02; Test Loss: 7.1609e+02. (Time: 4.4s)Step 31000 of 1000000; Loss: 7.1612e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.9181e+01; Test Loss: 7.1536e+02. (Time: 5.8s)Step 31500 of 1000000; Loss: 7.1539e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.1514e+00; Test Loss: 7.1485e+02. (Time: 4.5s)Step 32000 of 1000000; Loss: 7.1486e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9494e+03; Test Loss: 7.1442e+02. (Time: 4.8s)Step 32500 of 1000000; Loss: 7.1444e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1614e+03; Test Loss: 7.1398e+02. (Time: 5.3s)Step 33000 of 1000000; Loss: 7.1400e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.9925e+02; Test Loss: 7.1355e+02. (Time: 4.4s)Step 33500 of 1000000; Loss: 7.1357e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.6413e+01; Test Loss: 7.1312e+02. (Time: 5.6s)Step 34000 of 1000000; Loss: 7.1314e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.3299e+00; Test Loss: 7.1271e+02. (Time: 4.5s)Step 34500 of 1000000; Loss: 7.1273e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9376e+03; Test Loss: 7.1229e+02. (Time: 4.4s)Step 35000 of 1000000; Loss: 7.1231e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1532e+03; Test Loss: 7.1185e+02. (Time: 5.6s)Step 35500 of 1000000; Loss: 7.1187e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.9600e+02; Test Loss: 7.1147e+02. (Time: 4.4s)Step 36000 of 1000000; Loss: 7.1149e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.4149e+01; Test Loss: 7.1109e+02. (Time: 4.8s)Step 36500 of 1000000; Loss: 7.1111e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.6172e+00; Test Loss: 7.1068e+02. (Time: 5.6s)Step 37000 of 1000000; Loss: 7.1070e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9272e+03; Test Loss: 7.1036e+02. (Time: 4.7s)Step 37500 of 1000000; Loss: 7.1038e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1455e+03; Test Loss: 7.0990e+02. (Time: 5.7s)Step 38000 of 1000000; Loss: 7.0994e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.9305e+02; Test Loss: 7.0956e+02. (Time: 4.5s)Step 38500 of 1000000; Loss: 7.0957e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.2417e+01; Test Loss: 7.0926e+02. (Time: 4.4s)Step 39000 of 1000000; Loss: 7.0928e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.0541e+00; Test Loss: 7.0902e+02. (Time: 5.6s)Step 39500 of 1000000; Loss: 7.0904e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9170e+03; Test Loss: 7.0876e+02. (Time: 4.4s)Step 40000 of 1000000; Loss: 7.0876e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1373e+03; Test Loss: 7.0848e+02. (Time: 5.1s)Step 40500 of 1000000; Loss: 7.0849e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.9049e+02; Test Loss: 7.0824e+02. (Time: 4.9s)Step 41000 of 1000000; Loss: 7.0826e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1012e+01; Test Loss: 7.0802e+02. (Time: 4.5s)Step 41500 of 1000000; Loss: 7.0805e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.5270e+00; Test Loss: 7.0791e+02. (Time: 5.5s)Step 42000 of 1000000; Loss: 7.0791e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9082e+03; Test Loss: 7.0789e+02. (Time: 4.5s)Step 42500 of 1000000; Loss: 7.0788e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1290e+03; Test Loss: 7.0778e+02. (Time: 4.4s)Step 43000 of 1000000; Loss: 7.0778e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.8809e+02; Test Loss: 7.0774e+02. (Time: 5.6s)Step 43500 of 1000000; Loss: 7.0775e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.9797e+01; Test Loss: 7.0768e+02. (Time: 4.4s)Step 44000 of 1000000; Loss: 7.0769e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.1199e+00; Test Loss: 7.0758e+02. (Time: 5.1s)Step 44500 of 1000000; Loss: 7.0759e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8990e+03; Test Loss: 7.0757e+02. (Time: 5.1s)Step 45000 of 1000000; Loss: 7.0757e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1215e+03; Test Loss: 7.0736e+02. (Time: 4.5s)Step 45500 of 1000000; Loss: 7.0737e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.8552e+02; Test Loss: 7.0721e+02. (Time: 5.5s)Step 46000 of 1000000; Loss: 7.0722e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.8637e+01; Test Loss: 7.0724e+02. (Time: 4.4s)Step 46500 of 1000000; Loss: 7.0725e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.7387e+00; Test Loss: 7.0722e+02. (Time: 4.5s)Step 47000 of 1000000; Loss: 7.0722e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8905e+03; Test Loss: 7.0728e+02. (Time: 5.5s)Step 47500 of 1000000; Loss: 7.0728e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1145e+03; Test Loss: 7.0719e+02. (Time: 4.4s)Step 48000 of 1000000; Loss: 7.0719e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.8344e+02; Test Loss: 7.0709e+02. (Time: 4.9s)Step 48500 of 1000000; Loss: 7.0710e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.7668e+01; Test Loss: 7.0716e+02. (Time: 5.0s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3293\n",
            "Fold 1 test Avg NLL: 0.3293\n",
            "\n",
            "=== Fold 2 ===\n",
            "Step 500 of 500; Loss: 3.4177e+03; Test Loss: 1.3132e+03. (Time: 4.3s)Step 500 of 1000000; Loss: 1.3145e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.7097e+03; Test Loss: 1.2865e+03. (Time: 5.7s)Step 1000 of 1000000; Loss: 1.2879e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.6171e+02; Test Loss: 1.2582e+03. (Time: 4.5s)Step 1500 of 1000000; Loss: 1.2596e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2741e+02; Test Loss: 1.2295e+03. (Time: 4.5s)Step 2000 of 1000000; Loss: 1.2309e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1438e+03; Test Loss: 1.2004e+03. (Time: 5.8s)Step 2500 of 1000000; Loss: 1.2018e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1455e+03; Test Loss: 1.1764e+03. (Time: 4.7s)Step 3000 of 1000000; Loss: 1.1774e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.4964e+03; Test Loss: 1.1557e+03. (Time: 5.4s)Step 3500 of 1000000; Loss: 1.1567e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.1823e+02; Test Loss: 1.1373e+03. (Time: 5.4s)Step 4000 of 1000000; Loss: 1.1381e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0706e+02; Test Loss: 1.1213e+03. (Time: 4.6s)Step 4500 of 1000000; Loss: 1.1220e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.8981e+03; Test Loss: 1.1062e+03. (Time: 5.7s)Step 5000 of 1000000; Loss: 1.1069e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.9377e+03; Test Loss: 1.0913e+03. (Time: 4.6s)Step 5500 of 1000000; Loss: 1.0920e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.3711e+03; Test Loss: 1.0757e+03. (Time: 5.3s)Step 6000 of 1000000; Loss: 1.0765e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.2240e+02; Test Loss: 1.0598e+03. (Time: 5.2s)Step 6500 of 1000000; Loss: 1.0606e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.7693e+01; Test Loss: 1.0440e+03. (Time: 4.5s)Step 7000 of 1000000; Loss: 1.0447e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.7165e+03; Test Loss: 1.0284e+03. (Time: 5.5s)Step 7500 of 1000000; Loss: 1.0292e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.7726e+03; Test Loss: 1.0128e+03. (Time: 4.4s)Step 8000 of 1000000; Loss: 1.0135e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2370e+03; Test Loss: 9.9689e+02. (Time: 4.5s)Step 8500 of 1000000; Loss: 9.9769e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.9946e+02; Test Loss: 9.8084e+02. (Time: 5.7s)Step 9000 of 1000000; Loss: 9.8161e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.3379e+01; Test Loss: 9.6513e+02. (Time: 4.5s)Step 9500 of 1000000; Loss: 9.6589e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5516e+03; Test Loss: 9.5001e+02. (Time: 5.3s)Step 10000 of 1000000; Loss: 9.5072e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.6224e+03; Test Loss: 9.3547e+02. (Time: 5.0s)Step 10500 of 1000000; Loss: 9.3618e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0945e+03; Test Loss: 9.2104e+02. (Time: 4.5s)Step 11000 of 1000000; Loss: 9.2174e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.6382e+02; Test Loss: 9.0695e+02. (Time: 5.6s)Step 11500 of 1000000; Loss: 9.0763e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.0982e+01; Test Loss: 8.9336e+02. (Time: 4.5s)Step 12000 of 1000000; Loss: 8.9403e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3815e+03; Test Loss: 8.7983e+02. (Time: 4.6s)Step 12500 of 1000000; Loss: 8.8049e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.4600e+03; Test Loss: 8.6470e+02. (Time: 5.7s)Step 13000 of 1000000; Loss: 8.6557e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.7601e+02; Test Loss: 8.4617e+02. (Time: 4.4s)Step 13500 of 1000000; Loss: 8.4695e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.7860e+02; Test Loss: 8.3417e+02. (Time: 5.2s)Step 14000 of 1000000; Loss: 8.3464e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1069e+01; Test Loss: 8.2574e+02. (Time: 4.7s)Step 14500 of 1000000; Loss: 8.2612e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2273e+03; Test Loss: 8.1855e+02. (Time: 4.4s)Step 15000 of 1000000; Loss: 8.1888e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3224e+03; Test Loss: 8.1219e+02. (Time: 5.5s)Step 15500 of 1000000; Loss: 8.1248e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.2084e+02; Test Loss: 8.0643e+02. (Time: 4.5s)Step 16000 of 1000000; Loss: 8.0672e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.3921e+02; Test Loss: 8.0092e+02. (Time: 4.4s)Step 16500 of 1000000; Loss: 8.0118e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.6612e+01; Test Loss: 7.9605e+02. (Time: 5.6s)Step 17000 of 1000000; Loss: 7.9629e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1521e+03; Test Loss: 7.9148e+02. (Time: 4.5s)Step 17500 of 1000000; Loss: 7.9168e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2551e+03; Test Loss: 7.8766e+02. (Time: 5.1s)Step 18000 of 1000000; Loss: 7.8786e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.8695e+02; Test Loss: 7.8383e+02. (Time: 5.1s)Step 18500 of 1000000; Loss: 7.8404e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.8082e+01; Test Loss: 7.7954e+02. (Time: 4.7s)Step 19000 of 1000000; Loss: 7.7974e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.3168e+01; Test Loss: 7.7591e+02. (Time: 5.5s)Step 19500 of 1000000; Loss: 7.7606e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1089e+03; Test Loss: 7.7293e+02. (Time: 4.5s)Step 20000 of 1000000; Loss: 7.7306e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2198e+03; Test Loss: 7.7039e+02. (Time: 4.5s)Step 20500 of 1000000; Loss: 7.7050e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.6646e+02; Test Loss: 7.6831e+02. (Time: 5.7s)Step 21000 of 1000000; Loss: 7.6842e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.1781e+01; Test Loss: 7.6639e+02. (Time: 4.5s)Step 21500 of 1000000; Loss: 7.6648e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0429e+01; Test Loss: 7.6474e+02. (Time: 5.3s)Step 22000 of 1000000; Loss: 7.6483e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0828e+03; Test Loss: 7.6305e+02. (Time: 4.6s)Step 22500 of 1000000; Loss: 7.6313e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2037e+03; Test Loss: 7.6147e+02. (Time: 5.0s)Step 23000 of 1000000; Loss: 7.6155e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.5399e+02; Test Loss: 7.6030e+02. (Time: 5.6s)Step 23500 of 1000000; Loss: 7.6035e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.7786e+01; Test Loss: 7.5953e+02. (Time: 4.4s)Step 24000 of 1000000; Loss: 7.5956e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.9415e+00; Test Loss: 7.5879e+02. (Time: 4.7s)Step 24500 of 1000000; Loss: 7.5883e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0643e+03; Test Loss: 7.5811e+02. (Time: 5.5s)Step 25000 of 1000000; Loss: 7.5813e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1936e+03; Test Loss: 7.5759e+02. (Time: 4.5s)Step 25500 of 1000000; Loss: 7.5761e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.4358e+02; Test Loss: 7.5701e+02. (Time: 5.6s)Step 26000 of 1000000; Loss: 7.5704e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.8405e+01; Test Loss: 7.5655e+02. (Time: 4.4s)Step 26500 of 1000000; Loss: 7.5657e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.0361e+00; Test Loss: 7.5613e+02. (Time: 4.4s)Step 27000 of 1000000; Loss: 7.5615e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0471e+03; Test Loss: 7.5575e+02. (Time: 5.6s)Step 27500 of 1000000; Loss: 7.5577e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1823e+03; Test Loss: 7.5534e+02. (Time: 4.7s)Step 28000 of 1000000; Loss: 7.5536e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.3480e+02; Test Loss: 7.5493e+02. (Time: 5.2s)Step 28500 of 1000000; Loss: 7.5495e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.1840e+01; Test Loss: 7.5462e+02. (Time: 5.3s)Step 29000 of 1000000; Loss: 7.5464e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.3922e+00; Test Loss: 7.5410e+02. (Time: 4.5s)Step 29500 of 1000000; Loss: 7.5413e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0310e+03; Test Loss: 7.5365e+02. (Time: 5.4s)Step 30000 of 1000000; Loss: 7.5366e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1764e+03; Test Loss: 7.5321e+02. (Time: 4.4s)Step 30500 of 1000000; Loss: 7.5322e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.2632e+02; Test Loss: 7.5282e+02. (Time: 4.5s)Step 31000 of 1000000; Loss: 7.5284e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.6956e+01; Test Loss: 7.5246e+02. (Time: 5.6s)Step 31500 of 1000000; Loss: 7.5248e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.9629e+00; Test Loss: 7.5213e+02. (Time: 4.4s)Step 32000 of 1000000; Loss: 7.5215e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0188e+03; Test Loss: 7.5185e+02. (Time: 5.0s)Step 32500 of 1000000; Loss: 7.5186e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1693e+03; Test Loss: 7.5160e+02. (Time: 4.9s)Step 33000 of 1000000; Loss: 7.5160e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.1997e+02; Test Loss: 7.5137e+02. (Time: 4.4s)Step 33500 of 1000000; Loss: 7.5139e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.3468e+01; Test Loss: 7.5104e+02. (Time: 5.6s)Step 34000 of 1000000; Loss: 7.5105e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.6241e+00; Test Loss: 7.5077e+02. (Time: 4.6s)Step 34500 of 1000000; Loss: 7.5078e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0059e+03; Test Loss: 7.5048e+02. (Time: 4.6s)Step 35000 of 1000000; Loss: 7.5049e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1628e+03; Test Loss: 7.5021e+02. (Time: 5.7s)Step 35500 of 1000000; Loss: 7.5022e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.1497e+02; Test Loss: 7.4997e+02. (Time: 4.6s)Step 36000 of 1000000; Loss: 7.4998e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.0957e+01; Test Loss: 7.4974e+02. (Time: 5.3s)Step 36500 of 1000000; Loss: 7.4975e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.3148e+00; Test Loss: 7.4948e+02. (Time: 5.1s)Step 37000 of 1000000; Loss: 7.4950e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9947e+03; Test Loss: 7.4923e+02. (Time: 4.5s)Step 37500 of 1000000; Loss: 7.4923e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1555e+03; Test Loss: 7.4900e+02. (Time: 5.5s)Step 38000 of 1000000; Loss: 7.4900e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.1120e+02; Test Loss: 7.4877e+02. (Time: 4.4s)Step 38500 of 1000000; Loss: 7.4878e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.8731e+01; Test Loss: 7.4867e+02. (Time: 4.4s)Step 39000 of 1000000; Loss: 7.4868e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.9882e+00; Test Loss: 7.4858e+02. (Time: 5.6s)Step 39500 of 1000000; Loss: 7.4859e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9836e+03; Test Loss: 7.4829e+02. (Time: 4.4s)Step 40000 of 1000000; Loss: 7.4828e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1488e+03; Test Loss: 7.4804e+02. (Time: 5.3s)Step 40500 of 1000000; Loss: 7.4804e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.0728e+02; Test Loss: 7.4801e+02. (Time: 4.7s)Step 41000 of 1000000; Loss: 7.4801e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.6799e+01; Test Loss: 7.4801e+02. (Time: 4.3s)Step 41500 of 1000000; Loss: 7.4803e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.7026e+00; Test Loss: 7.4795e+02. (Time: 5.5s)Step 42000 of 1000000; Loss: 7.4797e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9720e+03; Test Loss: 7.4802e+02. (Time: 4.4s)Step 42500 of 1000000; Loss: 7.4800e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1436e+03; Test Loss: 7.4797e+02. (Time: 4.5s)Step 43000 of 1000000; Loss: 7.4795e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.0329e+02; Test Loss: 7.4800e+02. (Time: 6.4s)Step 43500 of 1000000; Loss: 7.4800e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5337e+01; Test Loss: 7.4803e+02. (Time: 4.6s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.2704\n",
            "Fold 2 test Avg NLL: 0.2704\n",
            "\n",
            "=== Fold 3 ===\n",
            "Step 500 of 500; Loss: 3.4359e+03; Test Loss: 1.2785e+03. (Time: 5.9s)Step 500 of 1000000; Loss: 1.2799e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.6936e+03; Test Loss: 1.2506e+03. (Time: 4.6s)Step 1000 of 1000000; Loss: 1.2519e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.7417e+02; Test Loss: 1.2218e+03. (Time: 4.6s)Step 1500 of 1000000; Loss: 1.2232e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.3088e+02; Test Loss: 1.1923e+03. (Time: 5.6s)Step 2000 of 1000000; Loss: 1.1937e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1431e+03; Test Loss: 1.1634e+03. (Time: 4.5s)Step 2500 of 1000000; Loss: 1.1648e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.2023e+03; Test Loss: 1.1398e+03. (Time: 5.4s)Step 3000 of 1000000; Loss: 1.1408e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.4601e+03; Test Loss: 1.1196e+03. (Time: 4.5s)Step 3500 of 1000000; Loss: 1.1206e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.2579e+02; Test Loss: 1.1016e+03. (Time: 4.5s)Step 4000 of 1000000; Loss: 1.1024e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1087e+02; Test Loss: 1.0863e+03. (Time: 5.6s)Step 4500 of 1000000; Loss: 1.0870e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.8811e+03; Test Loss: 1.0718e+03. (Time: 4.5s)Step 5000 of 1000000; Loss: 1.0725e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.0079e+03; Test Loss: 1.0574e+03. (Time: 4.9s)Step 5500 of 1000000; Loss: 1.0581e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.3281e+03; Test Loss: 1.0425e+03. (Time: 5.4s)Step 6000 of 1000000; Loss: 1.0432e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.2762e+02; Test Loss: 1.0273e+03. (Time: 4.5s)Step 6500 of 1000000; Loss: 1.0281e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.4182e+01; Test Loss: 1.0119e+03. (Time: 5.6s)Step 7000 of 1000000; Loss: 1.0126e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.6857e+03; Test Loss: 9.9656e+02. (Time: 4.5s)Step 7500 of 1000000; Loss: 9.9730e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.8439e+03; Test Loss: 9.8129e+02. (Time: 4.4s)Step 8000 of 1000000; Loss: 9.8205e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1949e+03; Test Loss: 9.6578e+02. (Time: 5.7s)Step 8500 of 1000000; Loss: 9.6657e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.1277e+02; Test Loss: 9.4996e+02. (Time: 4.5s)Step 9000 of 1000000; Loss: 9.5072e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.4472e+01; Test Loss: 9.3476e+02. (Time: 4.8s)Step 9500 of 1000000; Loss: 9.3548e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5072e+03; Test Loss: 9.2016e+02. (Time: 5.2s)Step 10000 of 1000000; Loss: 9.2087e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.6943e+03; Test Loss: 9.0574e+02. (Time: 4.5s)Step 10500 of 1000000; Loss: 9.0644e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0627e+03; Test Loss: 8.9144e+02. (Time: 5.6s)Step 11000 of 1000000; Loss: 8.9212e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.8745e+02; Test Loss: 8.7742e+02. (Time: 4.5s)Step 11500 of 1000000; Loss: 8.7811e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.3365e+01; Test Loss: 8.6332e+02. (Time: 4.6s)Step 12000 of 1000000; Loss: 8.6398e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3380e+03; Test Loss: 8.4982e+02. (Time: 5.7s)Step 12500 of 1000000; Loss: 8.5046e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5611e+03; Test Loss: 8.3722e+02. (Time: 4.5s)Step 13000 of 1000000; Loss: 8.3782e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.4365e+02; Test Loss: 8.2549e+02. (Time: 4.9s)Step 13500 of 1000000; Loss: 8.2605e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8741e+02; Test Loss: 8.1436e+02. (Time: 5.1s)Step 14000 of 1000000; Loss: 8.1492e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.0107e+01; Test Loss: 8.0288e+02. (Time: 4.7s)Step 14500 of 1000000; Loss: 8.0344e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1724e+03; Test Loss: 7.8646e+02. (Time: 5.5s)Step 15000 of 1000000; Loss: 7.8722e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.4484e+03; Test Loss: 7.7391e+02. (Time: 4.6s)Step 15500 of 1000000; Loss: 7.7447e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.3744e+02; Test Loss: 7.6365e+02. (Time: 4.5s)Step 16000 of 1000000; Loss: 7.6410e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1575e+02; Test Loss: 7.5584e+02. (Time: 5.7s)Step 16500 of 1000000; Loss: 7.5619e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.6402e+01; Test Loss: 7.4967e+02. (Time: 4.5s)Step 17000 of 1000000; Loss: 7.4991e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0649e+03; Test Loss: 7.4435e+02. (Time: 5.1s)Step 17500 of 1000000; Loss: 7.4459e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3923e+03; Test Loss: 7.3990e+02. (Time: 4.8s)Step 18000 of 1000000; Loss: 7.4010e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.9676e+02; Test Loss: 7.3642e+02. (Time: 4.4s)Step 18500 of 1000000; Loss: 7.3659e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.6627e+01; Test Loss: 7.3336e+02. (Time: 5.6s)Step 19000 of 1000000; Loss: 7.3350e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9155e+01; Test Loss: 7.3059e+02. (Time: 4.4s)Step 19500 of 1000000; Loss: 7.3070e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0254e+03; Test Loss: 7.2830e+02. (Time: 4.4s)Step 20000 of 1000000; Loss: 7.2840e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3677e+03; Test Loss: 7.2643e+02. (Time: 5.7s)Step 20500 of 1000000; Loss: 7.2653e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.7779e+02; Test Loss: 7.2488e+02. (Time: 4.5s)Step 21000 of 1000000; Loss: 7.2495e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.9752e+01; Test Loss: 7.2368e+02. (Time: 5.0s)Step 21500 of 1000000; Loss: 7.2374e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.4603e+01; Test Loss: 7.2254e+02. (Time: 4.9s)Step 22000 of 1000000; Loss: 7.2258e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0031e+03; Test Loss: 7.2163e+02. (Time: 4.5s)Step 22500 of 1000000; Loss: 7.2166e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3521e+03; Test Loss: 7.2060e+02. (Time: 6.3s)Step 23000 of 1000000; Loss: 7.2065e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.6768e+02; Test Loss: 7.1960e+02. (Time: 4.5s)Step 23500 of 1000000; Loss: 7.1965e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.0128e+01; Test Loss: 7.1855e+02. (Time: 4.5s)Step 24000 of 1000000; Loss: 7.1860e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1531e+01; Test Loss: 7.1766e+02. (Time: 5.5s)Step 24500 of 1000000; Loss: 7.1770e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9803e+03; Test Loss: 7.1665e+02. (Time: 4.5s)Step 25000 of 1000000; Loss: 7.1668e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3381e+03; Test Loss: 7.1582e+02. (Time: 5.7s)Step 25500 of 1000000; Loss: 7.1587e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.5935e+02; Test Loss: 7.1495e+02. (Time: 4.4s)Step 26000 of 1000000; Loss: 7.1500e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.3679e+01; Test Loss: 7.1422e+02. (Time: 4.5s)Step 26500 of 1000000; Loss: 7.1425e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.4999e+00; Test Loss: 7.1338e+02. (Time: 5.6s)Step 27000 of 1000000; Loss: 7.1342e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9641e+03; Test Loss: 7.1243e+02. (Time: 4.5s)Step 27500 of 1000000; Loss: 7.1244e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3239e+03; Test Loss: 7.1174e+02. (Time: 4.8s)Step 28000 of 1000000; Loss: 7.1179e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.5286e+02; Test Loss: 7.1098e+02. (Time: 5.4s)Step 28500 of 1000000; Loss: 7.1104e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.8704e+01; Test Loss: 7.1024e+02. (Time: 4.6s)Step 29000 of 1000000; Loss: 7.1028e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.6781e+00; Test Loss: 7.0964e+02. (Time: 6.0s)Step 29500 of 1000000; Loss: 7.0967e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9504e+03; Test Loss: 7.0897e+02. (Time: 4.7s)Step 30000 of 1000000; Loss: 7.0900e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3106e+03; Test Loss: 7.0838e+02. (Time: 4.7s)Step 30500 of 1000000; Loss: 7.0842e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.4782e+02; Test Loss: 7.0784e+02. (Time: 6.2s)Step 31000 of 1000000; Loss: 7.0787e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.4652e+01; Test Loss: 7.0727e+02. (Time: 4.9s)Step 31500 of 1000000; Loss: 7.0730e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.6770e+00; Test Loss: 7.0670e+02. (Time: 6.0s)Step 32000 of 1000000; Loss: 7.0672e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9399e+03; Test Loss: 7.0615e+02. (Time: 4.6s)Step 32500 of 1000000; Loss: 7.0616e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2994e+03; Test Loss: 7.0563e+02. (Time: 4.6s)Step 33000 of 1000000; Loss: 7.0567e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.4415e+02; Test Loss: 7.0520e+02. (Time: 5.9s)Step 33500 of 1000000; Loss: 7.0523e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.1877e+01; Test Loss: 7.0483e+02. (Time: 4.6s)Step 34000 of 1000000; Loss: 7.0485e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.1944e+00; Test Loss: 7.0439e+02. (Time: 5.7s)Step 34500 of 1000000; Loss: 7.0440e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9309e+03; Test Loss: 7.0399e+02. (Time: 4.9s)Step 35000 of 1000000; Loss: 7.0399e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2893e+03; Test Loss: 7.0362e+02. (Time: 4.7s)Step 35500 of 1000000; Loss: 7.0365e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.4072e+02; Test Loss: 7.0328e+02. (Time: 5.8s)Step 36000 of 1000000; Loss: 7.0331e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.9841e+01; Test Loss: 7.0296e+02. (Time: 4.7s)Step 36500 of 1000000; Loss: 7.0297e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.9158e+00; Test Loss: 7.0262e+02. (Time: 5.5s)Step 37000 of 1000000; Loss: 7.0263e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9224e+03; Test Loss: 7.0232e+02. (Time: 4.9s)Step 37500 of 1000000; Loss: 7.0232e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2808e+03; Test Loss: 7.0209e+02. (Time: 4.7s)Step 38000 of 1000000; Loss: 7.0211e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.3808e+02; Test Loss: 7.0193e+02. (Time: 5.9s)Step 38500 of 1000000; Loss: 7.0194e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.8197e+01; Test Loss: 7.0173e+02. (Time: 4.6s)Step 39000 of 1000000; Loss: 7.0174e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.7096e+00; Test Loss: 7.0155e+02. (Time: 4.7s)Step 39500 of 1000000; Loss: 7.0157e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9146e+03; Test Loss: 7.0133e+02. (Time: 5.5s)Step 40000 of 1000000; Loss: 7.0133e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2732e+03; Test Loss: 7.0107e+02. (Time: 4.5s)Step 40500 of 1000000; Loss: 7.0109e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.3541e+02; Test Loss: 7.0080e+02. (Time: 5.9s)Step 41000 of 1000000; Loss: 7.0082e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.6470e+01; Test Loss: 7.0059e+02. (Time: 4.6s)Step 41500 of 1000000; Loss: 7.0060e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.5145e+00; Test Loss: 7.0037e+02. (Time: 4.9s)Step 42000 of 1000000; Loss: 7.0038e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9079e+03; Test Loss: 7.0013e+02. (Time: 5.8s)Step 42500 of 1000000; Loss: 7.0013e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2659e+03; Test Loss: 6.9991e+02. (Time: 4.5s)Step 43000 of 1000000; Loss: 6.9991e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.3279e+02; Test Loss: 6.9965e+02. (Time: 5.5s)Step 43500 of 1000000; Loss: 6.9966e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.5185e+01; Test Loss: 6.9937e+02. (Time: 4.6s)Step 44000 of 1000000; Loss: 6.9939e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.3726e+00; Test Loss: 6.9926e+02. (Time: 4.5s)Step 44500 of 1000000; Loss: 6.9926e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9005e+03; Test Loss: 6.9906e+02. (Time: 5.7s)Step 45000 of 1000000; Loss: 6.9907e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2585e+03; Test Loss: 6.9885e+02. (Time: 4.4s)Step 45500 of 1000000; Loss: 6.9885e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.3011e+02; Test Loss: 6.9851e+02. (Time: 4.7s)Step 46000 of 1000000; Loss: 6.9852e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.3840e+01; Test Loss: 6.9826e+02. (Time: 5.2s)Step 46500 of 1000000; Loss: 6.9827e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.2211e+00; Test Loss: 6.9808e+02. (Time: 4.4s)Step 47000 of 1000000; Loss: 6.9809e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8943e+03; Test Loss: 6.9786e+02. (Time: 5.5s)Step 47500 of 1000000; Loss: 6.9790e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2522e+03; Test Loss: 6.9770e+02. (Time: 4.5s)Step 48000 of 1000000; Loss: 6.9768e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.2735e+02; Test Loss: 6.9743e+02. (Time: 5.5s)Step 48500 of 1000000; Loss: 6.9743e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.2458e+01; Test Loss: 6.9728e+02. (Time: 5.7s)Step 49000 of 1000000; Loss: 6.9729e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.0857e+00; Test Loss: 6.9708e+02. (Time: 4.6s)Step 49500 of 1000000; Loss: 6.9710e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8888e+03; Test Loss: 6.9692e+02. (Time: 5.8s)Step 50000 of 1000000; Loss: 6.9693e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2458e+03; Test Loss: 6.9669e+02. (Time: 4.5s)Step 50500 of 1000000; Loss: 6.9667e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.2451e+02; Test Loss: 6.9639e+02. (Time: 4.6s)Step 51000 of 1000000; Loss: 6.9640e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1216e+01; Test Loss: 6.9609e+02. (Time: 5.7s)Step 51500 of 1000000; Loss: 6.9611e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.9721e+00; Test Loss: 6.9574e+02. (Time: 4.7s)Step 52000 of 1000000; Loss: 6.9577e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8838e+03; Test Loss: 6.9552e+02. (Time: 5.1s)Step 52500 of 1000000; Loss: 6.9549e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2399e+03; Test Loss: 6.9530e+02. (Time: 5.2s)Step 53000 of 1000000; Loss: 6.9529e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.2262e+02; Test Loss: 6.9513e+02. (Time: 4.6s)Step 53500 of 1000000; Loss: 6.9513e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.0032e+01; Test Loss: 6.9501e+02. (Time: 5.6s)Step 54000 of 1000000; Loss: 6.9502e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.8578e+00; Test Loss: 6.9483e+02. (Time: 4.7s)Step 54500 of 1000000; Loss: 6.9489e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8793e+03; Test Loss: 6.9471e+02. (Time: 4.6s)Step 55000 of 1000000; Loss: 6.9472e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2347e+03; Test Loss: 6.9457e+02. (Time: 5.9s)Step 55500 of 1000000; Loss: 6.9455e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.2057e+02; Test Loss: 6.9435e+02. (Time: 4.7s)Step 56000 of 1000000; Loss: 6.9436e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.8966e+01; Test Loss: 6.9412e+02. (Time: 5.7s)Step 56500 of 1000000; Loss: 6.9415e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.7557e+00; Test Loss: 6.9395e+02. (Time: 4.8s)Step 57000 of 1000000; Loss: 6.9397e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8757e+03; Test Loss: 6.9375e+02. (Time: 4.7s)Step 57500 of 1000000; Loss: 6.9377e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2297e+03; Test Loss: 6.9355e+02. (Time: 6.0s)Step 58000 of 1000000; Loss: 6.9354e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.1887e+02; Test Loss: 6.9335e+02. (Time: 5.2s)Step 58500 of 1000000; Loss: 6.9335e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.7930e+01; Test Loss: 6.9321e+02. (Time: 6.5s)Step 59000 of 1000000; Loss: 6.9323e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.6615e+00; Test Loss: 6.9308e+02. (Time: 4.8s)Step 59500 of 1000000; Loss: 6.9309e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8725e+03; Test Loss: 6.9292e+02. (Time: 5.3s)Step 60000 of 1000000; Loss: 6.9293e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2251e+03; Test Loss: 6.9284e+02. (Time: 5.7s)Step 60500 of 1000000; Loss: 6.9283e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.1723e+02; Test Loss: 6.9274e+02. (Time: 4.7s)Step 61000 of 1000000; Loss: 6.9274e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.7146e+01; Test Loss: 6.9264e+02. (Time: 6.1s)Step 61500 of 1000000; Loss: 6.9264e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.5788e+00; Test Loss: 6.9258e+02. (Time: 4.7s)Step 62000 of 1000000; Loss: 6.9260e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8695e+03; Test Loss: 6.9252e+02. (Time: 4.6s)Step 62500 of 1000000; Loss: 6.9253e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2211e+03; Test Loss: 6.9248e+02. (Time: 5.6s)Step 63000 of 1000000; Loss: 6.9248e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.1612e+02; Test Loss: 6.9244e+02. (Time: 4.5s)Step 63500 of 1000000; Loss: 6.9244e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.6507e+01; Test Loss: 6.9236e+02. (Time: 5.4s)Step 64000 of 1000000; Loss: 6.9237e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.5037e+00; Test Loss: 6.9226e+02. (Time: 4.9s)Step 64500 of 1000000; Loss: 6.9226e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8666e+03; Test Loss: 6.9216e+02. (Time: 4.6s)Step 65000 of 1000000; Loss: 6.9216e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2172e+03; Test Loss: 6.9208e+02. (Time: 5.7s)Step 65500 of 1000000; Loss: 6.9207e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.1522e+02; Test Loss: 6.9204e+02. (Time: 4.6s)Step 66000 of 1000000; Loss: 6.9203e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5961e+01; Test Loss: 6.9202e+02. (Time: 5.3s)Step 66500 of 1000000; Loss: 6.9203e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.4487e+00; Test Loss: 6.9201e+02. (Time: 5.0s)Step 67000 of 1000000; Loss: 6.9202e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8634e+03; Test Loss: 6.9198e+02. (Time: 4.4s)Step 67500 of 1000000; Loss: 6.9198e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2135e+03; Test Loss: 6.9193e+02. (Time: 5.6s)Step 68000 of 1000000; Loss: 6.9192e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.1448e+02; Test Loss: 6.9193e+02. (Time: 4.5s)Step 68500 of 1000000; Loss: 6.9192e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5568e+01; Test Loss: 6.9199e+02. (Time: 4.5s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.2952\n",
            "Fold 3 test Avg NLL: 0.2952\n",
            "\n",
            "=== Fold 4 ===\n",
            "Step 500 of 500; Loss: 3.4368e+03; Test Loss: 1.2163e+03. (Time: 5.7s)Step 500 of 1000000; Loss: 1.2178e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8218e+03; Test Loss: 1.1869e+03. (Time: 4.4s)Step 1000 of 1000000; Loss: 1.1883e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.6726e+02; Test Loss: 1.1575e+03. (Time: 5.3s)Step 1500 of 1000000; Loss: 1.1590e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.4133e+02; Test Loss: 1.1279e+03. (Time: 4.7s)Step 2000 of 1000000; Loss: 1.1294e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1238e+03; Test Loss: 1.0988e+03. (Time: 4.5s)Step 2500 of 1000000; Loss: 1.1002e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1599e+03; Test Loss: 1.0754e+03. (Time: 5.6s)Step 3000 of 1000000; Loss: 1.0764e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.6089e+03; Test Loss: 1.0547e+03. (Time: 4.5s)Step 3500 of 1000000; Loss: 1.0557e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.2072e+02; Test Loss: 1.0371e+03. (Time: 4.7s)Step 4000 of 1000000; Loss: 1.0379e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1754e+02; Test Loss: 1.0214e+03. (Time: 5.6s)Step 4500 of 1000000; Loss: 1.0221e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.8559e+03; Test Loss: 1.0061e+03. (Time: 4.6s)Step 5000 of 1000000; Loss: 1.0069e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.9536e+03; Test Loss: 9.9119e+02. (Time: 5.6s)Step 5500 of 1000000; Loss: 9.9192e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.4820e+03; Test Loss: 9.7585e+02. (Time: 4.5s)Step 6000 of 1000000; Loss: 9.7662e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.2377e+02; Test Loss: 9.5986e+02. (Time: 5.4s)Step 6500 of 1000000; Loss: 9.6065e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.5879e+01; Test Loss: 9.4394e+02. (Time: 5.6s)Step 7000 of 1000000; Loss: 9.4471e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.6494e+03; Test Loss: 9.2819e+02. (Time: 4.6s)Step 7500 of 1000000; Loss: 9.2897e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.7787e+03; Test Loss: 9.1261e+02. (Time: 5.6s)Step 8000 of 1000000; Loss: 9.1337e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.3495e+03; Test Loss: 8.9763e+02. (Time: 4.6s)Step 8500 of 1000000; Loss: 8.9835e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.1309e+02; Test Loss: 8.8315e+02. (Time: 4.5s)Step 9000 of 1000000; Loss: 8.8387e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.1325e+01; Test Loss: 8.6832e+02. (Time: 5.6s)Step 9500 of 1000000; Loss: 8.6904e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.4674e+03; Test Loss: 8.5459e+02. (Time: 4.5s)Step 10000 of 1000000; Loss: 8.5524e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.6272e+03; Test Loss: 8.4155e+02. (Time: 4.8s)Step 10500 of 1000000; Loss: 8.4217e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2254e+03; Test Loss: 8.2890e+02. (Time: 5.6s)Step 11000 of 1000000; Loss: 8.2952e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.9897e+02; Test Loss: 8.1687e+02. (Time: 4.6s)Step 11500 of 1000000; Loss: 8.1745e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.9844e+01; Test Loss: 8.0559e+02. (Time: 5.7s)Step 12000 of 1000000; Loss: 8.0613e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3064e+03; Test Loss: 7.9498e+02. (Time: 4.5s)Step 12500 of 1000000; Loss: 7.9548e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.4949e+03; Test Loss: 7.8485e+02. (Time: 4.6s)Step 13000 of 1000000; Loss: 7.8534e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1373e+03; Test Loss: 7.7517e+02. (Time: 5.7s)Step 13500 of 1000000; Loss: 7.7563e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0977e+02; Test Loss: 7.6628e+02. (Time: 4.5s)Step 14000 of 1000000; Loss: 7.6673e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.5051e+01; Test Loss: 7.5723e+02. (Time: 5.1s)Step 14500 of 1000000; Loss: 7.5765e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1755e+03; Test Loss: 7.4835e+02. (Time: 5.1s)Step 15000 of 1000000; Loss: 7.4878e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3815e+03; Test Loss: 7.3983e+02. (Time: 4.5s)Step 15500 of 1000000; Loss: 7.4022e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0585e+03; Test Loss: 7.3180e+02. (Time: 5.6s)Step 16000 of 1000000; Loss: 7.3220e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2023e+02; Test Loss: 7.2341e+02. (Time: 4.6s)Step 16500 of 1000000; Loss: 7.2384e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0667e+01; Test Loss: 7.1504e+02. (Time: 4.5s)Step 17000 of 1000000; Loss: 7.1531e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0602e+03; Test Loss: 7.1022e+02. (Time: 5.6s)Step 17500 of 1000000; Loss: 7.1043e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2975e+03; Test Loss: 7.0634e+02. (Time: 4.4s)Step 18000 of 1000000; Loss: 7.0652e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.9902e+02; Test Loss: 7.0293e+02. (Time: 5.2s)Step 18500 of 1000000; Loss: 7.0309e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.0148e+01; Test Loss: 6.9966e+02. (Time: 4.7s)Step 19000 of 1000000; Loss: 6.9981e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.6348e+01; Test Loss: 6.9678e+02. (Time: 4.5s)Step 19500 of 1000000; Loss: 6.9692e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0028e+03; Test Loss: 6.9422e+02. (Time: 5.6s)Step 20000 of 1000000; Loss: 6.9432e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2649e+03; Test Loss: 6.9232e+02. (Time: 4.4s)Step 20500 of 1000000; Loss: 6.9242e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.7149e+02; Test Loss: 6.9009e+02. (Time: 4.4s)Step 21000 of 1000000; Loss: 6.9020e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.5654e+01; Test Loss: 6.8744e+02. (Time: 5.6s)Step 21500 of 1000000; Loss: 6.8755e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.4132e+01; Test Loss: 6.8549e+02. (Time: 4.5s)Step 22000 of 1000000; Loss: 6.8558e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9738e+03; Test Loss: 6.8403e+02. (Time: 5.3s)Step 22500 of 1000000; Loss: 6.8409e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2473e+03; Test Loss: 6.8273e+02. (Time: 4.6s)Step 23000 of 1000000; Loss: 6.8279e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.5636e+02; Test Loss: 6.8165e+02. (Time: 4.4s)Step 23500 of 1000000; Loss: 6.8169e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.5712e+01; Test Loss: 6.8061e+02. (Time: 5.5s)Step 24000 of 1000000; Loss: 6.8066e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2743e+01; Test Loss: 6.7966e+02. (Time: 4.4s)Step 24500 of 1000000; Loss: 6.7972e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9559e+03; Test Loss: 6.7869e+02. (Time: 4.5s)Step 25000 of 1000000; Loss: 6.7873e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2384e+03; Test Loss: 6.7780e+02. (Time: 5.8s)Step 25500 of 1000000; Loss: 6.7784e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.4770e+02; Test Loss: 6.7696e+02. (Time: 4.4s)Step 26000 of 1000000; Loss: 6.7700e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.8915e+01; Test Loss: 6.7615e+02. (Time: 5.4s)Step 26500 of 1000000; Loss: 6.7619e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1779e+01; Test Loss: 6.7547e+02. (Time: 4.5s)Step 27000 of 1000000; Loss: 6.7551e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9425e+03; Test Loss: 6.7475e+02. (Time: 4.4s)Step 27500 of 1000000; Loss: 6.7477e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2309e+03; Test Loss: 6.7409e+02. (Time: 5.5s)Step 28000 of 1000000; Loss: 6.7411e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.4159e+02; Test Loss: 6.7341e+02. (Time: 4.5s)Step 28500 of 1000000; Loss: 6.7344e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.4162e+01; Test Loss: 6.7265e+02. (Time: 4.6s)Step 29000 of 1000000; Loss: 6.7269e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1168e+01; Test Loss: 6.7197e+02. (Time: 5.5s)Step 29500 of 1000000; Loss: 6.7200e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9312e+03; Test Loss: 6.7128e+02. (Time: 4.4s)Step 30000 of 1000000; Loss: 6.7131e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2238e+03; Test Loss: 6.7061e+02. (Time: 5.4s)Step 30500 of 1000000; Loss: 6.7064e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.3616e+02; Test Loss: 6.6995e+02. (Time: 4.5s)Step 31000 of 1000000; Loss: 6.6998e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.0377e+01; Test Loss: 6.6933e+02. (Time: 4.8s)Step 31500 of 1000000; Loss: 6.6937e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0617e+01; Test Loss: 6.6868e+02. (Time: 5.7s)Step 32000 of 1000000; Loss: 6.6872e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9209e+03; Test Loss: 6.6800e+02. (Time: 4.6s)Step 32500 of 1000000; Loss: 6.6803e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2174e+03; Test Loss: 6.6732e+02. (Time: 4.8s)Step 33000 of 1000000; Loss: 6.6735e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.3206e+02; Test Loss: 6.6670e+02. (Time: 5.3s)Step 33500 of 1000000; Loss: 6.6672e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.7502e+01; Test Loss: 6.6605e+02. (Time: 5.4s)Step 34000 of 1000000; Loss: 6.6609e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0188e+01; Test Loss: 6.6537e+02. (Time: 5.5s)Step 34500 of 1000000; Loss: 6.6541e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9119e+03; Test Loss: 6.6478e+02. (Time: 4.5s)Step 35000 of 1000000; Loss: 6.6480e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2114e+03; Test Loss: 6.6426e+02. (Time: 4.7s)Step 35500 of 1000000; Loss: 6.6428e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.2848e+02; Test Loss: 6.6375e+02. (Time: 5.5s)Step 36000 of 1000000; Loss: 6.6377e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.5100e+01; Test Loss: 6.6315e+02. (Time: 4.4s)Step 36500 of 1000000; Loss: 6.6319e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.8031e+00; Test Loss: 6.6264e+02. (Time: 5.6s)Step 37000 of 1000000; Loss: 6.6267e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9032e+03; Test Loss: 6.6215e+02. (Time: 4.7s)Step 37500 of 1000000; Loss: 6.6216e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2058e+03; Test Loss: 6.6165e+02. (Time: 4.6s)Step 38000 of 1000000; Loss: 6.6167e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.2494e+02; Test Loss: 6.6109e+02. (Time: 5.6s)Step 38500 of 1000000; Loss: 6.6111e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.3091e+01; Test Loss: 6.6051e+02. (Time: 4.5s)Step 39000 of 1000000; Loss: 6.6054e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.4173e+00; Test Loss: 6.5989e+02. (Time: 4.9s)Step 39500 of 1000000; Loss: 6.5993e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8957e+03; Test Loss: 6.5943e+02. (Time: 5.4s)Step 40000 of 1000000; Loss: 6.5946e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2003e+03; Test Loss: 6.5901e+02. (Time: 4.6s)Step 40500 of 1000000; Loss: 6.5904e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.2175e+02; Test Loss: 6.5849e+02. (Time: 5.7s)Step 41000 of 1000000; Loss: 6.5850e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1250e+01; Test Loss: 6.5799e+02. (Time: 4.5s)Step 41500 of 1000000; Loss: 6.5802e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.1031e+00; Test Loss: 6.5746e+02. (Time: 4.5s)Step 42000 of 1000000; Loss: 6.5751e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8886e+03; Test Loss: 6.5707e+02. (Time: 5.7s)Step 42500 of 1000000; Loss: 6.5708e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1948e+03; Test Loss: 6.5670e+02. (Time: 4.4s)Step 43000 of 1000000; Loss: 6.5670e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.1850e+02; Test Loss: 6.5631e+02. (Time: 5.4s)Step 43500 of 1000000; Loss: 6.5634e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.9771e+01; Test Loss: 6.5595e+02. (Time: 5.1s)Step 44000 of 1000000; Loss: 6.5597e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.8423e+00; Test Loss: 6.5558e+02. (Time: 4.7s)Step 44500 of 1000000; Loss: 6.5560e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8816e+03; Test Loss: 6.5520e+02. (Time: 5.6s)Step 45000 of 1000000; Loss: 6.5521e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1887e+03; Test Loss: 6.5482e+02. (Time: 4.5s)Step 45500 of 1000000; Loss: 6.5483e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.1584e+02; Test Loss: 6.5462e+02. (Time: 5.0s)Step 46000 of 1000000; Loss: 6.5464e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.8578e+01; Test Loss: 6.5434e+02. (Time: 5.3s)Step 46500 of 1000000; Loss: 6.5434e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.6334e+00; Test Loss: 6.5405e+02. (Time: 4.5s)Step 47000 of 1000000; Loss: 6.5406e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8751e+03; Test Loss: 6.5377e+02. (Time: 5.6s)Step 47500 of 1000000; Loss: 6.5380e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1831e+03; Test Loss: 6.5352e+02. (Time: 4.5s)Step 48000 of 1000000; Loss: 6.5355e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.1366e+02; Test Loss: 6.5318e+02. (Time: 4.6s)Step 48500 of 1000000; Loss: 6.5320e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.7519e+01; Test Loss: 6.5291e+02. (Time: 5.7s)Step 49000 of 1000000; Loss: 6.5293e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.4376e+00; Test Loss: 6.5270e+02. (Time: 4.6s)Step 49500 of 1000000; Loss: 6.5273e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8689e+03; Test Loss: 6.5244e+02. (Time: 5.2s)Step 50000 of 1000000; Loss: 6.5244e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1780e+03; Test Loss: 6.5219e+02. (Time: 5.0s)Step 50500 of 1000000; Loss: 6.5221e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.1145e+02; Test Loss: 6.5193e+02. (Time: 4.5s)Step 51000 of 1000000; Loss: 6.5196e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.6533e+01; Test Loss: 6.5163e+02. (Time: 5.8s)Step 51500 of 1000000; Loss: 6.5167e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.2405e+00; Test Loss: 6.5145e+02. (Time: 4.8s)Step 52000 of 1000000; Loss: 6.5144e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8631e+03; Test Loss: 6.5123e+02. (Time: 5.0s)Step 52500 of 1000000; Loss: 6.5122e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1734e+03; Test Loss: 6.5096e+02. (Time: 5.6s)Step 53000 of 1000000; Loss: 6.5097e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.0932e+02; Test Loss: 6.5069e+02. (Time: 4.6s)Step 53500 of 1000000; Loss: 6.5070e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5698e+01; Test Loss: 6.5048e+02. (Time: 5.7s)Step 54000 of 1000000; Loss: 6.5049e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.0785e+00; Test Loss: 6.5012e+02. (Time: 4.7s)Step 54500 of 1000000; Loss: 6.5013e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8577e+03; Test Loss: 6.4982e+02. (Time: 4.8s)Step 55000 of 1000000; Loss: 6.4982e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1686e+03; Test Loss: 6.4961e+02. (Time: 6.2s)Step 55500 of 1000000; Loss: 6.4964e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.0672e+02; Test Loss: 6.4920e+02. (Time: 4.6s)Step 56000 of 1000000; Loss: 6.4924e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.4781e+01; Test Loss: 6.4890e+02. (Time: 5.8s)Step 56500 of 1000000; Loss: 6.4892e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.8800e+00; Test Loss: 6.4863e+02. (Time: 4.7s)Step 57000 of 1000000; Loss: 6.4863e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8524e+03; Test Loss: 6.4837e+02. (Time: 4.8s)Step 57500 of 1000000; Loss: 6.4836e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1631e+03; Test Loss: 6.4808e+02. (Time: 5.8s)Step 58000 of 1000000; Loss: 6.4811e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.0439e+02; Test Loss: 6.4787e+02. (Time: 4.8s)Step 58500 of 1000000; Loss: 6.4787e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3956e+01; Test Loss: 6.4777e+02. (Time: 5.8s)Step 59000 of 1000000; Loss: 6.4777e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.7312e+00; Test Loss: 6.4768e+02. (Time: 4.6s)Step 59500 of 1000000; Loss: 6.4765e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8469e+03; Test Loss: 6.4750e+02. (Time: 4.6s)Step 60000 of 1000000; Loss: 6.4750e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1584e+03; Test Loss: 6.4739e+02. (Time: 5.9s)Step 60500 of 1000000; Loss: 6.4738e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.0246e+02; Test Loss: 6.4720e+02. (Time: 5.6s)Step 61000 of 1000000; Loss: 6.4722e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3197e+01; Test Loss: 6.4700e+02. (Time: 6.7s)Step 61500 of 1000000; Loss: 6.4703e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.5509e+00; Test Loss: 6.4685e+02. (Time: 5.0s)Step 62000 of 1000000; Loss: 6.4687e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8419e+03; Test Loss: 6.4680e+02. (Time: 5.1s)Step 62500 of 1000000; Loss: 6.4679e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1539e+03; Test Loss: 6.4663e+02. (Time: 5.3s)Step 63000 of 1000000; Loss: 6.4667e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.9996e+02; Test Loss: 6.4648e+02. (Time: 5.7s)Step 63500 of 1000000; Loss: 6.4649e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2473e+01; Test Loss: 6.4631e+02. (Time: 6.1s)Step 64000 of 1000000; Loss: 6.4634e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.3918e+00; Test Loss: 6.4607e+02. (Time: 5.1s)Step 64500 of 1000000; Loss: 6.4607e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8374e+03; Test Loss: 6.4578e+02. (Time: 6.0s)Step 65000 of 1000000; Loss: 6.4580e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1488e+03; Test Loss: 6.4557e+02. (Time: 4.6s)Step 65500 of 1000000; Loss: 6.4560e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.9818e+02; Test Loss: 6.4550e+02. (Time: 4.6s)Step 66000 of 1000000; Loss: 6.4551e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1721e+01; Test Loss: 6.4544e+02. (Time: 5.9s)Step 66500 of 1000000; Loss: 6.4545e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.2519e+00; Test Loss: 6.4525e+02. (Time: 4.8s)Step 67000 of 1000000; Loss: 6.4523e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8330e+03; Test Loss: 6.4520e+02. (Time: 5.5s)Step 67500 of 1000000; Loss: 6.4518e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1442e+03; Test Loss: 6.4508e+02. (Time: 4.9s)Step 68000 of 1000000; Loss: 6.4510e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.9575e+02; Test Loss: 6.4504e+02. (Time: 4.6s)Step 68500 of 1000000; Loss: 6.4504e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1156e+01; Test Loss: 6.4499e+02. (Time: 5.8s)Step 69000 of 1000000; Loss: 6.4500e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.1317e+00; Test Loss: 6.4506e+02. (Time: 4.8s)Step 69500 of 1000000; Loss: 6.4501e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8294e+03; Test Loss: 6.4494e+02. (Time: 5.6s)Step 70000 of 1000000; Loss: 6.4495e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1401e+03; Test Loss: 6.4489e+02. (Time: 5.3s)Step 70500 of 1000000; Loss: 6.4490e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.9401e+02; Test Loss: 6.4486e+02. (Time: 5.2s)Step 71000 of 1000000; Loss: 6.4487e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0566e+01; Test Loss: 6.4502e+02. (Time: 5.9s)Step 71500 of 1000000; Loss: 6.4502e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.9717e+00; Test Loss: 6.4505e+02. (Time: 5.0s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3166\n",
            "Fold 4 test Avg NLL: 0.3166\n",
            "\n",
            "=== Fold 5 ===\n",
            "Step 500 of 500; Loss: 3.3952e+03; Test Loss: 1.2295e+03. (Time: 5.8s)Step 500 of 1000000; Loss: 1.2309e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8122e+03; Test Loss: 1.2026e+03. (Time: 4.6s)Step 1000 of 1000000; Loss: 1.2039e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.8854e+02; Test Loss: 1.1760e+03. (Time: 4.6s)Step 1500 of 1000000; Loss: 1.1773e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.6135e+02; Test Loss: 1.1502e+03. (Time: 6.0s)Step 2000 of 1000000; Loss: 1.1514e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1841e+03; Test Loss: 1.1250e+03. (Time: 4.9s)Step 2500 of 1000000; Loss: 1.1262e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1028e+03; Test Loss: 1.1035e+03. (Time: 5.6s)Step 3000 of 1000000; Loss: 1.1044e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.5971e+03; Test Loss: 1.0848e+03. (Time: 5.1s)Step 3500 of 1000000; Loss: 1.0857e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.4160e+02; Test Loss: 1.0680e+03. (Time: 4.5s)Step 4000 of 1000000; Loss: 1.0687e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.3785e+02; Test Loss: 1.0539e+03. (Time: 5.8s)Step 4500 of 1000000; Loss: 1.0545e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.9189e+03; Test Loss: 1.0406e+03. (Time: 4.6s)Step 5000 of 1000000; Loss: 1.0412e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.8750e+03; Test Loss: 1.0276e+03. (Time: 5.0s)Step 5500 of 1000000; Loss: 1.0282e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.4746e+03; Test Loss: 1.0143e+03. (Time: 5.4s)Step 6000 of 1000000; Loss: 1.0150e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.4414e+02; Test Loss: 1.0003e+03. (Time: 4.6s)Step 6500 of 1000000; Loss: 1.0010e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1858e+02; Test Loss: 9.8608e+02. (Time: 5.7s)Step 7000 of 1000000; Loss: 9.8676e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.7102e+03; Test Loss: 9.7209e+02. (Time: 4.5s)Step 7500 of 1000000; Loss: 9.7279e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.6839e+03; Test Loss: 9.5771e+02. (Time: 4.5s)Step 8000 of 1000000; Loss: 9.5842e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.3459e+03; Test Loss: 9.4372e+02. (Time: 5.8s)Step 8500 of 1000000; Loss: 9.4441e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.3675e+02; Test Loss: 9.2994e+02. (Time: 4.6s)Step 9000 of 1000000; Loss: 9.3061e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.7474e+01; Test Loss: 9.1646e+02. (Time: 5.4s)Step 9500 of 1000000; Loss: 9.1713e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5177e+03; Test Loss: 9.0303e+02. (Time: 4.7s)Step 10000 of 1000000; Loss: 9.0367e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5114e+03; Test Loss: 8.9049e+02. (Time: 4.4s)Step 10500 of 1000000; Loss: 8.9109e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2236e+03; Test Loss: 8.7874e+02. (Time: 5.6s)Step 11000 of 1000000; Loss: 8.7930e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.2379e+02; Test Loss: 8.6746e+02. (Time: 4.5s)Step 11500 of 1000000; Loss: 8.6801e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.5208e+01; Test Loss: 8.5663e+02. (Time: 4.7s)Step 12000 of 1000000; Loss: 8.5715e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3599e+03; Test Loss: 8.4582e+02. (Time: 5.6s)Step 12500 of 1000000; Loss: 8.4635e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3716e+03; Test Loss: 8.3519e+02. (Time: 4.5s)Step 13000 of 1000000; Loss: 8.3571e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1257e+03; Test Loss: 8.2484e+02. (Time: 5.7s)Step 13500 of 1000000; Loss: 8.2535e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2189e+02; Test Loss: 8.1464e+02. (Time: 4.7s)Step 14000 of 1000000; Loss: 8.1513e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.4750e+01; Test Loss: 8.0489e+02. (Time: 4.8s)Step 14500 of 1000000; Loss: 8.0535e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2277e+03; Test Loss: 7.9563e+02. (Time: 5.9s)Step 15000 of 1000000; Loss: 7.9607e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2482e+03; Test Loss: 7.8730e+02. (Time: 4.5s)Step 15500 of 1000000; Loss: 7.8768e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0464e+03; Test Loss: 7.8027e+02. (Time: 5.4s)Step 16000 of 1000000; Loss: 7.8059e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.4285e+02; Test Loss: 7.7403e+02. (Time: 4.7s)Step 16500 of 1000000; Loss: 7.7432e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.4599e+01; Test Loss: 7.6796e+02. (Time: 4.5s)Step 17000 of 1000000; Loss: 7.6826e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1199e+03; Test Loss: 7.6271e+02. (Time: 5.6s)Step 17500 of 1000000; Loss: 7.6294e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1660e+03; Test Loss: 7.5866e+02. (Time: 4.7s)Step 18000 of 1000000; Loss: 7.5881e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0020e+03; Test Loss: 7.5456e+02. (Time: 4.7s)Step 18500 of 1000000; Loss: 7.5473e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.2196e+01; Test Loss: 7.5206e+02. (Time: 5.5s)Step 19000 of 1000000; Loss: 7.5218e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9811e+01; Test Loss: 7.4991e+02. (Time: 4.5s)Step 19500 of 1000000; Loss: 7.5001e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0702e+03; Test Loss: 7.4819e+02. (Time: 5.6s)Step 20000 of 1000000; Loss: 7.4828e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1342e+03; Test Loss: 7.4671e+02. (Time: 4.5s)Step 20500 of 1000000; Loss: 7.4677e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.8856e+02; Test Loss: 7.4537e+02. (Time: 4.5s)Step 21000 of 1000000; Loss: 7.4543e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.0789e+01; Test Loss: 7.4418e+02. (Time: 5.6s)Step 21500 of 1000000; Loss: 7.4424e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.5975e+01; Test Loss: 7.4302e+02. (Time: 5.6s)Step 22000 of 1000000; Loss: 7.4308e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0453e+03; Test Loss: 7.4178e+02. (Time: 5.6s)Step 22500 of 1000000; Loss: 7.4184e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1187e+03; Test Loss: 7.4036e+02. (Time: 4.6s)Step 23000 of 1000000; Loss: 7.4043e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.7985e+02; Test Loss: 7.3913e+02. (Time: 4.5s)Step 23500 of 1000000; Loss: 7.3918e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.5858e+01; Test Loss: 7.3818e+02. (Time: 5.7s)Step 24000 of 1000000; Loss: 7.3822e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.3302e+01; Test Loss: 7.3760e+02. (Time: 4.5s)Step 24500 of 1000000; Loss: 7.3762e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0242e+03; Test Loss: 7.3707e+02. (Time: 5.0s)Step 25000 of 1000000; Loss: 7.3710e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1081e+03; Test Loss: 7.3672e+02. (Time: 5.4s)Step 25500 of 1000000; Loss: 7.3674e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.7281e+02; Test Loss: 7.3640e+02. (Time: 4.5s)Step 26000 of 1000000; Loss: 7.3641e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.4805e+01; Test Loss: 7.3612e+02. (Time: 5.6s)Step 26500 of 1000000; Loss: 7.3614e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1675e+01; Test Loss: 7.3570e+02. (Time: 4.6s)Step 27000 of 1000000; Loss: 7.3572e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0092e+03; Test Loss: 7.3527e+02. (Time: 4.5s)Step 27500 of 1000000; Loss: 7.3530e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0974e+03; Test Loss: 7.3497e+02. (Time: 5.7s)Step 28000 of 1000000; Loss: 7.3498e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.6702e+02; Test Loss: 7.3465e+02. (Time: 4.5s)Step 28500 of 1000000; Loss: 7.3466e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.7644e+01; Test Loss: 7.3429e+02. (Time: 5.1s)Step 29000 of 1000000; Loss: 7.3431e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0539e+01; Test Loss: 7.3405e+02. (Time: 5.0s)Step 29500 of 1000000; Loss: 7.3407e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9934e+03; Test Loss: 7.3420e+02. (Time: 4.7s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3006\n",
            "Fold 5 test Avg NLL: 0.3006\n",
            "\n",
            "=== Fold 6 ===\n",
            "Step 500 of 500; Loss: 3.3708e+03; Test Loss: 1.1912e+03. (Time: 5.6s)Step 500 of 1000000; Loss: 1.1927e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8956e+03; Test Loss: 1.1635e+03. (Time: 4.5s)Step 1000 of 1000000; Loss: 1.1648e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.5908e+02; Test Loss: 1.1360e+03. (Time: 4.4s)Step 1500 of 1000000; Loss: 1.1374e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8376e+02; Test Loss: 1.1079e+03. (Time: 5.5s)Step 2000 of 1000000; Loss: 1.1093e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1785e+03; Test Loss: 1.0808e+03. (Time: 4.6s)Step 2500 of 1000000; Loss: 1.0821e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.0883e+03; Test Loss: 1.0587e+03. (Time: 5.7s)Step 3000 of 1000000; Loss: 1.0597e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.6750e+03; Test Loss: 1.0385e+03. (Time: 4.4s)Step 3500 of 1000000; Loss: 1.0394e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.1265e+02; Test Loss: 1.0211e+03. (Time: 4.5s)Step 4000 of 1000000; Loss: 1.0219e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.5531e+02; Test Loss: 1.0063e+03. (Time: 5.7s)Step 4500 of 1000000; Loss: 1.0070e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.9258e+03; Test Loss: 9.9194e+02. (Time: 4.6s)Step 5000 of 1000000; Loss: 9.9264e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.8754e+03; Test Loss: 9.7739e+02. (Time: 5.0s)Step 5500 of 1000000; Loss: 9.7813e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.5453e+03; Test Loss: 9.6213e+02. (Time: 5.3s)Step 6000 of 1000000; Loss: 9.6287e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.0871e+02; Test Loss: 9.4698e+02. (Time: 4.5s)Step 6500 of 1000000; Loss: 9.4771e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2971e+02; Test Loss: 9.3236e+02. (Time: 5.8s)Step 7000 of 1000000; Loss: 9.3306e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.7282e+03; Test Loss: 9.1820e+02. (Time: 4.5s)Step 7500 of 1000000; Loss: 9.1889e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.6893e+03; Test Loss: 9.0412e+02. (Time: 4.5s)Step 8000 of 1000000; Loss: 9.0481e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.4114e+03; Test Loss: 8.9012e+02. (Time: 5.7s)Step 8500 of 1000000; Loss: 8.9082e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.9322e+02; Test Loss: 8.7596e+02. (Time: 4.5s)Step 9000 of 1000000; Loss: 8.7665e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0233e+02; Test Loss: 8.6237e+02. (Time: 5.0s)Step 9500 of 1000000; Loss: 8.6302e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5436e+03; Test Loss: 8.4946e+02. (Time: 5.1s)Step 10000 of 1000000; Loss: 8.5010e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5197e+03; Test Loss: 8.3613e+02. (Time: 4.5s)Step 10500 of 1000000; Loss: 8.3678e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2824e+03; Test Loss: 8.2281e+02. (Time: 5.6s)Step 11000 of 1000000; Loss: 8.2346e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.7000e+02; Test Loss: 8.0985e+02. (Time: 4.5s)Step 11500 of 1000000; Loss: 8.1047e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.3963e+01; Test Loss: 7.9735e+02. (Time: 4.7s)Step 12000 of 1000000; Loss: 7.9796e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3748e+03; Test Loss: 7.8532e+02. (Time: 5.6s)Step 12500 of 1000000; Loss: 7.8591e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3706e+03; Test Loss: 7.7388e+02. (Time: 4.4s)Step 13000 of 1000000; Loss: 7.7441e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1794e+03; Test Loss: 7.6307e+02. (Time: 5.2s)Step 13500 of 1000000; Loss: 7.6357e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.7932e+02; Test Loss: 7.5290e+02. (Time: 5.0s)Step 14000 of 1000000; Loss: 7.5336e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.3025e+01; Test Loss: 7.4382e+02. (Time: 4.4s)Step 14500 of 1000000; Loss: 7.4424e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2492e+03; Test Loss: 7.3570e+02. (Time: 5.6s)Step 15000 of 1000000; Loss: 7.3609e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2724e+03; Test Loss: 7.2810e+02. (Time: 4.4s)Step 15500 of 1000000; Loss: 7.2844e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1198e+03; Test Loss: 7.2264e+02. (Time: 4.5s)Step 16000 of 1000000; Loss: 7.2290e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0797e+02; Test Loss: 7.1926e+02. (Time: 5.7s)Step 16500 of 1000000; Loss: 7.1937e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5027e+01; Test Loss: 7.0512e+02. (Time: 4.4s)Step 17000 of 1000000; Loss: 7.0689e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1728e+03; Test Loss: 6.9665e+02. (Time: 5.1s)Step 17500 of 1000000; Loss: 6.9683e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2153e+03; Test Loss: 6.9336e+02. (Time: 4.9s)Step 18000 of 1000000; Loss: 6.9350e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0809e+03; Test Loss: 6.9087e+02. (Time: 4.5s)Step 18500 of 1000000; Loss: 6.9098e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.8476e+01; Test Loss: 6.8869e+02. (Time: 5.6s)Step 19000 of 1000000; Loss: 6.8879e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.6761e+01; Test Loss: 6.8668e+02. (Time: 4.5s)Step 19500 of 1000000; Loss: 6.8678e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1411e+03; Test Loss: 6.8471e+02. (Time: 4.4s)Step 20000 of 1000000; Loss: 6.8481e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1934e+03; Test Loss: 6.8290e+02. (Time: 5.7s)Step 20500 of 1000000; Loss: 6.8297e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0617e+03; Test Loss: 6.8126e+02. (Time: 4.4s)Step 21000 of 1000000; Loss: 6.8133e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.3875e+01; Test Loss: 6.7972e+02. (Time: 5.3s)Step 21500 of 1000000; Loss: 6.7979e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.4325e+01; Test Loss: 6.7840e+02. (Time: 4.6s)Step 22000 of 1000000; Loss: 6.7847e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1193e+03; Test Loss: 6.7735e+02. (Time: 5.4s)Step 22500 of 1000000; Loss: 6.7741e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1795e+03; Test Loss: 6.7637e+02. (Time: 5.5s)Step 23000 of 1000000; Loss: 6.7641e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0514e+03; Test Loss: 6.7520e+02. (Time: 4.4s)Step 23500 of 1000000; Loss: 6.7526e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.4162e+01; Test Loss: 6.7532e+02. (Time: 5.0s)Step 24000 of 1000000; Loss: 6.7511e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2427e+01; Test Loss: 6.7240e+02. (Time: 5.0s)Step 24500 of 1000000; Loss: 6.7249e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0994e+03; Test Loss: 6.7126e+02. (Time: 4.4s)Step 25000 of 1000000; Loss: 6.7131e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1669e+03; Test Loss: 6.7050e+02. (Time: 5.6s)Step 25500 of 1000000; Loss: 6.7055e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0434e+03; Test Loss: 6.6972e+02. (Time: 4.4s)Step 26000 of 1000000; Loss: 6.6975e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.6922e+01; Test Loss: 6.6918e+02. (Time: 4.4s)Step 26500 of 1000000; Loss: 6.6919e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1205e+01; Test Loss: 6.6867e+02. (Time: 5.9s)Step 27000 of 1000000; Loss: 6.6867e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0860e+03; Test Loss: 6.6856e+02. (Time: 4.4s)Step 27500 of 1000000; Loss: 6.6885e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1590e+03; Test Loss: 6.6732e+02. (Time: 4.8s)Step 28000 of 1000000; Loss: 6.6736e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0371e+03; Test Loss: 6.6626e+02. (Time: 5.2s)Step 28500 of 1000000; Loss: 6.6630e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.2961e+01; Test Loss: 6.6544e+02. (Time: 4.4s)Step 29000 of 1000000; Loss: 6.6547e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0414e+01; Test Loss: 6.6482e+02. (Time: 5.5s)Step 29500 of 1000000; Loss: 6.6485e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0743e+03; Test Loss: 6.6431e+02. (Time: 4.5s)Step 30000 of 1000000; Loss: 6.6435e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1510e+03; Test Loss: 6.6358e+02. (Time: 4.5s)Step 30500 of 1000000; Loss: 6.6361e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0301e+03; Test Loss: 6.6278e+02. (Time: 5.5s)Step 31000 of 1000000; Loss: 6.6282e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.9855e+01; Test Loss: 6.6214e+02. (Time: 4.5s)Step 31500 of 1000000; Loss: 6.6220e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.8389e+00; Test Loss: 6.6152e+02. (Time: 4.7s)Step 32000 of 1000000; Loss: 6.6152e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0636e+03; Test Loss: 6.6194e+02. (Time: 5.3s)Step 32500 of 1000000; Loss: 6.6182e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1425e+03; Test Loss: 6.5952e+02. (Time: 4.5s)Step 33000 of 1000000; Loss: 6.5955e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0238e+03; Test Loss: 6.5874e+02. (Time: 5.6s)Step 33500 of 1000000; Loss: 6.5877e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.6636e+01; Test Loss: 6.5797e+02. (Time: 4.5s)Step 34000 of 1000000; Loss: 6.5800e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.1969e+00; Test Loss: 6.5730e+02. (Time: 4.5s)Step 34500 of 1000000; Loss: 6.5733e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0545e+03; Test Loss: 6.5675e+02. (Time: 5.5s)Step 35000 of 1000000; Loss: 6.5679e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1321e+03; Test Loss: 6.5634e+02. (Time: 4.4s)Step 35500 of 1000000; Loss: 6.5636e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0185e+03; Test Loss: 6.5603e+02. (Time: 4.8s)Step 36000 of 1000000; Loss: 6.5604e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.4624e+01; Test Loss: 6.5587e+02. (Time: 5.4s)Step 36500 of 1000000; Loss: 6.5587e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.7662e+00; Test Loss: 6.5557e+02. (Time: 4.4s)Step 37000 of 1000000; Loss: 6.5558e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0454e+03; Test Loss: 6.5523e+02. (Time: 5.6s)Step 37500 of 1000000; Loss: 6.5526e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1232e+03; Test Loss: 6.5496e+02. (Time: 4.4s)Step 38000 of 1000000; Loss: 6.5497e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0142e+03; Test Loss: 6.5463e+02. (Time: 4.4s)Step 38500 of 1000000; Loss: 6.5464e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.2899e+01; Test Loss: 6.5439e+02. (Time: 5.7s)Step 39000 of 1000000; Loss: 6.5441e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.4000e+00; Test Loss: 6.5421e+02. (Time: 4.5s)Step 39500 of 1000000; Loss: 6.5422e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0382e+03; Test Loss: 6.5407e+02. (Time: 4.7s)Step 40000 of 1000000; Loss: 6.5409e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1141e+03; Test Loss: 6.5394e+02. (Time: 5.3s)Step 40500 of 1000000; Loss: 6.5394e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0096e+03; Test Loss: 6.5377e+02. (Time: 4.4s)Step 41000 of 1000000; Loss: 6.5377e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1575e+01; Test Loss: 6.5354e+02. (Time: 5.5s)Step 41500 of 1000000; Loss: 6.5355e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.0873e+00; Test Loss: 6.5333e+02. (Time: 4.4s)Step 42000 of 1000000; Loss: 6.5334e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0306e+03; Test Loss: 6.5310e+02. (Time: 4.5s)Step 42500 of 1000000; Loss: 6.5312e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1078e+03; Test Loss: 6.5291e+02. (Time: 5.5s)Step 43000 of 1000000; Loss: 6.5291e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0060e+03; Test Loss: 6.5274e+02. (Time: 4.4s)Step 43500 of 1000000; Loss: 6.5275e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.0447e+01; Test Loss: 6.5257e+02. (Time: 4.6s)Step 44000 of 1000000; Loss: 6.5258e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.8359e+00; Test Loss: 6.5241e+02. (Time: 5.4s)Step 44500 of 1000000; Loss: 6.5241e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0229e+03; Test Loss: 6.5224e+02. (Time: 4.5s)Step 45000 of 1000000; Loss: 6.5226e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1019e+03; Test Loss: 6.5209e+02. (Time: 5.5s)Step 45500 of 1000000; Loss: 6.5211e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0026e+03; Test Loss: 6.5195e+02. (Time: 4.4s)Step 46000 of 1000000; Loss: 6.5196e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.9440e+01; Test Loss: 6.5175e+02. (Time: 4.4s)Step 46500 of 1000000; Loss: 6.5175e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.5858e+00; Test Loss: 6.5158e+02. (Time: 5.6s)Step 47000 of 1000000; Loss: 6.5159e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0161e+03; Test Loss: 6.5143e+02. (Time: 4.5s)Step 47500 of 1000000; Loss: 6.5144e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0960e+03; Test Loss: 6.5125e+02. (Time: 4.5s)Step 48000 of 1000000; Loss: 6.5127e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.9920e+02; Test Loss: 6.5112e+02. (Time: 5.8s)Step 48500 of 1000000; Loss: 6.5113e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.8556e+01; Test Loss: 6.5093e+02. (Time: 4.6s)Step 49000 of 1000000; Loss: 6.5094e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.3628e+00; Test Loss: 6.5069e+02. (Time: 5.5s)Step 49500 of 1000000; Loss: 6.5073e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0098e+03; Test Loss: 6.5054e+02. (Time: 4.5s)Step 50000 of 1000000; Loss: 6.5059e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0897e+03; Test Loss: 6.5046e+02. (Time: 4.4s)Step 50500 of 1000000; Loss: 6.5046e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.9581e+02; Test Loss: 6.5033e+02. (Time: 5.6s)Step 51000 of 1000000; Loss: 6.5034e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.7843e+01; Test Loss: 6.5015e+02. (Time: 4.5s)Step 51500 of 1000000; Loss: 6.5013e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.1939e+00; Test Loss: 6.4997e+02. (Time: 5.0s)Step 52000 of 1000000; Loss: 6.4994e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0043e+03; Test Loss: 6.4977e+02. (Time: 5.1s)Step 52500 of 1000000; Loss: 6.4976e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0847e+03; Test Loss: 6.4962e+02. (Time: 4.5s)Step 53000 of 1000000; Loss: 6.4962e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.9286e+02; Test Loss: 6.4956e+02. (Time: 5.6s)Step 53500 of 1000000; Loss: 6.4956e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.7020e+01; Test Loss: 6.4945e+02. (Time: 4.4s)Step 54000 of 1000000; Loss: 6.4946e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.9946e+00; Test Loss: 6.4929e+02. (Time: 5.8s)Step 54500 of 1000000; Loss: 6.4932e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9988e+03; Test Loss: 6.4919e+02. (Time: 5.3s)Step 55000 of 1000000; Loss: 6.4919e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0797e+03; Test Loss: 6.4911e+02. (Time: 4.6s)Step 55500 of 1000000; Loss: 6.4913e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.9032e+02; Test Loss: 6.4906e+02. (Time: 5.7s)Step 56000 of 1000000; Loss: 6.4906e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.6437e+01; Test Loss: 6.4905e+02. (Time: 4.5s)Step 56500 of 1000000; Loss: 6.4902e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.8679e+00; Test Loss: 6.4892e+02. (Time: 4.5s)Step 57000 of 1000000; Loss: 6.4889e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9935e+03; Test Loss: 6.4883e+02. (Time: 5.8s)Step 57500 of 1000000; Loss: 6.4885e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0746e+03; Test Loss: 6.4879e+02. (Time: 4.6s)Step 58000 of 1000000; Loss: 6.4880e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.8775e+02; Test Loss: 6.4877e+02. (Time: 5.0s)Step 58500 of 1000000; Loss: 6.4878e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5904e+01; Test Loss: 6.4874e+02. (Time: 5.0s)Step 59000 of 1000000; Loss: 6.4872e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.7007e+00; Test Loss: 6.4874e+02. (Time: 4.4s)Step 59500 of 1000000; Loss: 6.4874e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9889e+03; Test Loss: 6.4871e+02. (Time: 5.6s)Step 60000 of 1000000; Loss: 6.4871e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0702e+03; Test Loss: 6.4866e+02. (Time: 4.5s)Step 60500 of 1000000; Loss: 6.4869e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.8464e+02; Test Loss: 6.4866e+02. (Time: 4.4s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.2432\n",
            "Fold 6 test Avg NLL: 0.2432\n",
            "\n",
            "=== Fold 7 ===\n",
            "Step 500 of 500; Loss: 3.4150e+03; Test Loss: 1.1358e+03. (Time: 5.7s)Step 500 of 1000000; Loss: 1.1373e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9000e+03; Test Loss: 1.1073e+03. (Time: 4.4s)Step 1000 of 1000000; Loss: 1.1086e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.7124e+02; Test Loss: 1.0785e+03. (Time: 5.1s)Step 1500 of 1000000; Loss: 1.0800e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8017e+02; Test Loss: 1.0499e+03. (Time: 4.7s)Step 2000 of 1000000; Loss: 1.0513e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1615e+03; Test Loss: 1.0210e+03. (Time: 4.4s)Step 2500 of 1000000; Loss: 1.0224e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1194e+03; Test Loss: 9.9754e+02. (Time: 5.5s)Step 3000 of 1000000; Loss: 9.9859e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.6909e+03; Test Loss: 9.7623e+02. (Time: 4.4s)Step 3500 of 1000000; Loss: 9.7724e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.2678e+02; Test Loss: 9.5772e+02. (Time: 4.4s)Step 4000 of 1000000; Loss: 9.5853e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.4882e+02; Test Loss: 9.4187e+02. (Time: 5.5s)Step 4500 of 1000000; Loss: 9.4264e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.9028e+03; Test Loss: 9.2642e+02. (Time: 4.4s)Step 5000 of 1000000; Loss: 9.2718e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.8981e+03; Test Loss: 9.1111e+02. (Time: 4.6s)Step 5500 of 1000000; Loss: 9.1186e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.5718e+03; Test Loss: 8.9543e+02. (Time: 5.4s)Step 6000 of 1000000; Loss: 8.9620e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.3190e+02; Test Loss: 8.7945e+02. (Time: 4.4s)Step 6500 of 1000000; Loss: 8.8024e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2288e+02; Test Loss: 8.6339e+02. (Time: 5.6s)Step 7000 of 1000000; Loss: 8.6417e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.6952e+03; Test Loss: 8.4772e+02. (Time: 4.4s)Step 7500 of 1000000; Loss: 8.4848e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.7010e+03; Test Loss: 8.3230e+02. (Time: 4.5s)Step 8000 of 1000000; Loss: 8.3305e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.4509e+03; Test Loss: 8.1708e+02. (Time: 5.6s)Step 8500 of 1000000; Loss: 8.1781e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.3024e+02; Test Loss: 8.0200e+02. (Time: 4.6s)Step 9000 of 1000000; Loss: 8.0275e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.5028e+01; Test Loss: 7.8644e+02. (Time: 4.7s)Step 9500 of 1000000; Loss: 7.8720e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5086e+03; Test Loss: 7.7132e+02. (Time: 5.3s)Step 10000 of 1000000; Loss: 7.7205e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5271e+03; Test Loss: 7.5619e+02. (Time: 4.4s)Step 10500 of 1000000; Loss: 7.5694e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.3337e+03; Test Loss: 7.4073e+02. (Time: 5.4s)Step 11000 of 1000000; Loss: 7.4148e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.0776e+02; Test Loss: 7.2541e+02. (Time: 4.5s)Step 11500 of 1000000; Loss: 7.2615e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.5370e+01; Test Loss: 7.1053e+02. (Time: 4.5s)Step 12000 of 1000000; Loss: 7.1124e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3429e+03; Test Loss: 6.9626e+02. (Time: 5.5s)Step 12500 of 1000000; Loss: 6.9692e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3637e+03; Test Loss: 6.8300e+02. (Time: 4.4s)Step 13000 of 1000000; Loss: 6.8362e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2306e+03; Test Loss: 6.7110e+02. (Time: 4.6s)Step 13500 of 1000000; Loss: 6.7163e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0619e+02; Test Loss: 6.6077e+02. (Time: 5.4s)Step 14000 of 1000000; Loss: 6.6126e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.4693e+01; Test Loss: 6.4957e+02. (Time: 4.5s)Step 14500 of 1000000; Loss: 6.5011e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1982e+03; Test Loss: 6.3892e+02. (Time: 5.7s)Step 15000 of 1000000; Loss: 6.3943e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2345e+03; Test Loss: 6.2933e+02. (Time: 4.5s)Step 15500 of 1000000; Loss: 6.2976e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1479e+03; Test Loss: 6.2129e+02. (Time: 4.4s)Step 16000 of 1000000; Loss: 6.2164e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.3883e+02; Test Loss: 6.1532e+02. (Time: 5.5s)Step 16500 of 1000000; Loss: 6.1557e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.8411e+01; Test Loss: 6.1042e+02. (Time: 4.4s)Step 17000 of 1000000; Loss: 6.1065e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1107e+03; Test Loss: 6.0625e+02. (Time: 4.4s)Step 17500 of 1000000; Loss: 6.0644e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1720e+03; Test Loss: 6.0279e+02. (Time: 5.6s)Step 18000 of 1000000; Loss: 6.0295e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1106e+03; Test Loss: 6.0004e+02. (Time: 4.4s)Step 18500 of 1000000; Loss: 6.0016e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0014e+02; Test Loss: 5.9729e+02. (Time: 5.3s)Step 19000 of 1000000; Loss: 5.9742e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9514e+01; Test Loss: 5.9481e+02. (Time: 4.7s)Step 19500 of 1000000; Loss: 5.9494e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0696e+03; Test Loss: 5.9226e+02. (Time: 4.5s)Step 20000 of 1000000; Loss: 5.9241e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1535e+03; Test Loss: 5.8928e+02. (Time: 5.5s)Step 20500 of 1000000; Loss: 5.8943e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0892e+03; Test Loss: 5.8706e+02. (Time: 4.5s)Step 21000 of 1000000; Loss: 5.8716e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.3441e+01; Test Loss: 5.8527e+02. (Time: 4.7s)Step 21500 of 1000000; Loss: 5.8534e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.4401e+01; Test Loss: 5.8368e+02. (Time: 5.4s)Step 22000 of 1000000; Loss: 5.8375e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0449e+03; Test Loss: 5.8213e+02. (Time: 4.4s)Step 22500 of 1000000; Loss: 5.8224e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1433e+03; Test Loss: 5.8093e+02. (Time: 5.3s)Step 23000 of 1000000; Loss: 5.8098e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0782e+03; Test Loss: 5.7992e+02. (Time: 4.5s)Step 23500 of 1000000; Loss: 5.7993e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.3451e+01; Test Loss: 5.7879e+02. (Time: 4.4s)Step 24000 of 1000000; Loss: 5.7881e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2732e+01; Test Loss: 5.7774e+02. (Time: 5.6s)Step 24500 of 1000000; Loss: 5.7780e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0261e+03; Test Loss: 5.7650e+02. (Time: 4.4s)Step 25000 of 1000000; Loss: 5.7659e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1313e+03; Test Loss: 5.7568e+02. (Time: 4.5s)Step 25500 of 1000000; Loss: 5.7573e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0686e+03; Test Loss: 5.7487e+02. (Time: 6.2s)Step 26000 of 1000000; Loss: 5.7490e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.6581e+01; Test Loss: 5.7440e+02. (Time: 4.4s)Step 26500 of 1000000; Loss: 5.7441e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1554e+01; Test Loss: 5.7390e+02. (Time: 5.6s)Step 27000 of 1000000; Loss: 5.7393e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0106e+03; Test Loss: 5.7327e+02. (Time: 4.4s)Step 27500 of 1000000; Loss: 5.7333e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1210e+03; Test Loss: 5.7266e+02. (Time: 4.4s)Step 28000 of 1000000; Loss: 5.7269e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0611e+03; Test Loss: 5.7215e+02. (Time: 5.7s)Step 28500 of 1000000; Loss: 5.7217e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.0800e+01; Test Loss: 5.7157e+02. (Time: 4.5s)Step 29000 of 1000000; Loss: 5.7157e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0742e+01; Test Loss: 5.7091e+02. (Time: 5.0s)Step 29500 of 1000000; Loss: 5.7093e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9970e+03; Test Loss: 5.7022e+02. (Time: 5.0s)Step 30000 of 1000000; Loss: 5.7029e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1109e+03; Test Loss: 5.6972e+02. (Time: 4.5s)Step 30500 of 1000000; Loss: 5.6975e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0557e+03; Test Loss: 5.6940e+02. (Time: 5.7s)Step 31000 of 1000000; Loss: 5.6940e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.6437e+01; Test Loss: 5.6902e+02. (Time: 4.6s)Step 31500 of 1000000; Loss: 5.6902e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0069e+01; Test Loss: 5.6862e+02. (Time: 4.5s)Step 32000 of 1000000; Loss: 5.6863e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9847e+03; Test Loss: 5.6827e+02. (Time: 5.7s)Step 32500 of 1000000; Loss: 5.6832e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1019e+03; Test Loss: 5.6801e+02. (Time: 4.4s)Step 33000 of 1000000; Loss: 5.6805e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0503e+03; Test Loss: 5.6775e+02. (Time: 5.4s)Step 33500 of 1000000; Loss: 5.6775e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.2815e+01; Test Loss: 5.6751e+02. (Time: 4.9s)Step 34000 of 1000000; Loss: 5.6748e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.4954e+00; Test Loss: 5.6724e+02. (Time: 4.5s)Step 34500 of 1000000; Loss: 5.6722e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9727e+03; Test Loss: 5.6694e+02. (Time: 5.5s)Step 35000 of 1000000; Loss: 5.6698e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0941e+03; Test Loss: 5.6673e+02. (Time: 4.4s)Step 35500 of 1000000; Loss: 5.6677e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0449e+03; Test Loss: 5.6656e+02. (Time: 4.5s)Step 36000 of 1000000; Loss: 5.6655e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.9526e+01; Test Loss: 5.6646e+02. (Time: 5.7s)Step 36500 of 1000000; Loss: 5.6644e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.0552e+00; Test Loss: 5.6635e+02. (Time: 4.4s)Step 37000 of 1000000; Loss: 5.6631e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9615e+03; Test Loss: 5.6612e+02. (Time: 5.1s)Step 37500 of 1000000; Loss: 5.6617e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0870e+03; Test Loss: 5.6602e+02. (Time: 5.0s)Step 38000 of 1000000; Loss: 5.6605e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0402e+03; Test Loss: 5.6597e+02. (Time: 4.5s)Step 38500 of 1000000; Loss: 5.6596e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.6543e+01; Test Loss: 5.6596e+02. (Time: 5.6s)Step 39000 of 1000000; Loss: 5.6593e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.7318e+00; Test Loss: 5.6603e+02. (Time: 4.5s)Step 39500 of 1000000; Loss: 5.6599e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9512e+03; Test Loss: 5.6587e+02. (Time: 4.5s)Step 40000 of 1000000; Loss: 5.6592e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0806e+03; Test Loss: 5.6595e+02. (Time: 5.8s)Step 40500 of 1000000; Loss: 5.6598e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0357e+03; Test Loss: 5.6604e+02. (Time: 4.6s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3192\n",
            "Fold 7 test Avg NLL: 0.3192\n",
            "\n",
            "=== Fold 8 ===\n",
            "Step 500 of 500; Loss: 3.3944e+03; Test Loss: 1.1661e+03. (Time: 5.6s)Step 500 of 1000000; Loss: 1.1675e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8417e+03; Test Loss: 1.1373e+03. (Time: 4.4s)Step 1000 of 1000000; Loss: 1.1387e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.7242e+02; Test Loss: 1.1072e+03. (Time: 4.5s)Step 1500 of 1000000; Loss: 1.1088e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9261e+02; Test Loss: 1.0762e+03. (Time: 5.6s)Step 2000 of 1000000; Loss: 1.0778e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1404e+03; Test Loss: 1.0456e+03. (Time: 4.4s)Step 2500 of 1000000; Loss: 1.0471e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1153e+03; Test Loss: 1.0212e+03. (Time: 4.8s)Step 3000 of 1000000; Loss: 1.0223e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.6235e+03; Test Loss: 9.9969e+02. (Time: 5.2s)Step 3500 of 1000000; Loss: 1.0007e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.2602e+02; Test Loss: 9.8064e+02. (Time: 4.6s)Step 4000 of 1000000; Loss: 9.8151e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.6185e+02; Test Loss: 9.6453e+02. (Time: 5.7s)Step 4500 of 1000000; Loss: 9.6528e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.8699e+03; Test Loss: 9.4911e+02. (Time: 4.5s)Step 5000 of 1000000; Loss: 9.4986e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.9075e+03; Test Loss: 9.3405e+02. (Time: 4.6s)Step 5500 of 1000000; Loss: 9.3479e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.4960e+03; Test Loss: 9.1826e+02. (Time: 5.7s)Step 6000 of 1000000; Loss: 9.1903e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.2945e+02; Test Loss: 9.0252e+02. (Time: 4.5s)Step 6500 of 1000000; Loss: 9.0329e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.3559e+02; Test Loss: 8.8714e+02. (Time: 5.3s)Step 7000 of 1000000; Loss: 8.8787e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.6612e+03; Test Loss: 8.7232e+02. (Time: 6.1s)Step 7500 of 1000000; Loss: 8.7304e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.7332e+03; Test Loss: 8.5763e+02. (Time: 4.8s)Step 8000 of 1000000; Loss: 8.5834e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.3657e+03; Test Loss: 8.4291e+02. (Time: 6.2s)Step 8500 of 1000000; Loss: 8.4362e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.2313e+02; Test Loss: 8.2809e+02. (Time: 5.2s)Step 9000 of 1000000; Loss: 8.2882e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0516e+02; Test Loss: 8.1359e+02. (Time: 6.3s)Step 9500 of 1000000; Loss: 8.1428e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.4689e+03; Test Loss: 7.9966e+02. (Time: 5.5s)Step 10000 of 1000000; Loss: 8.0034e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5719e+03; Test Loss: 7.8599e+02. (Time: 5.7s)Step 10500 of 1000000; Loss: 7.8666e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2343e+03; Test Loss: 7.7270e+02. (Time: 5.8s)Step 11000 of 1000000; Loss: 7.7336e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.0465e+02; Test Loss: 7.5942e+02. (Time: 4.8s)Step 11500 of 1000000; Loss: 7.6007e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.3756e+01; Test Loss: 7.4631e+02. (Time: 5.9s)Step 12000 of 1000000; Loss: 7.4696e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2942e+03; Test Loss: 7.3344e+02. (Time: 4.7s)Step 12500 of 1000000; Loss: 7.3404e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.4208e+03; Test Loss: 7.2139e+02. (Time: 5.0s)Step 13000 of 1000000; Loss: 7.2197e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1258e+03; Test Loss: 7.0987e+02. (Time: 5.5s)Step 13500 of 1000000; Loss: 7.1042e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1215e+02; Test Loss: 6.9847e+02. (Time: 4.7s)Step 14000 of 1000000; Loss: 6.9903e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.8377e+01; Test Loss: 6.8602e+02. (Time: 6.0s)Step 14500 of 1000000; Loss: 6.8657e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1414e+03; Test Loss: 6.7609e+02. (Time: 4.7s)Step 15000 of 1000000; Loss: 6.7653e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2906e+03; Test Loss: 6.6751e+02. (Time: 5.1s)Step 15500 of 1000000; Loss: 6.6790e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0435e+03; Test Loss: 6.6008e+02. (Time: 5.8s)Step 16000 of 1000000; Loss: 6.6040e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.5052e+02; Test Loss: 6.5352e+02. (Time: 4.7s)Step 16500 of 1000000; Loss: 6.5383e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.0133e+01; Test Loss: 6.4764e+02. (Time: 5.8s)Step 17000 of 1000000; Loss: 6.4791e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0622e+03; Test Loss: 6.4276e+02. (Time: 4.5s)Step 17500 of 1000000; Loss: 6.4299e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2275e+03; Test Loss: 6.3824e+02. (Time: 4.4s)Step 18000 of 1000000; Loss: 6.3843e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0102e+03; Test Loss: 6.3435e+02. (Time: 5.7s)Step 18500 of 1000000; Loss: 6.3454e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0387e+02; Test Loss: 6.3103e+02. (Time: 5.8s)Step 19000 of 1000000; Loss: 6.3117e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2907e+01; Test Loss: 6.2833e+02. (Time: 5.6s)Step 19500 of 1000000; Loss: 6.2845e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0283e+03; Test Loss: 6.2603e+02. (Time: 4.6s)Step 20000 of 1000000; Loss: 6.2613e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2032e+03; Test Loss: 6.2395e+02. (Time: 4.5s)Step 20500 of 1000000; Loss: 6.2406e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.9600e+02; Test Loss: 6.2207e+02. (Time: 5.6s)Step 21000 of 1000000; Loss: 6.2216e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.5145e+01; Test Loss: 6.2028e+02. (Time: 4.6s)Step 21500 of 1000000; Loss: 6.2037e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8186e+01; Test Loss: 6.1864e+02. (Time: 5.0s)Step 22000 of 1000000; Loss: 6.1872e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0075e+03; Test Loss: 6.1715e+02. (Time: 5.2s)Step 22500 of 1000000; Loss: 6.1722e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1930e+03; Test Loss: 6.1560e+02. (Time: 4.5s)Step 23000 of 1000000; Loss: 6.1569e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.8511e+02; Test Loss: 6.1399e+02. (Time: 5.6s)Step 23500 of 1000000; Loss: 6.1408e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.9240e+01; Test Loss: 6.1285e+02. (Time: 4.5s)Step 24000 of 1000000; Loss: 6.1291e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.4460e+01; Test Loss: 6.1181e+02. (Time: 4.6s)Step 24500 of 1000000; Loss: 6.1185e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9907e+03; Test Loss: 6.1069e+02. (Time: 5.6s)Step 25000 of 1000000; Loss: 6.1073e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1843e+03; Test Loss: 6.0982e+02. (Time: 4.4s)Step 25500 of 1000000; Loss: 6.0987e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.7830e+02; Test Loss: 6.0900e+02. (Time: 5.1s)Step 26000 of 1000000; Loss: 6.0901e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.0985e+01; Test Loss: 6.0817e+02. (Time: 4.9s)Step 26500 of 1000000; Loss: 6.0820e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2286e+01; Test Loss: 6.0728e+02. (Time: 4.5s)Step 27000 of 1000000; Loss: 6.0731e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9753e+03; Test Loss: 6.0664e+02. (Time: 5.7s)Step 27500 of 1000000; Loss: 6.0665e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1765e+03; Test Loss: 6.0631e+02. (Time: 4.5s)Step 28000 of 1000000; Loss: 6.0635e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.7261e+02; Test Loss: 6.0560e+02. (Time: 4.4s)Step 28500 of 1000000; Loss: 6.0564e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.4385e+01; Test Loss: 6.0502e+02. (Time: 5.6s)Step 29000 of 1000000; Loss: 6.0504e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0438e+01; Test Loss: 6.0460e+02. (Time: 4.5s)Step 29500 of 1000000; Loss: 6.0463e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9624e+03; Test Loss: 6.0388e+02. (Time: 4.9s)Step 30000 of 1000000; Loss: 6.0392e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1699e+03; Test Loss: 6.0346e+02. (Time: 5.1s)Step 30500 of 1000000; Loss: 6.0349e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.6814e+02; Test Loss: 6.0291e+02. (Time: 4.5s)Step 31000 of 1000000; Loss: 6.0293e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.0580e+01; Test Loss: 6.0241e+02. (Time: 5.6s)Step 31500 of 1000000; Loss: 6.0243e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.3015e+00; Test Loss: 6.0192e+02. (Time: 4.5s)Step 32000 of 1000000; Loss: 6.0194e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9516e+03; Test Loss: 6.0140e+02. (Time: 4.5s)Step 32500 of 1000000; Loss: 6.0143e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1634e+03; Test Loss: 6.0091e+02. (Time: 5.7s)Step 33000 of 1000000; Loss: 6.0094e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.6410e+02; Test Loss: 6.0046e+02. (Time: 4.5s)Step 33500 of 1000000; Loss: 6.0048e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.7558e+01; Test Loss: 6.0001e+02. (Time: 4.8s)Step 34000 of 1000000; Loss: 6.0003e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.4785e+00; Test Loss: 5.9971e+02. (Time: 5.1s)Step 34500 of 1000000; Loss: 5.9972e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9426e+03; Test Loss: 5.9936e+02. (Time: 4.4s)Step 35000 of 1000000; Loss: 5.9939e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1569e+03; Test Loss: 5.9907e+02. (Time: 5.6s)Step 35500 of 1000000; Loss: 5.9909e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.6029e+02; Test Loss: 5.9868e+02. (Time: 4.5s)Step 36000 of 1000000; Loss: 5.9870e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.5100e+01; Test Loss: 5.9824e+02. (Time: 4.7s)Step 36500 of 1000000; Loss: 5.9826e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.8955e+00; Test Loss: 5.9785e+02. (Time: 5.8s)Step 37000 of 1000000; Loss: 5.9786e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9342e+03; Test Loss: 5.9745e+02. (Time: 4.5s)Step 37500 of 1000000; Loss: 5.9749e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1515e+03; Test Loss: 5.9710e+02. (Time: 5.0s)Step 38000 of 1000000; Loss: 5.9712e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.5729e+02; Test Loss: 5.9675e+02. (Time: 5.1s)Step 38500 of 1000000; Loss: 5.9677e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.3022e+01; Test Loss: 5.9644e+02. (Time: 4.5s)Step 39000 of 1000000; Loss: 5.9645e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.4580e+00; Test Loss: 5.9614e+02. (Time: 5.7s)Step 39500 of 1000000; Loss: 5.9615e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9265e+03; Test Loss: 5.9582e+02. (Time: 4.6s)Step 40000 of 1000000; Loss: 5.9585e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1464e+03; Test Loss: 5.9553e+02. (Time: 4.5s)Step 40500 of 1000000; Loss: 5.9555e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.5443e+02; Test Loss: 5.9524e+02. (Time: 5.7s)Step 41000 of 1000000; Loss: 5.9526e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1309e+01; Test Loss: 5.9501e+02. (Time: 4.4s)Step 41500 of 1000000; Loss: 5.9501e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.1856e+00; Test Loss: 5.9481e+02. (Time: 5.1s)Step 42000 of 1000000; Loss: 5.9481e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9204e+03; Test Loss: 5.9458e+02. (Time: 5.0s)Step 42500 of 1000000; Loss: 5.9460e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1417e+03; Test Loss: 5.9436e+02. (Time: 4.4s)Step 43000 of 1000000; Loss: 5.9438e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.5189e+02; Test Loss: 5.9415e+02. (Time: 5.6s)Step 43500 of 1000000; Loss: 5.9416e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.9943e+01; Test Loss: 5.9392e+02. (Time: 4.5s)Step 44000 of 1000000; Loss: 5.9393e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.9609e+00; Test Loss: 5.9370e+02. (Time: 4.4s)Step 44500 of 1000000; Loss: 5.9370e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9148e+03; Test Loss: 5.9356e+02. (Time: 5.6s)Step 45000 of 1000000; Loss: 5.9358e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1376e+03; Test Loss: 5.9344e+02. (Time: 4.5s)Step 45500 of 1000000; Loss: 5.9346e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.4913e+02; Test Loss: 5.9333e+02. (Time: 5.0s)Step 46000 of 1000000; Loss: 5.9333e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.8814e+01; Test Loss: 5.9320e+02. (Time: 5.1s)Step 46500 of 1000000; Loss: 5.9321e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.7513e+00; Test Loss: 5.9301e+02. (Time: 4.5s)Step 47000 of 1000000; Loss: 5.9301e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9098e+03; Test Loss: 5.9283e+02. (Time: 5.6s)Step 47500 of 1000000; Loss: 5.9285e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1331e+03; Test Loss: 5.9266e+02. (Time: 4.4s)Step 48000 of 1000000; Loss: 5.9268e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.4649e+02; Test Loss: 5.9250e+02. (Time: 4.5s)Step 48500 of 1000000; Loss: 5.9251e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.7832e+01; Test Loss: 5.9239e+02. (Time: 5.7s)Step 49000 of 1000000; Loss: 5.9238e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.5686e+00; Test Loss: 5.9222e+02. (Time: 4.5s)Step 49500 of 1000000; Loss: 5.9223e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9049e+03; Test Loss: 5.9212e+02. (Time: 5.1s)Step 50000 of 1000000; Loss: 5.9213e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1294e+03; Test Loss: 5.9202e+02. (Time: 5.1s)Step 50500 of 1000000; Loss: 5.9204e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.4424e+02; Test Loss: 5.9195e+02. (Time: 4.5s)Step 51000 of 1000000; Loss: 5.9196e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.6964e+01; Test Loss: 5.9188e+02. (Time: 5.7s)Step 51500 of 1000000; Loss: 5.9188e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.4177e+00; Test Loss: 5.9185e+02. (Time: 4.6s)Step 52000 of 1000000; Loss: 5.9185e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9006e+03; Test Loss: 5.9190e+02. (Time: 4.5s)Step 52500 of 1000000; Loss: 5.9190e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1256e+03; Test Loss: 5.9199e+02. (Time: 5.6s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3557\n",
            "Fold 8 test Avg NLL: 0.3557\n",
            "\n",
            "=== Fold 9 ===\n",
            "Step 500 of 500; Loss: 3.4101e+03; Test Loss: 1.1700e+03. (Time: 4.4s)Step 500 of 1000000; Loss: 1.1713e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8565e+03; Test Loss: 1.1419e+03. (Time: 5.5s)Step 1000 of 1000000; Loss: 1.1433e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.6652e+02; Test Loss: 1.1121e+03. (Time: 4.5s)Step 1500 of 1000000; Loss: 1.1137e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9333e+02; Test Loss: 1.0806e+03. (Time: 5.8s)Step 2000 of 1000000; Loss: 1.0822e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1202e+03; Test Loss: 1.0486e+03. (Time: 5.5s)Step 2500 of 1000000; Loss: 1.0501e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1478e+03; Test Loss: 1.0221e+03. (Time: 4.4s)Step 3000 of 1000000; Loss: 1.0233e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.6088e+03; Test Loss: 9.9893e+02. (Time: 5.5s)Step 3500 of 1000000; Loss: 1.0000e+03. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.2100e+02; Test Loss: 9.7997e+02. (Time: 4.5s)Step 4000 of 1000000; Loss: 9.8082e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.6000e+02; Test Loss: 9.6348e+02. (Time: 4.4s)Step 4500 of 1000000; Loss: 9.6426e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.8585e+03; Test Loss: 9.4745e+02. (Time: 5.6s)Step 5000 of 1000000; Loss: 9.4823e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.9446e+03; Test Loss: 9.3167e+02. (Time: 4.4s)Step 5500 of 1000000; Loss: 9.3244e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.4730e+03; Test Loss: 9.1525e+02. (Time: 4.4s)Step 6000 of 1000000; Loss: 9.1607e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.2211e+02; Test Loss: 8.9832e+02. (Time: 5.6s)Step 6500 of 1000000; Loss: 8.9916e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.3214e+02; Test Loss: 8.8153e+02. (Time: 4.5s)Step 7000 of 1000000; Loss: 8.8233e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.6592e+03; Test Loss: 8.6504e+02. (Time: 5.4s)Step 7500 of 1000000; Loss: 8.6585e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.7720e+03; Test Loss: 8.4822e+02. (Time: 4.6s)Step 8000 of 1000000; Loss: 8.4905e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.3346e+03; Test Loss: 8.3136e+02. (Time: 4.4s)Step 8500 of 1000000; Loss: 8.3219e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.0810e+02; Test Loss: 8.1444e+02. (Time: 5.6s)Step 9000 of 1000000; Loss: 8.1528e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0154e+02; Test Loss: 7.9782e+02. (Time: 4.5s)Step 9500 of 1000000; Loss: 7.9860e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.4828e+03; Test Loss: 7.8229e+02. (Time: 4.5s)Step 10000 of 1000000; Loss: 7.8303e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.6200e+03; Test Loss: 7.6730e+02. (Time: 5.7s)Step 10500 of 1000000; Loss: 7.6802e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1954e+03; Test Loss: 7.5283e+02. (Time: 4.6s)Step 11000 of 1000000; Loss: 7.5355e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.8853e+02; Test Loss: 7.3876e+02. (Time: 5.7s)Step 11500 of 1000000; Loss: 7.3945e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.3254e+01; Test Loss: 7.2508e+02. (Time: 4.5s)Step 12000 of 1000000; Loss: 7.2573e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3203e+03; Test Loss: 7.1183e+02. (Time: 4.5s)Step 12500 of 1000000; Loss: 7.1245e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.4943e+03; Test Loss: 6.9930e+02. (Time: 5.6s)Step 13000 of 1000000; Loss: 6.9990e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0739e+03; Test Loss: 6.8760e+02. (Time: 4.5s)Step 13500 of 1000000; Loss: 6.8818e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9691e+02; Test Loss: 6.7451e+02. (Time: 4.8s)Step 14000 of 1000000; Loss: 6.7518e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.8673e+01; Test Loss: 6.6010e+02. (Time: 5.2s)Step 14500 of 1000000; Loss: 6.6084e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1490e+03; Test Loss: 6.4638e+02. (Time: 4.5s)Step 15000 of 1000000; Loss: 6.4692e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3855e+03; Test Loss: 6.3645e+02. (Time: 5.6s)Step 15500 of 1000000; Loss: 6.3691e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.5560e+02; Test Loss: 6.2671e+02. (Time: 4.4s)Step 16000 of 1000000; Loss: 6.2723e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0053e+02; Test Loss: 6.1823e+02. (Time: 4.4s)Step 16500 of 1000000; Loss: 6.1853e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0255e+01; Test Loss: 6.1305e+02. (Time: 5.7s)Step 17000 of 1000000; Loss: 6.1326e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0611e+03; Test Loss: 6.0879e+02. (Time: 4.5s)Step 17500 of 1000000; Loss: 6.0898e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3381e+03; Test Loss: 6.0523e+02. (Time: 4.6s)Step 18000 of 1000000; Loss: 6.0540e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.0917e+02; Test Loss: 6.0202e+02. (Time: 5.4s)Step 18500 of 1000000; Loss: 6.0217e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 7.2440e+01; Test Loss: 5.9936e+02. (Time: 4.4s)Step 19000 of 1000000; Loss: 5.9948e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.4351e+01; Test Loss: 5.9683e+02. (Time: 5.4s)Step 19500 of 1000000; Loss: 5.9695e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0224e+03; Test Loss: 5.9452e+02. (Time: 4.6s)Step 20000 of 1000000; Loss: 5.9463e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3142e+03; Test Loss: 5.9240e+02. (Time: 4.4s)Step 20500 of 1000000; Loss: 5.9248e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.8848e+02; Test Loss: 5.9082e+02. (Time: 5.6s)Step 21000 of 1000000; Loss: 5.9089e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.7662e+01; Test Loss: 5.8972e+02. (Time: 4.4s)Step 21500 of 1000000; Loss: 5.8975e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.2204e+01; Test Loss: 5.8901e+02. (Time: 4.4s)Step 22000 of 1000000; Loss: 5.8903e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.0018e+03; Test Loss: 5.8846e+02. (Time: 5.5s)Step 22500 of 1000000; Loss: 5.8847e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2991e+03; Test Loss: 5.8793e+02. (Time: 4.5s)Step 23000 of 1000000; Loss: 5.8795e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.7711e+02; Test Loss: 5.8735e+02. (Time: 5.4s)Step 23500 of 1000000; Loss: 5.8740e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.0148e+01; Test Loss: 5.8645e+02. (Time: 4.7s)Step 24000 of 1000000; Loss: 5.8651e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.1362e+01; Test Loss: 5.8552e+02. (Time: 4.4s)Step 24500 of 1000000; Loss: 5.8555e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9850e+03; Test Loss: 5.8452e+02. (Time: 5.6s)Step 25000 of 1000000; Loss: 5.8458e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2869e+03; Test Loss: 5.8354e+02. (Time: 4.4s)Step 25500 of 1000000; Loss: 5.8359e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.7014e+02; Test Loss: 5.8251e+02. (Time: 4.4s)Step 26000 of 1000000; Loss: 5.8256e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.5221e+01; Test Loss: 5.8146e+02. (Time: 5.6s)Step 26500 of 1000000; Loss: 5.8151e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0810e+01; Test Loss: 5.8033e+02. (Time: 4.4s)Step 27000 of 1000000; Loss: 5.8038e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9705e+03; Test Loss: 5.7928e+02. (Time: 5.2s)Step 27500 of 1000000; Loss: 5.7934e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2755e+03; Test Loss: 5.7814e+02. (Time: 4.7s)Step 28000 of 1000000; Loss: 5.7819e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.6444e+02; Test Loss: 5.7707e+02. (Time: 4.4s)Step 28500 of 1000000; Loss: 5.7714e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 4.1689e+01; Test Loss: 5.7587e+02. (Time: 5.6s)Step 29000 of 1000000; Loss: 5.7593e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0408e+01; Test Loss: 5.7462e+02. (Time: 4.4s)Step 29500 of 1000000; Loss: 5.7468e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9562e+03; Test Loss: 5.7349e+02. (Time: 4.5s)Step 30000 of 1000000; Loss: 5.7355e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2661e+03; Test Loss: 5.7227e+02. (Time: 5.6s)Step 30500 of 1000000; Loss: 5.7231e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.5903e+02; Test Loss: 5.7123e+02. (Time: 4.4s)Step 31000 of 1000000; Loss: 5.7129e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.8750e+01; Test Loss: 5.7027e+02. (Time: 5.1s)Step 31500 of 1000000; Loss: 5.7032e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.0019e+01; Test Loss: 5.6928e+02. (Time: 4.8s)Step 32000 of 1000000; Loss: 5.6931e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9433e+03; Test Loss: 5.6844e+02. (Time: 4.4s)Step 32500 of 1000000; Loss: 5.6848e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2573e+03; Test Loss: 5.6743e+02. (Time: 5.5s)Step 33000 of 1000000; Loss: 5.6748e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.5472e+02; Test Loss: 5.6642e+02. (Time: 4.4s)Step 33500 of 1000000; Loss: 5.6648e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.6357e+01; Test Loss: 5.6551e+02. (Time: 4.5s)Step 34000 of 1000000; Loss: 5.6556e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.7620e+00; Test Loss: 5.6459e+02. (Time: 5.7s)Step 34500 of 1000000; Loss: 5.6464e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9308e+03; Test Loss: 5.6373e+02. (Time: 4.5s)Step 35000 of 1000000; Loss: 5.6377e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2485e+03; Test Loss: 5.6277e+02. (Time: 5.2s)Step 35500 of 1000000; Loss: 5.6282e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.5142e+02; Test Loss: 5.6176e+02. (Time: 4.9s)Step 36000 of 1000000; Loss: 5.6182e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.3955e+01; Test Loss: 5.6087e+02. (Time: 4.4s)Step 36500 of 1000000; Loss: 5.6092e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.5770e+00; Test Loss: 5.6009e+02. (Time: 5.6s)Step 37000 of 1000000; Loss: 5.6012e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9192e+03; Test Loss: 5.5926e+02. (Time: 5.9s)Step 37500 of 1000000; Loss: 5.5930e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2396e+03; Test Loss: 5.5839e+02. (Time: 5.3s)Step 38000 of 1000000; Loss: 5.5842e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.4907e+02; Test Loss: 5.5764e+02. (Time: 4.9s)Step 38500 of 1000000; Loss: 5.5768e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 3.1546e+01; Test Loss: 5.5697e+02. (Time: 4.4s)Step 39000 of 1000000; Loss: 5.5701e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.3764e+00; Test Loss: 5.5630e+02. (Time: 5.5s)Step 39500 of 1000000; Loss: 5.5634e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9099e+03; Test Loss: 5.5573e+02. (Time: 4.5s)Step 40000 of 1000000; Loss: 5.5576e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2314e+03; Test Loss: 5.5523e+02. (Time: 4.5s)Step 40500 of 1000000; Loss: 5.5524e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.4665e+02; Test Loss: 5.5474e+02. (Time: 5.5s)Step 41000 of 1000000; Loss: 5.5477e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.9596e+01; Test Loss: 5.5423e+02. (Time: 4.5s)Step 41500 of 1000000; Loss: 5.5426e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.2145e+00; Test Loss: 5.5371e+02. (Time: 5.0s)Step 42000 of 1000000; Loss: 5.5374e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.9005e+03; Test Loss: 5.5322e+02. (Time: 4.9s)Step 42500 of 1000000; Loss: 5.5324e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2241e+03; Test Loss: 5.5273e+02. (Time: 4.5s)Step 43000 of 1000000; Loss: 5.5274e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.4478e+02; Test Loss: 5.5223e+02. (Time: 5.6s)Step 43500 of 1000000; Loss: 5.5226e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.7719e+01; Test Loss: 5.5175e+02. (Time: 4.6s)Step 44000 of 1000000; Loss: 5.5178e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 9.0231e+00; Test Loss: 5.5131e+02. (Time: 4.7s)Step 44500 of 1000000; Loss: 5.5134e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8922e+03; Test Loss: 5.5087e+02. (Time: 5.7s)Step 45000 of 1000000; Loss: 5.5088e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2168e+03; Test Loss: 5.5040e+02. (Time: 4.5s)Step 45500 of 1000000; Loss: 5.5041e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.4296e+02; Test Loss: 5.4996e+02. (Time: 5.6s)Step 46000 of 1000000; Loss: 5.4999e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.6491e+01; Test Loss: 5.4955e+02. (Time: 4.7s)Step 46500 of 1000000; Loss: 5.4959e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.8009e+00; Test Loss: 5.4919e+02. (Time: 4.6s)Step 47000 of 1000000; Loss: 5.4922e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8846e+03; Test Loss: 5.4887e+02. (Time: 6.2s)Step 47500 of 1000000; Loss: 5.4888e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2107e+03; Test Loss: 5.4855e+02. (Time: 4.8s)Step 48000 of 1000000; Loss: 5.4855e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.4049e+02; Test Loss: 5.4810e+02. (Time: 5.7s)Step 48500 of 1000000; Loss: 5.4813e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.5270e+01; Test Loss: 5.4773e+02. (Time: 5.3s)Step 49000 of 1000000; Loss: 5.4775e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.5750e+00; Test Loss: 5.4743e+02. (Time: 5.1s)Step 49500 of 1000000; Loss: 5.4745e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8780e+03; Test Loss: 5.4718e+02. (Time: 6.7s)Step 50000 of 1000000; Loss: 5.4718e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2051e+03; Test Loss: 5.4700e+02. (Time: 5.1s)Step 50500 of 1000000; Loss: 5.4699e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.3857e+02; Test Loss: 5.4676e+02. (Time: 6.0s)Step 51000 of 1000000; Loss: 5.4678e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.4357e+01; Test Loss: 5.4654e+02. (Time: 4.9s)Step 51500 of 1000000; Loss: 5.4656e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.3971e+00; Test Loss: 5.4636e+02. (Time: 4.8s)Step 52000 of 1000000; Loss: 5.4638e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8716e+03; Test Loss: 5.4621e+02. (Time: 5.8s)Step 52500 of 1000000; Loss: 5.4621e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1997e+03; Test Loss: 5.4607e+02. (Time: 4.6s)Step 53000 of 1000000; Loss: 5.4606e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.3696e+02; Test Loss: 5.4591e+02. (Time: 5.8s)Step 53500 of 1000000; Loss: 5.4591e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3639e+01; Test Loss: 5.4576e+02. (Time: 4.5s)Step 54000 of 1000000; Loss: 5.4578e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.2726e+00; Test Loss: 5.4567e+02. (Time: 4.6s)Step 54500 of 1000000; Loss: 5.4569e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8661e+03; Test Loss: 5.4569e+02. (Time: 5.7s)Step 55000 of 1000000; Loss: 5.4568e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1946e+03; Test Loss: 5.4564e+02. (Time: 4.6s)Step 55500 of 1000000; Loss: 5.4562e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.3529e+02; Test Loss: 5.4558e+02. (Time: 5.3s)Step 56000 of 1000000; Loss: 5.4558e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.3057e+01; Test Loss: 5.4550e+02. (Time: 5.1s)Step 56500 of 1000000; Loss: 5.4552e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.1375e+00; Test Loss: 5.4544e+02. (Time: 4.6s)Step 57000 of 1000000; Loss: 5.4546e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 1.8610e+03; Test Loss: 5.4545e+02. (Time: 5.7s)Step 57500 of 1000000; Loss: 5.4543e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.1903e+03; Test Loss: 5.4543e+02. (Time: 4.5s)Step 58000 of 1000000; Loss: 5.4540e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 8.3378e+02; Test Loss: 5.4542e+02. (Time: 4.5s)Step 58500 of 1000000; Loss: 5.4542e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 2.2402e+01; Test Loss: 5.4540e+02. (Time: 5.7s)\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3600\n",
            "Fold 9 test Avg NLL: 0.3600\n",
            "\n",
            "All folds Avg NLL: [0.28717226 0.32933408 0.27035245 0.29517442 0.31657386 0.30063987\n",
            " 0.24318115 0.31915826 0.35574731 0.35999218]\n",
            "Mean NLL over folds: 0.3077\n",
            "Stddev over folds : 0.0364\n",
            "Std. Error (SE)    : 0.0115\n"
          ]
        }
      ],
      "source": [
        "## Generate outerloop datasets\n",
        "# 这是为了把上面猴子的数据给合并，然后调用我的这个format_into_datasets，生成三个数据集\n",
        "# 假设你已经有三个 np.ndarray：xs1,ys1；xs2,ys2；xs3,ys3\n",
        "xs_list = [xs_V, xs_w, xs_i]\n",
        "ys_list = [ys_V, ys_w, ys_i]\n",
        "\n",
        "folds = format_into_datasets_10_multi(\n",
        "    xs_list, ys_list,\n",
        "    dataset_constructor=rnn_utils.DatasetRNN,\n",
        "    batch_size=best_params['batch_size'],\n",
        "    random_seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Define the best Vanilla RNN with 32 units\n",
        "n_hidden = best_params['hidden_size']\n",
        "def make_vanilla_rnn():\n",
        "    model = hk.DeepRNN(\n",
        "        [hk.VanillaRNN(n_hidden), hk.Linear(output_size=2)]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "n_folds = len(folds)\n",
        "avg_nlls = np.zeros(n_folds)\n",
        "best_models = []\n",
        "\n",
        "if best_params['lr'] == 1e-3:\n",
        "    n_step_max = 10000\n",
        "elif best_params['lr'] == 1e-4:\n",
        "    n_step_max = 30000\n",
        "\n",
        "for i, (train_ds, val_ds, test_ds) in enumerate(folds):\n",
        "    print(f\"=== Fold {i} ===\")\n",
        "    # 用 train/val 训练并选超参\n",
        "    params, _ = rnn_utils.fit_model(\n",
        "        model_fun=make_vanilla_rnn,\n",
        "        dataset_train=train_ds,\n",
        "        dataset_test=val_ds,      # 用 val_ds 做 early-stop\n",
        "        optimizer=optax.chain(\n",
        "            optax.add_decayed_weights(best_params['weight_decay']),\n",
        "            optax.adam(learning_rate=best_params['lr'])\n",
        "        ),\n",
        "        n_steps_per_call=200,\n",
        "        n_steps_max=n_step_max,\n",
        "        # early_stop_step=200,\n",
        "    )\n",
        "\n",
        "    best_models.append(params)\n",
        "\n",
        "    # 在 test_ds 上计算平均 NLL\n",
        "    avg_nll = compute_negative_log_likelihood(test_ds, make_vanilla_rnn, params)\n",
        "    print(f\"Fold {i} test Avg NLL: {avg_nll:.4f}\\n\")\n",
        "    avg_nlls[i] = avg_nll\n",
        "\n",
        "# 汇总\n",
        "mean_nll = avg_nlls.mean()\n",
        "std_nll  = avg_nlls.std(ddof=1)    # 样本标准差\n",
        "se_nll   = std_nll / np.sqrt(n_folds)\n",
        "\n",
        "print(\"All folds Avg NLL:\", avg_nlls)\n",
        "print(f\"Mean NLL over folds: {mean_nll:.4f}\")\n",
        "print(f\"Stddev over folds : {std_nll:.4f}\")\n",
        "print(f\"Std. Error (SE)    : {se_nll:.4f}\")\n",
        "\n",
        "# save the results\n",
        "import pickle\n",
        "best_dict = {'best_params': best_params, 'best_models': best_models, 'NLLs': avg_nlls}\n",
        "with open('../Results/monkey_best_res.pkl', 'wb') as f:\n",
        "    pickle.dump(best_dict, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ute8Yj_xOMd8",
        "outputId": "78a99dbd-9a2c-4565-d35e-f52680e55f0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalized Likelihoods for GRU\n",
            "Training Dataset\n",
            "Average Normalized Likelihood: 81.1%\n",
            "Held-Out Dataset\n",
            "Average Normalized Likelihood: 84.1%\n"
          ]
        }
      ],
      "source": [
        "#@title Compute quality-of-fit: Held-out Normalized Likelihood\n",
        "# Compute log-likelihood\n",
        "print('Normalized Likelihoods for RNN')\n",
        "print('Training Dataset')\n",
        "training_likelihood = compute_log_likelihood(dataset_m_train, make_vanilla_rnn, gru_params)\n",
        "print('Held-Out Dataset')\n",
        "testing_likelihood = compute_log_likelihood(dataset_m_test, make_vanilla_rnn, gru_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_p5zBXbdIVx",
        "outputId": "ecd39638-6f62-435f-af97-4efa4063d3a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Held-Out Dataset\n",
            "Average Normalized Likelihood: 84.1%\n",
            "Average Negative Log-Likelihood: 0.2636\n"
          ]
        }
      ],
      "source": [
        "print('Held-Out Dataset')\n",
        "testing_likelihood = compute_log_likelihood(dataset_m_test, make_vanilla_rnn, gru_params)\n",
        "testing_likelihood = compute_negative_log_likelihood(dataset_m_test, make_vanilla_rnn, gru_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd4D6oblqlz6"
      },
      "source": [
        "### For Mice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tP0udL-Tqm-S",
        "outputId": "a7474cd1-4723-428a-fa6c-900ebe09ba7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping existing configuration: hidden_size=2, lr=0.001, weight_decay=0.001, batch_size=32\n",
            "Skipping existing configuration: hidden_size=2, lr=0.001, weight_decay=0.001, batch_size=64\n",
            "Skipping existing configuration: hidden_size=2, lr=0.001, weight_decay=0.0001, batch_size=32\n",
            "Skipping existing configuration: hidden_size=2, lr=0.001, weight_decay=0.0001, batch_size=64\n",
            "Skipping existing configuration: hidden_size=2, lr=0.001, weight_decay=1e-05, batch_size=32\n",
            "Skipping existing configuration: hidden_size=2, lr=0.001, weight_decay=1e-05, batch_size=64\n",
            "Skipping existing configuration: hidden_size=2, lr=0.0001, weight_decay=0.001, batch_size=32\n",
            "Skipping existing configuration: hidden_size=2, lr=0.0001, weight_decay=0.001, batch_size=64\n",
            "Skipping existing configuration: hidden_size=2, lr=0.0001, weight_decay=0.0001, batch_size=32\n",
            "Skipping existing configuration: hidden_size=2, lr=0.0001, weight_decay=0.0001, batch_size=64\n",
            "Skipping existing configuration: hidden_size=2, lr=0.0001, weight_decay=1e-05, batch_size=32\n",
            "Skipping existing configuration: hidden_size=2, lr=0.0001, weight_decay=1e-05, batch_size=64\n",
            "Skipping existing configuration: hidden_size=64, lr=0.001, weight_decay=0.001, batch_size=32\n",
            "Skipping existing configuration: hidden_size=64, lr=0.001, weight_decay=0.001, batch_size=64\n",
            "Skipping existing configuration: hidden_size=64, lr=0.001, weight_decay=0.0001, batch_size=32\n",
            "Skipping existing configuration: hidden_size=64, lr=0.001, weight_decay=0.0001, batch_size=64\n",
            "Skipping existing configuration: hidden_size=64, lr=0.001, weight_decay=1e-05, batch_size=32\n",
            "Skipping existing configuration: hidden_size=64, lr=0.001, weight_decay=1e-05, batch_size=64\n",
            "Skipping existing configuration: hidden_size=64, lr=0.0001, weight_decay=0.001, batch_size=32\n",
            "Skipping existing configuration: hidden_size=64, lr=0.0001, weight_decay=0.001, batch_size=64\n",
            "Skipping existing configuration: hidden_size=64, lr=0.0001, weight_decay=0.0001, batch_size=32\n",
            "Skipping existing configuration: hidden_size=64, lr=0.0001, weight_decay=0.0001, batch_size=64\n",
            "Skipping existing configuration: hidden_size=64, lr=0.0001, weight_decay=1e-05, batch_size=32\n",
            "Skipping existing configuration: hidden_size=64, lr=0.0001, weight_decay=1e-05, batch_size=64\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'hidden_size': [2, 64], \n",
        "    'lr': [1e-3, 1e-4],\n",
        "    'weight_decay': [1e-3, 1e-4, 1e-5],\n",
        "    'batch_size': [32, 64],\n",
        "}\n",
        "\n",
        "# Prepare to collect results\n",
        "output_path = '../Results/mice_grid_search_results.csv'\n",
        "# 1. 读取已存在的结果\n",
        "if os.path.exists(output_path):\n",
        "    df_existing = pd.read_csv(output_path)\n",
        "else:\n",
        "    # 定义空的 DataFrame，并确保列对齐\n",
        "    df_existing = pd.DataFrame(columns=[\n",
        "        'hidden_size', 'lr', 'weight_decay', 'batch_size', 'avg_val_nll', 'time'\n",
        "    ])\n",
        "# 2. 是否第一次写 header\n",
        "write_header = not os.path.exists(output_path)\n",
        "\n",
        "# Loop through all combinations\n",
        "for hs, lr, wd, bs in itertools.product(\n",
        "        param_grid['hidden_size'],\n",
        "        param_grid['lr'],\n",
        "        param_grid['weight_decay'],\n",
        "        param_grid['batch_size'],\n",
        "):\n",
        "    # 跳过已完成的配置\n",
        "    mask = (\n",
        "        (df_existing['hidden_size'] == hs) &\n",
        "        (df_existing['lr'] == lr) &\n",
        "        (df_existing['weight_decay'] == wd) &\n",
        "        (df_existing['batch_size'] == bs)\n",
        "    )\n",
        "    if mask.any():\n",
        "        print(f\"Skipping existing configuration: hidden_size={hs}, lr={lr}, \"\n",
        "              f\"weight_decay={wd}, batch_size={bs}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Running configuration: hidden_size={hs}, lr={lr}, weight_decay={wd}, batch_size={bs}\")\n",
        "    dataset_r_train, dataset_r_test, dataset_r_validate = rat_data.format_into_datasets(xs_array,ys_array, rnn_utils.DatasetRNN,438, 55, 55,\n",
        "                                                                                           bs, 42)\n",
        "    n_hidden = hs\n",
        "    def make_vanilla_rnn():\n",
        "        model = hk.DeepRNN(\n",
        "            [hk.VanillaRNN(n_hidden), hk.Linear(output_size=2)]\n",
        "        )\n",
        "        return model\n",
        "    \n",
        "    # # set n_step_max for each lr\n",
        "    # if lr == 1e-3:\n",
        "    #     n_step_max = 15000\n",
        "    # elif lr == 1e-4:\n",
        "    #     n_step_max = 50000\n",
        "\n",
        "    # Fit the model\n",
        "    t0 = time.time()\n",
        "    vanillaRNN_params, _, all_losses = rnn_utils.fit_model(\n",
        "        model_fun=make_vanilla_rnn,\n",
        "        dataset_train=dataset_r_train,\n",
        "        dataset_test=dataset_r_validate,\n",
        "        loss_fun='categorical',\n",
        "        optimizer=optax.chain(\n",
        "            optax.add_decayed_weights(wd),  # L2 regularization\n",
        "            optax.adam(learning_rate=lr)    # Adam optimizer\n",
        "        ),\n",
        "        n_steps_per_call=200,\n",
        "        n_steps_max=100000,\n",
        "        return_all_losses=True,\n",
        "        early_stop_step=400,\n",
        "        if_early_stop=True\n",
        "    )\n",
        "\n",
        "    # Extract validation losses (assuming all_losses is a dict with 'test' key)\n",
        "    avg_nll = compute_negative_log_likelihood(dataset_r_validate, make_vanilla_rnn, vanillaRNN_params)\n",
        "\n",
        "    # Record the result\n",
        "    result = {\n",
        "        'hidden_size': hs,\n",
        "        'lr': lr,\n",
        "        'weight_decay': wd,\n",
        "        'batch_size': bs,\n",
        "        'avg_val_nll': avg_nll,\n",
        "        'time': time.time() - t0\n",
        "    }\n",
        "\n",
        "    # 保存当次结果到 CSV\n",
        "    pd.DataFrame([result]).to_csv(\n",
        "        output_path,\n",
        "        mode='a',            # 追加而非覆盖\n",
        "        index=False,\n",
        "        header=write_header  # 只有第一次写入 header\n",
        "    )\n",
        "    write_header = False   # header 只写一次\n",
        "\n",
        "    print(f\"Saved result for hs={hs}, lr={lr}, wd={wd}, bs={bs}, used time {result['time']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    hidden_size      lr  weight_decay  batch_size  avg_val_nll        time\n",
            "0             4  0.0010       0.00100          32     0.363457  388.695748\n",
            "1             4  0.0010       0.00100          64     0.387707  339.140196\n",
            "2             4  0.0010       0.00010          32     0.363530  394.618540\n",
            "3             4  0.0010       0.00010          64     0.387695  353.918199\n",
            "4             4  0.0010       0.00001          32     0.363463  395.954787\n",
            "..          ...     ...           ...         ...          ...         ...\n",
            "67           64  0.0001       0.00100          64     0.381583  123.226999\n",
            "68           64  0.0001       0.00010          32     0.357463  141.919497\n",
            "69           64  0.0001       0.00010          64     0.381908  120.329490\n",
            "70           64  0.0001       0.00001          32     0.358942  120.453786\n",
            "71           64  0.0001       0.00001          64     0.381636  119.434480\n",
            "\n",
            "[72 rows x 6 columns]\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('../Results/mice_grid_search_results.csv')\n",
        "\n",
        "# 打印完整的 DataFrame\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "最优参数： {'hidden_size': 64.0, 'lr': 0.0001, 'weight_decay': 0.0001, 'batch_size': 32.0}\n",
            "对应最小 avg_val_nll: 0.3574631807033438\n"
          ]
        }
      ],
      "source": [
        "# select the best hyperparams\n",
        "# 1. 找到 'avg_val_nll' 最小值所在的行索引\n",
        "best_idx = df['avg_val_nll'].idxmin()\n",
        "# 2. 取出该行\n",
        "best_row = df.loc[best_idx]\n",
        "# 3. 如果你只想要超参（不包括 avg_val_nll 本身），可以这样：\n",
        "best_params = best_row[['hidden_size', 'lr', 'weight_decay', 'batch_size']].to_dict()\n",
        "print(\"最优参数：\", best_params)\n",
        "print(\"对应最小 avg_val_nll:\", best_row['avg_val_nll'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Outer loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Generate outerloop datasets\n",
        "# 这是为了把上面猴子的数据给合并，然后调用我的这个format_into_datasets，生成三个数据集\n",
        "# 假设你已经有三个 np.ndarray：xs1,ys1；xs2,ys2；xs3,ys3\n",
        "\n",
        "folds = rat_data.format_into_datasets_10(\n",
        "    xs_array, ys_array,\n",
        "    dataset_constructor=rnn_utils.DatasetRNN,\n",
        "    n_validate_sessions=55,\n",
        "    batch_size=int(best_params['batch_size']),\n",
        "    random_seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VAu0K2dpwKte",
        "outputId": "e7af12b9-a288-419d-c391-7989822e0867"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Fold 0 ===\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 200 of 200; Loss: 1.9224e+03; Test Loss: 1.2935e+03. (Time: 4.1s)updating best model ..\n",
            "Step 200 of 100000; Loss: 1.6162e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5966e+03; Test Loss: 1.2202e+03. (Time: 4.1s)updating best model ..\n",
            "Step 400 of 100000; Loss: 1.4764e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6547e+03; Test Loss: 1.2070e+03. (Time: 4.2s)updating best model ..\n",
            "Step 600 of 100000; Loss: 1.4513e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7702e+03; Test Loss: 1.1993e+03. (Time: 4.0s)updating best model ..\n",
            "Step 800 of 100000; Loss: 1.4389e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6738e+03; Test Loss: 1.1924e+03. (Time: 4.0s)updating best model ..\n",
            "Step 1000 of 100000; Loss: 1.4287e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1224e+03; Test Loss: 1.1882e+03. (Time: 4.5s)updating best model ..\n",
            "Step 1200 of 100000; Loss: 1.4210e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4626e+03; Test Loss: 1.1837e+03. (Time: 4.7s)updating best model ..\n",
            "Step 1400 of 100000; Loss: 1.4146e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6942e+03; Test Loss: 1.1790e+03. (Time: 4.2s)updating best model ..\n",
            "Step 1600 of 100000; Loss: 1.4083e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4198e+03; Test Loss: 1.1766e+03. (Time: 4.4s)updating best model ..\n",
            "Step 1800 of 100000; Loss: 1.4036e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1486e+03; Test Loss: 1.1741e+03. (Time: 4.3s)updating best model ..\n",
            "Step 2000 of 100000; Loss: 1.4004e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5868e+03; Test Loss: 1.1729e+03. (Time: 4.1s)updating best model ..\n",
            "Step 2200 of 100000; Loss: 1.3978e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4827e+03; Test Loss: 1.1725e+03. (Time: 4.0s)updating best model ..\n",
            "Step 2400 of 100000; Loss: 1.3954e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3759e+03; Test Loss: 1.1712e+03. (Time: 4.1s)updating best model ..\n",
            "Step 2600 of 100000; Loss: 1.3942e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4339e+03; Test Loss: 1.1713e+03. (Time: 4.1s)updating best model ..\n",
            "Step 2800 of 100000; Loss: 1.3929e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6070e+03; Test Loss: 1.1717e+03. (Time: 4.1s)updating best model ..\n",
            "Step 3000 of 100000; Loss: 1.3913e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4191e+03; Test Loss: 1.1684e+03. (Time: 4.2s)updating best model ..\n",
            "Step 3200 of 100000; Loss: 1.3853e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5584e+03; Test Loss: 1.1613e+03. (Time: 4.0s)updating best model ..\n",
            "Step 3400 of 100000; Loss: 1.3802e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6867e+03; Test Loss: 1.1644e+03. (Time: 4.4s)updating best model ..\n",
            "Step 3600 of 100000; Loss: 1.3780e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6160e+03; Test Loss: 1.1637e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3800 of 100000; Loss: 1.3769e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0605e+03; Test Loss: 1.1605e+03. (Time: 4.0s)updating best model ..\n",
            "Step 4000 of 100000; Loss: 1.3757e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4272e+03; Test Loss: 1.1608e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4200 of 100000; Loss: 1.3745e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6574e+03; Test Loss: 1.1612e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4400 of 100000; Loss: 1.3741e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3917e+03; Test Loss: 1.1598e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4600 of 100000; Loss: 1.3733e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1047e+03; Test Loss: 1.1589e+03. (Time: 4.1s)updating best model ..\n",
            "Step 4800 of 100000; Loss: 1.3728e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5420e+03; Test Loss: 1.1605e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5000 of 100000; Loss: 1.3726e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4400e+03; Test Loss: 1.1608e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5200 of 100000; Loss: 1.3723e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3411e+03; Test Loss: 1.1585e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5400 of 100000; Loss: 1.3722e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3945e+03; Test Loss: 1.1596e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5600 of 100000; Loss: 1.3720e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5729e+03; Test Loss: 1.1621e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5800 of 100000; Loss: 1.3720e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4028e+03; Test Loss: 1.1593e+03. (Time: 3.9s)Step 6000 of 100000; Loss: 1.3720e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5453e+03; Test Loss: 1.1593e+03. (Time: 3.9s)Step 6200 of 100000; Loss: 1.3720e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6630e+03; Test Loss: 1.1607e+03. (Time: 4.0s)\n",
            "Stopping early as the loss at step 400 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3986\n",
            "Fold 0 test Avg NLL: 0.3986\n",
            "\n",
            "=== Fold 1 ===\n",
            "Step 200 of 200; Loss: 1.3531e+03; Test Loss: 1.3652e+03. (Time: 4.0s)updating best model ..\n",
            "Step 200 of 100000; Loss: 1.7010e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7221e+03; Test Loss: 1.2416e+03. (Time: 4.3s)updating best model ..\n",
            "Step 400 of 100000; Loss: 1.5129e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5806e+03; Test Loss: 1.2115e+03. (Time: 3.9s)updating best model ..\n",
            "Step 600 of 100000; Loss: 1.4769e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4639e+03; Test Loss: 1.1932e+03. (Time: 3.9s)updating best model ..\n",
            "Step 800 of 100000; Loss: 1.4567e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4973e+03; Test Loss: 1.1801e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1000 of 100000; Loss: 1.4421e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6813e+03; Test Loss: 1.1698e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1200 of 100000; Loss: 1.4318e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4708e+03; Test Loss: 1.1620e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1400 of 100000; Loss: 1.4234e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6034e+03; Test Loss: 1.1543e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1600 of 100000; Loss: 1.4152e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7235e+03; Test Loss: 1.1482e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1800 of 100000; Loss: 1.4094e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6395e+03; Test Loss: 1.1442e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2000 of 100000; Loss: 1.4050e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0918e+03; Test Loss: 1.1404e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2200 of 100000; Loss: 1.4015e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4407e+03; Test Loss: 1.1375e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2400 of 100000; Loss: 1.3993e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6782e+03; Test Loss: 1.1359e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2600 of 100000; Loss: 1.3977e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4070e+03; Test Loss: 1.1344e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2800 of 100000; Loss: 1.3965e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1278e+03; Test Loss: 1.1336e+03. (Time: 4.0s)updating best model ..\n",
            "Step 3000 of 100000; Loss: 1.3951e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5628e+03; Test Loss: 1.1284e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3200 of 100000; Loss: 1.3921e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4570e+03; Test Loss: 1.1228e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3400 of 100000; Loss: 1.3885e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3554e+03; Test Loss: 1.1235e+03. (Time: 4.1s)updating best model ..\n",
            "Step 3600 of 100000; Loss: 1.3869e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4071e+03; Test Loss: 1.1214e+03. (Time: 4.2s)updating best model ..\n",
            "Step 3800 of 100000; Loss: 1.3857e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5818e+03; Test Loss: 1.1188e+03. (Time: 4.0s)updating best model ..\n",
            "Step 4000 of 100000; Loss: 1.3848e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4110e+03; Test Loss: 1.1183e+03. (Time: 4.4s)updating best model ..\n",
            "Step 4200 of 100000; Loss: 1.3839e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5514e+03; Test Loss: 1.1224e+03. (Time: 4.1s)updating best model ..\n",
            "Step 4400 of 100000; Loss: 1.3836e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6731e+03; Test Loss: 1.1180e+03. (Time: 4.5s)updating best model ..\n",
            "Step 4600 of 100000; Loss: 1.3834e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6095e+03; Test Loss: 1.1177e+03. (Time: 4.1s)\n",
            "Stopping early as the loss at step 400 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.4029\n",
            "Fold 1 test Avg NLL: 0.4029\n",
            "\n",
            "=== Fold 2 ===\n",
            "Step 200 of 200; Loss: 1.3530e+03; Test Loss: 1.3659e+03. (Time: 4.3s)updating best model ..\n",
            "Step 200 of 100000; Loss: 1.7014e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7263e+03; Test Loss: 1.2435e+03. (Time: 4.1s)updating best model ..\n",
            "Step 400 of 100000; Loss: 1.5145e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7250e+03; Test Loss: 1.2132e+03. (Time: 4.1s)updating best model ..\n",
            "Step 600 of 100000; Loss: 1.4786e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4696e+03; Test Loss: 1.1948e+03. (Time: 4.2s)updating best model ..\n",
            "Step 800 of 100000; Loss: 1.4580e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4968e+03; Test Loss: 1.1833e+03. (Time: 4.5s)updating best model ..\n",
            "Step 1000 of 100000; Loss: 1.4441e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6817e+03; Test Loss: 1.1718e+03. (Time: 4.3s)updating best model ..\n",
            "Step 1200 of 100000; Loss: 1.4328e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4743e+03; Test Loss: 1.1636e+03. (Time: 4.1s)updating best model ..\n",
            "Step 1400 of 100000; Loss: 1.4239e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6045e+03; Test Loss: 1.1573e+03. (Time: 4.0s)updating best model ..\n",
            "Step 1600 of 100000; Loss: 1.4163e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7231e+03; Test Loss: 1.1508e+03. (Time: 4.0s)updating best model ..\n",
            "Step 1800 of 100000; Loss: 1.4108e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6388e+03; Test Loss: 1.1462e+03. (Time: 4.5s)updating best model ..\n",
            "Step 2000 of 100000; Loss: 1.4061e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0904e+03; Test Loss: 1.1417e+03. (Time: 4.0s)updating best model ..\n",
            "Step 2200 of 100000; Loss: 1.4023e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4414e+03; Test Loss: 1.1389e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2400 of 100000; Loss: 1.3993e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6774e+03; Test Loss: 1.1370e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2600 of 100000; Loss: 1.3973e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6031e+03; Test Loss: 1.1349e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2800 of 100000; Loss: 1.3960e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1293e+03; Test Loss: 1.1357e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3000 of 100000; Loss: 1.3949e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5719e+03; Test Loss: 1.1333e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3200 of 100000; Loss: 1.3938e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6332e+03; Test Loss: 1.1306e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3400 of 100000; Loss: 1.3930e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3619e+03; Test Loss: 1.1281e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3600 of 100000; Loss: 1.3913e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4092e+03; Test Loss: 1.1259e+03. (Time: 4.1s)updating best model ..\n",
            "Step 3800 of 100000; Loss: 1.3877e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5850e+03; Test Loss: 1.1222e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4000 of 100000; Loss: 1.3867e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4132e+03; Test Loss: 1.1208e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4200 of 100000; Loss: 1.3857e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5487e+03; Test Loss: 1.1257e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4400 of 100000; Loss: 1.3852e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6696e+03; Test Loss: 1.1217e+03. (Time: 4.1s)updating best model ..\n",
            "Step 4600 of 100000; Loss: 1.3848e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6130e+03; Test Loss: 1.1191e+03. (Time: 4.0s)updating best model ..\n",
            "Step 4800 of 100000; Loss: 1.3842e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0464e+03; Test Loss: 1.1219e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5000 of 100000; Loss: 1.3839e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4171e+03; Test Loss: 1.1208e+03. (Time: 4.3s)updating best model ..\n",
            "Step 5200 of 100000; Loss: 1.3835e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6504e+03; Test Loss: 1.1192e+03. (Time: 4.5s)updating best model ..\n",
            "Step 5400 of 100000; Loss: 1.3829e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5556e+03; Test Loss: 1.1189e+03. (Time: 4.1s)Step 5600 of 100000; Loss: 1.3832e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1038e+03; Test Loss: 1.1189e+03. (Time: 4.2s)updating best model ..\n",
            "Step 5800 of 100000; Loss: 1.3826e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5335e+03; Test Loss: 1.1172e+03. (Time: 4.0s)\n",
            "Stopping early as the loss at step 400 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3802\n",
            "Fold 2 test Avg NLL: 0.3802\n",
            "\n",
            "=== Fold 3 ===\n",
            "Step 200 of 200; Loss: 1.3543e+03; Test Loss: 1.3672e+03. (Time: 4.0s)updating best model ..\n",
            "Step 200 of 100000; Loss: 1.7030e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7248e+03; Test Loss: 1.2431e+03. (Time: 4.0s)updating best model ..\n",
            "Step 400 of 100000; Loss: 1.5142e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6164e+03; Test Loss: 1.2125e+03. (Time: 4.0s)updating best model ..\n",
            "Step 600 of 100000; Loss: 1.4782e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4669e+03; Test Loss: 1.1953e+03. (Time: 4.1s)updating best model ..\n",
            "Step 800 of 100000; Loss: 1.4583e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4979e+03; Test Loss: 1.1848e+03. (Time: 4.0s)updating best model ..\n",
            "Step 1000 of 100000; Loss: 1.4453e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4750e+03; Test Loss: 1.1750e+03. (Time: 4.0s)updating best model ..\n",
            "Step 1200 of 100000; Loss: 1.4356e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4724e+03; Test Loss: 1.1670e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1400 of 100000; Loss: 1.4270e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6064e+03; Test Loss: 1.1601e+03. (Time: 4.0s)updating best model ..\n",
            "Step 1600 of 100000; Loss: 1.4191e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5212e+03; Test Loss: 1.1535e+03. (Time: 4.1s)updating best model ..\n",
            "Step 1800 of 100000; Loss: 1.4130e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6391e+03; Test Loss: 1.1482e+03. (Time: 4.4s)updating best model ..\n",
            "Step 2000 of 100000; Loss: 1.4074e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0925e+03; Test Loss: 1.1447e+03. (Time: 4.2s)updating best model ..\n",
            "Step 2200 of 100000; Loss: 1.4038e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4429e+03; Test Loss: 1.1422e+03. (Time: 4.4s)updating best model ..\n",
            "Step 2400 of 100000; Loss: 1.4015e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6756e+03; Test Loss: 1.1406e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2600 of 100000; Loss: 1.3996e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6029e+03; Test Loss: 1.1392e+03. (Time: 4.2s)updating best model ..\n",
            "Step 2800 of 100000; Loss: 1.3983e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1290e+03; Test Loss: 1.1384e+03. (Time: 4.1s)updating best model ..\n",
            "Step 3000 of 100000; Loss: 1.3976e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5660e+03; Test Loss: 1.1355e+03. (Time: 4.1s)updating best model ..\n",
            "Step 3200 of 100000; Loss: 1.3961e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5463e+03; Test Loss: 1.1303e+03. (Time: 4.1s)updating best model ..\n",
            "Step 3400 of 100000; Loss: 1.3919e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3565e+03; Test Loss: 1.1274e+03. (Time: 4.3s)updating best model ..\n",
            "Step 3600 of 100000; Loss: 1.3886e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4033e+03; Test Loss: 1.1275e+03. (Time: 4.0s)updating best model ..\n",
            "Step 3800 of 100000; Loss: 1.3874e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4193e+03; Test Loss: 1.1256e+03. (Time: 4.5s)updating best model ..\n",
            "Step 4000 of 100000; Loss: 1.3862e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4099e+03; Test Loss: 1.1232e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4200 of 100000; Loss: 1.3855e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5495e+03; Test Loss: 1.1263e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4400 of 100000; Loss: 1.3851e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4559e+03; Test Loss: 1.1233e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4600 of 100000; Loss: 1.3845e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6095e+03; Test Loss: 1.1220e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4800 of 100000; Loss: 1.3841e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0488e+03; Test Loss: 1.1231e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5000 of 100000; Loss: 1.3839e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4210e+03; Test Loss: 1.1239e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5200 of 100000; Loss: 1.3836e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6489e+03; Test Loss: 1.1213e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5400 of 100000; Loss: 1.3835e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5574e+03; Test Loss: 1.1227e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5600 of 100000; Loss: 1.3832e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0992e+03; Test Loss: 1.1244e+03. (Time: 3.9s)Step 5800 of 100000; Loss: 1.3836e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5402e+03; Test Loss: 1.1231e+03. (Time: 3.9s)\n",
            "Stopping early as the loss at step 400 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3964\n",
            "Fold 3 test Avg NLL: 0.3964\n",
            "\n",
            "=== Fold 4 ===\n",
            "Step 200 of 200; Loss: 1.4906e+03; Test Loss: 1.3671e+03. (Time: 3.9s)updating best model ..\n",
            "Step 200 of 100000; Loss: 1.7039e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7275e+03; Test Loss: 1.2427e+03. (Time: 3.9s)updating best model ..\n",
            "Step 400 of 100000; Loss: 1.5149e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6134e+03; Test Loss: 1.2120e+03. (Time: 3.9s)updating best model ..\n",
            "Step 600 of 100000; Loss: 1.4779e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4680e+03; Test Loss: 1.1933e+03. (Time: 3.9s)updating best model ..\n",
            "Step 800 of 100000; Loss: 1.4568e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4960e+03; Test Loss: 1.1818e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1000 of 100000; Loss: 1.4430e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4704e+03; Test Loss: 1.1708e+03. (Time: 4.4s)updating best model ..\n",
            "Step 1200 of 100000; Loss: 1.4318e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4728e+03; Test Loss: 1.1622e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1400 of 100000; Loss: 1.4231e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6032e+03; Test Loss: 1.1559e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1600 of 100000; Loss: 1.4161e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5280e+03; Test Loss: 1.1489e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1800 of 100000; Loss: 1.4099e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6395e+03; Test Loss: 1.1443e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2000 of 100000; Loss: 1.4045e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0932e+03; Test Loss: 1.1408e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2200 of 100000; Loss: 1.4011e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6343e+03; Test Loss: 1.1379e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2400 of 100000; Loss: 1.3985e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6792e+03; Test Loss: 1.1363e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2600 of 100000; Loss: 1.3967e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6069e+03; Test Loss: 1.1346e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2800 of 100000; Loss: 1.3955e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2701e+03; Test Loss: 1.1341e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3000 of 100000; Loss: 1.3946e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5733e+03; Test Loss: 1.1335e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3200 of 100000; Loss: 1.3939e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5513e+03; Test Loss: 1.1309e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3400 of 100000; Loss: 1.3925e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3587e+03; Test Loss: 1.1264e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3600 of 100000; Loss: 1.3892e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4042e+03; Test Loss: 1.1207e+03. (Time: 4.0s)updating best model ..\n",
            "Step 3800 of 100000; Loss: 1.3863e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4208e+03; Test Loss: 1.1186e+03. (Time: 4.0s)updating best model ..\n",
            "Step 4000 of 100000; Loss: 1.3850e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4154e+03; Test Loss: 1.1177e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4200 of 100000; Loss: 1.3840e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5484e+03; Test Loss: 1.1196e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4400 of 100000; Loss: 1.3833e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4676e+03; Test Loss: 1.1173e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4600 of 100000; Loss: 1.3827e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6164e+03; Test Loss: 1.1164e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4800 of 100000; Loss: 1.3822e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0527e+03; Test Loss: 1.1177e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5000 of 100000; Loss: 1.3816e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5846e+03; Test Loss: 1.1176e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5200 of 100000; Loss: 1.3815e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6498e+03; Test Loss: 1.1159e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5400 of 100000; Loss: 1.3809e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5646e+03; Test Loss: 1.1178e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5600 of 100000; Loss: 1.3808e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2451e+03; Test Loss: 1.1170e+03. (Time: 4.3s)updating best model ..\n",
            "Step 5800 of 100000; Loss: 1.3805e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5433e+03; Test Loss: 1.1156e+03. (Time: 3.9s)updating best model ..\n",
            "Step 6000 of 100000; Loss: 1.3802e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5288e+03; Test Loss: 1.1196e+03. (Time: 3.9s)updating best model ..\n",
            "Step 6200 of 100000; Loss: 1.3801e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3371e+03; Test Loss: 1.1166e+03. (Time: 3.9s)updating best model ..\n",
            "Step 6400 of 100000; Loss: 1.3797e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3834e+03; Test Loss: 1.1150e+03. (Time: 3.9s)updating best model ..\n",
            "Step 6600 of 100000; Loss: 1.3796e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4100e+03; Test Loss: 1.1155e+03. (Time: 3.9s)updating best model ..\n",
            "Step 6800 of 100000; Loss: 1.3795e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4035e+03; Test Loss: 1.1141e+03. (Time: 4.0s)updating best model ..\n",
            "Step 7000 of 100000; Loss: 1.3795e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5342e+03; Test Loss: 1.1174e+03. (Time: 4.0s)updating best model ..\n",
            "Step 7200 of 100000; Loss: 1.3795e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4538e+03; Test Loss: 1.1140e+03. (Time: 3.9s)updating best model ..\n",
            "Step 7400 of 100000; Loss: 1.3792e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6046e+03; Test Loss: 1.1134e+03. (Time: 3.9s)updating best model ..\n",
            "Step 7600 of 100000; Loss: 1.3790e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0402e+03; Test Loss: 1.1171e+03. (Time: 3.9s)Step 7800 of 100000; Loss: 1.3790e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5699e+03; Test Loss: 1.1147e+03. (Time: 3.9s)updating best model ..\n",
            "Step 8000 of 100000; Loss: 1.3788e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6405e+03; Test Loss: 1.1135e+03. (Time: 3.9s)updating best model ..\n",
            "Step 8200 of 100000; Loss: 1.3787e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5548e+03; Test Loss: 1.1160e+03. (Time: 3.9s)updating best model ..\n",
            "Step 8400 of 100000; Loss: 1.3787e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2376e+03; Test Loss: 1.1167e+03. (Time: 4.0s)updating best model ..\n",
            "Step 8600 of 100000; Loss: 1.3785e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5333e+03; Test Loss: 1.1133e+03. (Time: 3.9s)Step 8800 of 100000; Loss: 1.3785e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5190e+03; Test Loss: 1.1156e+03. (Time: 3.9s)updating best model ..\n",
            "Step 9000 of 100000; Loss: 1.3782e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3281e+03; Test Loss: 1.1146e+03. (Time: 3.9s)updating best model ..\n",
            "Step 9200 of 100000; Loss: 1.3781e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3713e+03; Test Loss: 1.1136e+03. (Time: 3.9s)Step 9400 of 100000; Loss: 1.3782e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4019e+03; Test Loss: 1.1153e+03. (Time: 3.9s)\n",
            "Stopping early as the loss at step 400 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3867\n",
            "Fold 4 test Avg NLL: 0.3867\n",
            "\n",
            "=== Fold 5 ===\n",
            "Step 200 of 200; Loss: 1.8199e+03; Test Loss: 1.3672e+03. (Time: 3.9s)updating best model ..\n",
            "Step 200 of 100000; Loss: 1.7046e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7267e+03; Test Loss: 1.2443e+03. (Time: 4.0s)updating best model ..\n",
            "Step 400 of 100000; Loss: 1.5155e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6169e+03; Test Loss: 1.2137e+03. (Time: 3.9s)updating best model ..\n",
            "Step 600 of 100000; Loss: 1.4791e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5556e+03; Test Loss: 1.1959e+03. (Time: 3.9s)updating best model ..\n",
            "Step 800 of 100000; Loss: 1.4588e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4997e+03; Test Loss: 1.1845e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1000 of 100000; Loss: 1.4448e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4737e+03; Test Loss: 1.1738e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1200 of 100000; Loss: 1.4344e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4746e+03; Test Loss: 1.1651e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1400 of 100000; Loss: 1.4251e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6038e+03; Test Loss: 1.1586e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1600 of 100000; Loss: 1.4176e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5290e+03; Test Loss: 1.1516e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1800 of 100000; Loss: 1.4116e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6385e+03; Test Loss: 1.1474e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2000 of 100000; Loss: 1.4067e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0936e+03; Test Loss: 1.1444e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2200 of 100000; Loss: 1.4037e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6387e+03; Test Loss: 1.1411e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2400 of 100000; Loss: 1.4013e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6796e+03; Test Loss: 1.1397e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2600 of 100000; Loss: 1.3993e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6057e+03; Test Loss: 1.1377e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2800 of 100000; Loss: 1.3979e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5712e+03; Test Loss: 1.1366e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3000 of 100000; Loss: 1.3967e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5744e+03; Test Loss: 1.1363e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3200 of 100000; Loss: 1.3959e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5520e+03; Test Loss: 1.1329e+03. (Time: 4.0s)updating best model ..\n",
            "Step 3400 of 100000; Loss: 1.3949e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4764e+03; Test Loss: 1.1280e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3600 of 100000; Loss: 1.3915e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4089e+03; Test Loss: 1.1254e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3800 of 100000; Loss: 1.3876e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4218e+03; Test Loss: 1.1220e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4000 of 100000; Loss: 1.3857e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4169e+03; Test Loss: 1.1207e+03. (Time: 4.3s)updating best model ..\n",
            "Step 4200 of 100000; Loss: 1.3839e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5508e+03; Test Loss: 1.1223e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4400 of 100000; Loss: 1.3835e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4689e+03; Test Loss: 1.1196e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4600 of 100000; Loss: 1.3829e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6085e+03; Test Loss: 1.1187e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4800 of 100000; Loss: 1.3827e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0495e+03; Test Loss: 1.1211e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5000 of 100000; Loss: 1.3822e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5860e+03; Test Loss: 1.1179e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5200 of 100000; Loss: 1.3820e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6519e+03; Test Loss: 1.1182e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5400 of 100000; Loss: 1.3814e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5570e+03; Test Loss: 1.1206e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5600 of 100000; Loss: 1.3812e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5370e+03; Test Loss: 1.1170e+03. (Time: 3.9s)Step 5800 of 100000; Loss: 1.3812e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5387e+03; Test Loss: 1.1182e+03. (Time: 3.9s)Step 6000 of 100000; Loss: 1.3812e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5164e+03; Test Loss: 1.1182e+03. (Time: 3.9s)Step 6200 of 100000; Loss: 1.3817e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4568e+03; Test Loss: 1.1208e+03. (Time: 4.0s)\n",
            "Stopping early as the loss at step 400 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3826\n",
            "Fold 5 test Avg NLL: 0.3826\n",
            "\n",
            "=== Fold 6 ===\n",
            "Step 200 of 200; Loss: 1.8210e+03; Test Loss: 1.3678e+03. (Time: 3.9s)updating best model ..\n",
            "Step 200 of 100000; Loss: 1.7042e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7271e+03; Test Loss: 1.2432e+03. (Time: 3.9s)updating best model ..\n",
            "Step 400 of 100000; Loss: 1.5152e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6154e+03; Test Loss: 1.2129e+03. (Time: 3.9s)updating best model ..\n",
            "Step 600 of 100000; Loss: 1.4793e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4435e+03; Test Loss: 1.1961e+03. (Time: 3.9s)updating best model ..\n",
            "Step 800 of 100000; Loss: 1.4590e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4971e+03; Test Loss: 1.1832e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1000 of 100000; Loss: 1.4444e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4714e+03; Test Loss: 1.1710e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1200 of 100000; Loss: 1.4320e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3511e+03; Test Loss: 1.1625e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1400 of 100000; Loss: 1.4225e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6019e+03; Test Loss: 1.1556e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1600 of 100000; Loss: 1.4153e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5245e+03; Test Loss: 1.1497e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1800 of 100000; Loss: 1.4098e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3233e+03; Test Loss: 1.1460e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2000 of 100000; Loss: 1.4055e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0898e+03; Test Loss: 1.1418e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2200 of 100000; Loss: 1.4021e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6347e+03; Test Loss: 1.1385e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2400 of 100000; Loss: 1.3992e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6795e+03; Test Loss: 1.1372e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2600 of 100000; Loss: 1.3972e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6065e+03; Test Loss: 1.1352e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2800 of 100000; Loss: 1.3958e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5686e+03; Test Loss: 1.1343e+03. (Time: 4.0s)updating best model ..\n",
            "Step 3000 of 100000; Loss: 1.3947e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5756e+03; Test Loss: 1.1340e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3200 of 100000; Loss: 1.3939e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5541e+03; Test Loss: 1.1320e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3400 of 100000; Loss: 1.3931e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3761e+03; Test Loss: 1.1318e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3600 of 100000; Loss: 1.3921e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4112e+03; Test Loss: 1.1269e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3800 of 100000; Loss: 1.3912e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4242e+03; Test Loss: 1.1248e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4000 of 100000; Loss: 1.3878e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2973e+03; Test Loss: 1.1224e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4200 of 100000; Loss: 1.3856e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5468e+03; Test Loss: 1.1203e+03. (Time: 4.0s)updating best model ..\n",
            "Step 4400 of 100000; Loss: 1.3844e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4700e+03; Test Loss: 1.1198e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4600 of 100000; Loss: 1.3833e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2869e+03; Test Loss: 1.1200e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4800 of 100000; Loss: 1.3828e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0451e+03; Test Loss: 1.1171e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5000 of 100000; Loss: 1.3823e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5794e+03; Test Loss: 1.1184e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5200 of 100000; Loss: 1.3817e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6478e+03; Test Loss: 1.1204e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5400 of 100000; Loss: 1.3815e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5649e+03; Test Loss: 1.1156e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5600 of 100000; Loss: 1.3812e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5345e+03; Test Loss: 1.1155e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5800 of 100000; Loss: 1.3808e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5442e+03; Test Loss: 1.1163e+03. (Time: 4.0s)updating best model ..\n",
            "Step 6000 of 100000; Loss: 1.3805e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5248e+03; Test Loss: 1.1144e+03. (Time: 3.9s)updating best model ..\n",
            "Step 6200 of 100000; Loss: 1.3803e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3605e+03; Test Loss: 1.1148e+03. (Time: 4.3s)updating best model ..\n",
            "Step 6400 of 100000; Loss: 1.3798e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3801e+03; Test Loss: 1.1139e+03. (Time: 4.0s)updating best model ..\n",
            "Step 6600 of 100000; Loss: 1.3797e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4084e+03; Test Loss: 1.1138e+03. (Time: 3.9s)updating best model ..\n",
            "Step 6800 of 100000; Loss: 1.3797e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2822e+03; Test Loss: 1.1147e+03. (Time: 3.9s)Step 7000 of 100000; Loss: 1.3799e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5310e+03; Test Loss: 1.1135e+03. (Time: 3.9s)\n",
            "Stopping early as the loss at step 400 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3437\n",
            "Fold 6 test Avg NLL: 0.3437\n",
            "\n",
            "=== Fold 7 ===\n",
            "Step 200 of 200; Loss: 1.8204e+03; Test Loss: 1.3690e+03. (Time: 3.9s)updating best model ..\n",
            "Step 200 of 100000; Loss: 1.7059e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7493e+03; Test Loss: 1.2449e+03. (Time: 3.9s)updating best model ..\n",
            "Step 400 of 100000; Loss: 1.5161e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6160e+03; Test Loss: 1.2142e+03. (Time: 3.9s)updating best model ..\n",
            "Step 600 of 100000; Loss: 1.4786e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4412e+03; Test Loss: 1.1958e+03. (Time: 3.9s)updating best model ..\n",
            "Step 800 of 100000; Loss: 1.4576e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4989e+03; Test Loss: 1.1835e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1000 of 100000; Loss: 1.4436e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4717e+03; Test Loss: 1.1726e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1200 of 100000; Loss: 1.4323e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3533e+03; Test Loss: 1.1643e+03. (Time: 4.3s)updating best model ..\n",
            "Step 1400 of 100000; Loss: 1.4236e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6038e+03; Test Loss: 1.1565e+03. (Time: 4.3s)updating best model ..\n",
            "Step 1600 of 100000; Loss: 1.4157e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5274e+03; Test Loss: 1.1495e+03. (Time: 4.1s)updating best model ..\n",
            "Step 1800 of 100000; Loss: 1.4094e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3274e+03; Test Loss: 1.1453e+03. (Time: 4.0s)updating best model ..\n",
            "Step 2000 of 100000; Loss: 1.4044e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0905e+03; Test Loss: 1.1413e+03. (Time: 4.2s)updating best model ..\n",
            "Step 2200 of 100000; Loss: 1.4010e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6333e+03; Test Loss: 1.1381e+03. (Time: 4.3s)updating best model ..\n",
            "Step 2400 of 100000; Loss: 1.3987e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5362e+03; Test Loss: 1.1369e+03. (Time: 4.6s)updating best model ..\n",
            "Step 2600 of 100000; Loss: 1.3967e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6047e+03; Test Loss: 1.1345e+03. (Time: 4.2s)updating best model ..\n",
            "Step 2800 of 100000; Loss: 1.3951e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5666e+03; Test Loss: 1.1337e+03. (Time: 4.6s)updating best model ..\n",
            "Step 3000 of 100000; Loss: 1.3941e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5713e+03; Test Loss: 1.1322e+03. (Time: 4.3s)updating best model ..\n",
            "Step 3200 of 100000; Loss: 1.3934e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5460e+03; Test Loss: 1.1277e+03. (Time: 4.3s)updating best model ..\n",
            "Step 3400 of 100000; Loss: 1.3898e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3763e+03; Test Loss: 1.1222e+03. (Time: 4.4s)updating best model ..\n",
            "Step 3600 of 100000; Loss: 1.3864e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4065e+03; Test Loss: 1.1193e+03. (Time: 4.4s)updating best model ..\n",
            "Step 3800 of 100000; Loss: 1.3841e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4185e+03; Test Loss: 1.1225e+03. (Time: 4.1s)updating best model ..\n",
            "Step 4000 of 100000; Loss: 1.3833e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2872e+03; Test Loss: 1.1203e+03. (Time: 4.1s)updating best model ..\n",
            "Step 4200 of 100000; Loss: 1.3825e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5456e+03; Test Loss: 1.1196e+03. (Time: 4.1s)updating best model ..\n",
            "Step 4400 of 100000; Loss: 1.3822e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4669e+03; Test Loss: 1.1191e+03. (Time: 4.1s)updating best model ..\n",
            "Step 4600 of 100000; Loss: 1.3814e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2658e+03; Test Loss: 1.1200e+03. (Time: 4.0s)\n",
            "Stopping early as the loss at step 400 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.4211\n",
            "Fold 7 test Avg NLL: 0.4211\n",
            "\n",
            "=== Fold 8 ===\n",
            "Step 200 of 200; Loss: 1.8217e+03; Test Loss: 1.3682e+03. (Time: 4.2s)updating best model ..\n",
            "Step 200 of 100000; Loss: 1.7040e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7164e+03; Test Loss: 1.2441e+03. (Time: 4.3s)updating best model ..\n",
            "Step 400 of 100000; Loss: 1.5155e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6158e+03; Test Loss: 1.2142e+03. (Time: 4.1s)updating best model ..\n",
            "Step 600 of 100000; Loss: 1.4792e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4406e+03; Test Loss: 1.1964e+03. (Time: 4.5s)updating best model ..\n",
            "Step 800 of 100000; Loss: 1.4586e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7658e+03; Test Loss: 1.1841e+03. (Time: 4.1s)updating best model ..\n",
            "Step 1000 of 100000; Loss: 1.4445e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4728e+03; Test Loss: 1.1738e+03. (Time: 4.0s)updating best model ..\n",
            "Step 1200 of 100000; Loss: 1.4342e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3541e+03; Test Loss: 1.1660e+03. (Time: 4.0s)updating best model ..\n",
            "Step 1400 of 100000; Loss: 1.4255e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5538e+03; Test Loss: 1.1583e+03. (Time: 4.4s)updating best model ..\n",
            "Step 1600 of 100000; Loss: 1.4177e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5303e+03; Test Loss: 1.1518e+03. (Time: 4.3s)updating best model ..\n",
            "Step 1800 of 100000; Loss: 1.4115e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3342e+03; Test Loss: 1.1478e+03. (Time: 4.5s)updating best model ..\n",
            "Step 2000 of 100000; Loss: 1.4070e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1617e+03; Test Loss: 1.1437e+03. (Time: 4.1s)updating best model ..\n",
            "Step 2200 of 100000; Loss: 1.4034e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6365e+03; Test Loss: 1.1405e+03. (Time: 4.1s)updating best model ..\n",
            "Step 2400 of 100000; Loss: 1.4006e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5391e+03; Test Loss: 1.1386e+03. (Time: 4.2s)updating best model ..\n",
            "Step 2600 of 100000; Loss: 1.3986e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6075e+03; Test Loss: 1.1363e+03. (Time: 4.0s)updating best model ..\n",
            "Step 2800 of 100000; Loss: 1.3973e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5719e+03; Test Loss: 1.1353e+03. (Time: 4.1s)updating best model ..\n",
            "Step 3000 of 100000; Loss: 1.3961e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6204e+03; Test Loss: 1.1347e+03. (Time: 4.2s)updating best model ..\n",
            "Step 3200 of 100000; Loss: 1.3950e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5586e+03; Test Loss: 1.1333e+03. (Time: 4.0s)updating best model ..\n",
            "Step 3400 of 100000; Loss: 1.3942e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3746e+03; Test Loss: 1.1339e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3600 of 100000; Loss: 1.3936e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7167e+03; Test Loss: 1.1320e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3800 of 100000; Loss: 1.3931e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4301e+03; Test Loss: 1.1318e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4000 of 100000; Loss: 1.3921e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2959e+03; Test Loss: 1.1262e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4200 of 100000; Loss: 1.3900e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5082e+03; Test Loss: 1.1251e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4400 of 100000; Loss: 1.3877e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4738e+03; Test Loss: 1.1229e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4600 of 100000; Loss: 1.3864e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2764e+03; Test Loss: 1.1230e+03. (Time: 4.0s)updating best model ..\n",
            "Step 4800 of 100000; Loss: 1.3853e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1179e+03; Test Loss: 1.1225e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5000 of 100000; Loss: 1.3846e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5819e+03; Test Loss: 1.1207e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5200 of 100000; Loss: 1.3841e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5053e+03; Test Loss: 1.1203e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5400 of 100000; Loss: 1.3836e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5626e+03; Test Loss: 1.1217e+03. (Time: 4.4s)Step 5600 of 100000; Loss: 1.3836e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5424e+03; Test Loss: 1.1201e+03. (Time: 4.0s)updating best model ..\n",
            "Step 5800 of 100000; Loss: 1.3833e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6078e+03; Test Loss: 1.1204e+03. (Time: 3.9s)updating best model ..\n",
            "Step 6000 of 100000; Loss: 1.3831e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5280e+03; Test Loss: 1.1224e+03. (Time: 4.1s)updating best model ..\n",
            "Step 6200 of 100000; Loss: 1.3830e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3597e+03; Test Loss: 1.1197e+03. (Time: 3.9s)\n",
            "Stopping early as the loss at step 400 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.3661\n",
            "Fold 8 test Avg NLL: 0.3661\n",
            "\n",
            "=== Fold 9 ===\n",
            "Step 200 of 200; Loss: 1.8249e+03; Test Loss: 1.3727e+03. (Time: 3.9s)updating best model ..\n",
            "Step 200 of 100000; Loss: 1.7106e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7185e+03; Test Loss: 1.2464e+03. (Time: 3.9s)updating best model ..\n",
            "Step 400 of 100000; Loss: 1.5173e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6166e+03; Test Loss: 1.2148e+03. (Time: 3.9s)updating best model ..\n",
            "Step 600 of 100000; Loss: 1.4796e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4422e+03; Test Loss: 1.1966e+03. (Time: 3.9s)updating best model ..\n",
            "Step 800 of 100000; Loss: 1.4587e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7648e+03; Test Loss: 1.1836e+03. (Time: 4.0s)updating best model ..\n",
            "Step 1000 of 100000; Loss: 1.4443e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4734e+03; Test Loss: 1.1729e+03. (Time: 4.0s)updating best model ..\n",
            "Step 1200 of 100000; Loss: 1.4332e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3525e+03; Test Loss: 1.1648e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1400 of 100000; Loss: 1.4244e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4822e+03; Test Loss: 1.1574e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1600 of 100000; Loss: 1.4169e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5278e+03; Test Loss: 1.1502e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1800 of 100000; Loss: 1.4103e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3325e+03; Test Loss: 1.1467e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2000 of 100000; Loss: 1.4061e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0426e+03; Test Loss: 1.1428e+03. (Time: 4.1s)updating best model ..\n",
            "Step 2200 of 100000; Loss: 1.4029e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6355e+03; Test Loss: 1.1399e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2400 of 100000; Loss: 1.4005e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5337e+03; Test Loss: 1.1388e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2600 of 100000; Loss: 1.3987e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6042e+03; Test Loss: 1.1369e+03. (Time: 3.9s)updating best model ..\n",
            "Step 2800 of 100000; Loss: 1.3973e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5698e+03; Test Loss: 1.1372e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3000 of 100000; Loss: 1.3958e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6152e+03; Test Loss: 1.1321e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3200 of 100000; Loss: 1.3943e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5393e+03; Test Loss: 1.1361e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3400 of 100000; Loss: 1.3905e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3721e+03; Test Loss: 1.1252e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3600 of 100000; Loss: 1.3883e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7075e+03; Test Loss: 1.1262e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3800 of 100000; Loss: 1.3873e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4193e+03; Test Loss: 1.1269e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4000 of 100000; Loss: 1.3864e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2903e+03; Test Loss: 1.1246e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4200 of 100000; Loss: 1.3860e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4137e+03; Test Loss: 1.1244e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4400 of 100000; Loss: 1.3856e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4712e+03; Test Loss: 1.1231e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4600 of 100000; Loss: 1.3853e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2692e+03; Test Loss: 1.1239e+03. (Time: 3.9s)updating best model ..\n",
            "Step 4800 of 100000; Loss: 1.3848e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0004e+03; Test Loss: 1.1235e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5000 of 100000; Loss: 1.3847e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5846e+03; Test Loss: 1.1222e+03. (Time: 3.9s)updating best model ..\n",
            "Step 5200 of 100000; Loss: 1.3843e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5043e+03; Test Loss: 1.1230e+03. (Time: 4.0s)updating best model ..\n",
            "Step 5400 of 100000; Loss: 1.3841e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5598e+03; Test Loss: 1.1237e+03. (Time: 4.3s)updating best model ..\n",
            "Step 5600 of 100000; Loss: 1.3839e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5361e+03; Test Loss: 1.1224e+03. (Time: 4.0s)updating best model ..\n",
            "Step 5800 of 100000; Loss: 1.3838e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6052e+03; Test Loss: 1.1239e+03. (Time: 3.9s)\n",
            "Stopping early as the loss at step 400 did not improve over step 1.\n",
            "Average Negative Log-Likelihood: 0.4052\n",
            "Fold 9 test Avg NLL: 0.4052\n",
            "\n",
            "All folds Avg NLL: [0.39855401 0.40292565 0.38018771 0.39644519 0.38666687 0.3826156\n",
            " 0.34369084 0.42111904 0.36613102 0.40523097]\n",
            "Mean NLL over folds: 0.3884\n",
            "Stddev over folds : 0.0219\n",
            "Std. Error (SE)    : 0.0069\n"
          ]
        }
      ],
      "source": [
        "## Define the best Vanilla RNN with 32 units\n",
        "n_hidden = int(best_params['hidden_size'])\n",
        "def make_vanilla_rnn():\n",
        "    model = hk.DeepRNN(\n",
        "        [hk.VanillaRNN(n_hidden), hk.Linear(output_size=2)]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "n_folds = len(folds)\n",
        "avg_nlls = np.zeros(n_folds)\n",
        "best_models = []\n",
        "\n",
        "# # set n_step_max for each lr\n",
        "# if best_params['lr'] == 1e-3:\n",
        "#     n_step_max = 15000\n",
        "# elif best_params['lr'] == 1e-4:\n",
        "#     n_step_max = 50000\n",
        "\n",
        "for i, (train_ds, val_ds, test_ds) in enumerate(folds):\n",
        "    print(f\"=== Fold {i} ===\")\n",
        "    # 用 train/val 训练并选超参\n",
        "    params, _ = rnn_utils.fit_model(\n",
        "        model_fun=make_vanilla_rnn,\n",
        "        dataset_train=train_ds,\n",
        "        dataset_test=val_ds,      # 用 val_ds 做 early-stop\n",
        "        optimizer=optax.chain(\n",
        "            optax.add_decayed_weights(best_params['weight_decay']),\n",
        "            optax.adam(learning_rate=best_params['lr'])\n",
        "        ),\n",
        "        n_steps_per_call=200,\n",
        "        n_steps_max=100000,\n",
        "        early_stop_step=400,\n",
        "        if_early_stop=True\n",
        "    )\n",
        "\n",
        "    # 在 test_ds 上计算平均 NLL\n",
        "    avg_nll = compute_negative_log_likelihood(test_ds, make_vanilla_rnn, params)\n",
        "    print(f\"Fold {i} test Avg NLL: {avg_nll:.4f}\\n\")\n",
        "    avg_nlls[i] = avg_nll\n",
        "    best_models.append(params)\n",
        "\n",
        "# 汇总\n",
        "mean_nll = avg_nlls.mean()\n",
        "std_nll  = avg_nlls.std(ddof=1)    # 样本标准差\n",
        "se_nll   = std_nll / np.sqrt(n_folds)\n",
        "\n",
        "print(\"All folds Avg NLL:\", avg_nlls)\n",
        "print(f\"Mean NLL over folds: {mean_nll:.4f}\")\n",
        "print(f\"Stddev over folds : {std_nll:.4f}\")\n",
        "print(f\"Std. Error (SE)    : {se_nll:.4f}\")\n",
        "\n",
        "# save the results\n",
        "import pickle\n",
        "best_dict = {'best_params': best_params, 'best_models': best_models, 'NLLs': avg_nlls}\n",
        "with open('../Results/mice_best_res.pkl', 'wb') as f:\n",
        "    pickle.dump(best_dict, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "for RNN\n",
            "All folds Avg NLL: [0.39855401 0.40292565 0.38018771 0.39644519 0.38666687 0.3826156\n",
            " 0.34369084 0.42111904 0.36613102 0.40523097]\n",
            "Mean NLL over folds: 0.3884\n",
            "Stddev over folds : 0.0219\n",
            "Std. Error (SE)    : 0.0069\n"
          ]
        }
      ],
      "source": [
        "print('for RNN')\n",
        "print(\"All folds Avg NLL:\", avg_nlls)\n",
        "print(f\"Mean NLL over folds: {mean_nll:.4f}\")\n",
        "print(f\"Stddev over folds : {std_nll:.4f}\")\n",
        "print(f\"Std. Error (SE)    : {se_nll:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpQo1mfmz0Yb",
        "outputId": "369939a7-ced2-495d-e2c1-6c670239cad4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Fold 0 ===\n",
            "Step 200 of 200; Loss: 2.4714e+03; Test Loss: 1.5477e+03. (Time: 2.5s)updating best model ..\n",
            "Step 200 of 1000000; Loss: 1.9266e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1752e+03; Test Loss: 1.5344e+03. (Time: 2.5s)updating best model ..\n",
            "Step 400 of 1000000; Loss: 1.9086e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7023e+03; Test Loss: 1.5215e+03. (Time: 2.4s)updating best model ..\n",
            "Step 600 of 1000000; Loss: 1.8912e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3715e+03; Test Loss: 1.5092e+03. (Time: 2.4s)updating best model ..\n",
            "Step 800 of 1000000; Loss: 1.8745e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1609e+03; Test Loss: 1.4973e+03. (Time: 2.4s)updating best model ..\n",
            "Step 1000 of 1000000; Loss: 1.8583e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1585e+03; Test Loss: 1.4858e+03. (Time: 2.4s)updating best model ..\n",
            "Step 1200 of 1000000; Loss: 1.8426e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9514e+03; Test Loss: 1.4747e+03. (Time: 2.4s)updating best model ..\n",
            "Step 1400 of 1000000; Loss: 1.8275e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1544e+03; Test Loss: 1.4640e+03. (Time: 2.4s)updating best model ..\n",
            "Step 1600 of 1000000; Loss: 1.8128e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1825e+03; Test Loss: 1.4536e+03. (Time: 2.4s)updating best model ..\n",
            "Step 1800 of 1000000; Loss: 1.7985e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1594e+03; Test Loss: 1.4436e+03. (Time: 2.4s)updating best model ..\n",
            "Step 2000 of 1000000; Loss: 1.7848e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1864e+03; Test Loss: 1.4339e+03. (Time: 2.4s)updating best model ..\n",
            "Step 2200 of 1000000; Loss: 1.7714e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0569e+03; Test Loss: 1.4245e+03. (Time: 2.4s)updating best model ..\n",
            "Step 2400 of 1000000; Loss: 1.7585e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5533e+03; Test Loss: 1.4154e+03. (Time: 2.4s)updating best model ..\n",
            "Step 2600 of 1000000; Loss: 1.7459e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9321e+03; Test Loss: 1.4067e+03. (Time: 2.4s)updating best model ..\n",
            "Step 2800 of 1000000; Loss: 1.7338e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2043e+03; Test Loss: 1.3982e+03. (Time: 2.4s)updating best model ..\n",
            "Step 3000 of 1000000; Loss: 1.7220e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9173e+03; Test Loss: 1.3900e+03. (Time: 2.4s)updating best model ..\n",
            "Step 3200 of 1000000; Loss: 1.7105e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5122e+03; Test Loss: 1.3821e+03. (Time: 2.4s)updating best model ..\n",
            "Step 3400 of 1000000; Loss: 1.6995e+03. (Time: 0.0s)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 200 of 200; Loss: 2.1004e+03; Test Loss: 1.3744e+03. (Time: 2.9s)updating best model ..\n",
            "Step 3600 of 1000000; Loss: 1.6887e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9142e+03; Test Loss: 1.3670e+03. (Time: 2.4s)updating best model ..\n",
            "Step 3800 of 1000000; Loss: 1.6783e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8957e+03; Test Loss: 1.3598e+03. (Time: 2.4s)updating best model ..\n",
            "Step 4000 of 1000000; Loss: 1.6682e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7546e+03; Test Loss: 1.3529e+03. (Time: 2.4s)updating best model ..\n",
            "Step 4200 of 1000000; Loss: 1.6584e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9441e+03; Test Loss: 1.3462e+03. (Time: 2.4s)updating best model ..\n",
            "Step 4400 of 1000000; Loss: 1.6489e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9228e+03; Test Loss: 1.3397e+03. (Time: 2.4s)updating best model ..\n",
            "Step 4600 of 1000000; Loss: 1.6397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9645e+03; Test Loss: 1.3335e+03. (Time: 2.4s)updating best model ..\n",
            "Step 4800 of 1000000; Loss: 1.6308e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9996e+03; Test Loss: 1.3274e+03. (Time: 2.4s)updating best model ..\n",
            "Step 5000 of 1000000; Loss: 1.6222e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9026e+03; Test Loss: 1.3216e+03. (Time: 2.4s)updating best model ..\n",
            "Step 5200 of 1000000; Loss: 1.6139e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3871e+03; Test Loss: 1.3160e+03. (Time: 2.4s)updating best model ..\n",
            "Step 5400 of 1000000; Loss: 1.6058e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7626e+03; Test Loss: 1.3106e+03. (Time: 2.4s)updating best model ..\n",
            "Step 5600 of 1000000; Loss: 1.5980e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.0251e+03; Test Loss: 1.3053e+03. (Time: 2.4s)updating best model ..\n",
            "Step 5800 of 1000000; Loss: 1.5904e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7403e+03; Test Loss: 1.3003e+03. (Time: 2.4s)updating best model ..\n",
            "Step 6000 of 1000000; Loss: 1.5831e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3857e+03; Test Loss: 1.2955e+03. (Time: 2.4s)updating best model ..\n",
            "Step 6200 of 1000000; Loss: 1.5761e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9152e+03; Test Loss: 1.2908e+03. (Time: 2.4s)updating best model ..\n",
            "Step 6400 of 1000000; Loss: 1.5693e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7492e+03; Test Loss: 1.2863e+03. (Time: 2.4s)updating best model ..\n",
            "Step 6600 of 1000000; Loss: 1.5627e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7149e+03; Test Loss: 1.2820e+03. (Time: 2.4s)updating best model ..\n",
            "Step 6800 of 1000000; Loss: 1.5563e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6283e+03; Test Loss: 1.2778e+03. (Time: 2.4s)updating best model ..\n",
            "Step 7000 of 1000000; Loss: 1.5502e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8114e+03; Test Loss: 1.2738e+03. (Time: 2.4s)updating best model ..\n",
            "Step 7200 of 1000000; Loss: 1.5443e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7438e+03; Test Loss: 1.2700e+03. (Time: 2.4s)updating best model ..\n",
            "Step 7400 of 1000000; Loss: 1.5386e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8342e+03; Test Loss: 1.2663e+03. (Time: 2.4s)updating best model ..\n",
            "Step 7600 of 1000000; Loss: 1.5331e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8809e+03; Test Loss: 1.2627e+03. (Time: 2.4s)updating best model ..\n",
            "Step 7800 of 1000000; Loss: 1.5278e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8032e+03; Test Loss: 1.2593e+03. (Time: 2.4s)updating best model ..\n",
            "Step 8000 of 1000000; Loss: 1.5227e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2741e+03; Test Loss: 1.2560e+03. (Time: 2.4s)updating best model ..\n",
            "Step 8200 of 1000000; Loss: 1.5178e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6486e+03; Test Loss: 1.2529e+03. (Time: 2.4s)updating best model ..\n",
            "Step 8400 of 1000000; Loss: 1.5130e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9036e+03; Test Loss: 1.2499e+03. (Time: 2.4s)updating best model ..\n",
            "Step 8600 of 1000000; Loss: 1.5085e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6199e+03; Test Loss: 1.2470e+03. (Time: 2.4s)updating best model ..\n",
            "Step 8800 of 1000000; Loss: 1.5041e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3030e+03; Test Loss: 1.2442e+03. (Time: 2.4s)updating best model ..\n",
            "Step 9000 of 1000000; Loss: 1.4999e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7907e+03; Test Loss: 1.2416e+03. (Time: 2.4s)updating best model ..\n",
            "Step 9200 of 1000000; Loss: 1.4958e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6414e+03; Test Loss: 1.2391e+03. (Time: 2.4s)updating best model ..\n",
            "Step 9400 of 1000000; Loss: 1.4920e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5937e+03; Test Loss: 1.2367e+03. (Time: 2.4s)updating best model ..\n",
            "Step 9600 of 1000000; Loss: 1.4882e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5526e+03; Test Loss: 1.2344e+03. (Time: 2.4s)updating best model ..\n",
            "Step 9800 of 1000000; Loss: 1.4847e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7339e+03; Test Loss: 1.2322e+03. (Time: 2.4s)updating best model ..\n",
            "Step 10000 of 1000000; Loss: 1.4813e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6237e+03; Test Loss: 1.2301e+03. (Time: 2.4s)updating best model ..\n",
            "Step 10200 of 1000000; Loss: 1.4780e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7489e+03; Test Loss: 1.2281e+03. (Time: 2.4s)updating best model ..\n",
            "Step 10400 of 1000000; Loss: 1.4749e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8094e+03; Test Loss: 1.2262e+03. (Time: 2.5s)updating best model ..\n",
            "Step 10600 of 1000000; Loss: 1.4719e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7401e+03; Test Loss: 1.2245e+03. (Time: 2.4s)updating best model ..\n",
            "Step 10800 of 1000000; Loss: 1.4690e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2004e+03; Test Loss: 1.2228e+03. (Time: 2.4s)updating best model ..\n",
            "Step 11000 of 1000000; Loss: 1.4664e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5736e+03; Test Loss: 1.2212e+03. (Time: 2.4s)updating best model ..\n",
            "Step 11200 of 1000000; Loss: 1.4638e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8218e+03; Test Loss: 1.2198e+03. (Time: 2.4s)updating best model ..\n",
            "Step 11400 of 1000000; Loss: 1.4614e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5407e+03; Test Loss: 1.2184e+03. (Time: 2.4s)updating best model ..\n",
            "Step 11600 of 1000000; Loss: 1.4591e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2512e+03; Test Loss: 1.2171e+03. (Time: 2.4s)updating best model ..\n",
            "Step 11800 of 1000000; Loss: 1.4569e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7104e+03; Test Loss: 1.2159e+03. (Time: 2.4s)updating best model ..\n",
            "Step 12000 of 1000000; Loss: 1.4548e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5738e+03; Test Loss: 1.2148e+03. (Time: 2.4s)updating best model ..\n",
            "Step 12200 of 1000000; Loss: 1.4529e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5160e+03; Test Loss: 1.2138e+03. (Time: 2.4s)updating best model ..\n",
            "Step 12400 of 1000000; Loss: 1.4511e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5126e+03; Test Loss: 1.2129e+03. (Time: 2.4s)updating best model ..\n",
            "Step 12600 of 1000000; Loss: 1.4494e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6943e+03; Test Loss: 1.2121e+03. (Time: 2.5s)updating best model ..\n",
            "Step 12800 of 1000000; Loss: 1.4479e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5460e+03; Test Loss: 1.2113e+03. (Time: 2.5s)updating best model ..\n",
            "Step 13000 of 1000000; Loss: 1.4464e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6973e+03; Test Loss: 1.2107e+03. (Time: 2.4s)updating best model ..\n",
            "Step 13200 of 1000000; Loss: 1.4450e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7719e+03; Test Loss: 1.2101e+03. (Time: 3.1s)updating best model ..\n",
            "Step 13400 of 1000000; Loss: 1.4438e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7051e+03; Test Loss: 1.2096e+03. (Time: 2.4s)updating best model ..\n",
            "Step 13600 of 1000000; Loss: 1.4427e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1555e+03; Test Loss: 1.2092e+03. (Time: 2.4s)updating best model ..\n",
            "Step 13800 of 1000000; Loss: 1.4416e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5287e+03; Test Loss: 1.2088e+03. (Time: 2.4s)updating best model ..\n",
            "Step 14000 of 1000000; Loss: 1.4407e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7725e+03; Test Loss: 1.2085e+03. (Time: 2.4s)updating best model ..\n",
            "Step 14200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4934e+03; Test Loss: 1.2083e+03. (Time: 2.5s)updating best model ..\n",
            "Step 14400 of 1000000; Loss: 1.4391e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2225e+03; Test Loss: 1.2081e+03. (Time: 2.4s)updating best model ..\n",
            "Step 14600 of 1000000; Loss: 1.4384e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6639e+03; Test Loss: 1.2080e+03. (Time: 2.4s)updating best model ..\n",
            "Step 14800 of 1000000; Loss: 1.4378e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5359e+03; Test Loss: 1.2079e+03. (Time: 2.4s)updating best model ..\n",
            "Step 15000 of 1000000; Loss: 1.4373e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4704e+03; Test Loss: 1.2080e+03. (Time: 2.5s)updating best model ..\n",
            "Step 15200 of 1000000; Loss: 1.4369e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4973e+03; Test Loss: 1.2080e+03. (Time: 2.4s)updating best model ..\n",
            "Step 15400 of 1000000; Loss: 1.4366e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6801e+03; Test Loss: 1.2081e+03. (Time: 2.4s)updating best model ..\n",
            "Step 15600 of 1000000; Loss: 1.4363e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5007e+03; Test Loss: 1.2083e+03. (Time: 2.4s)updating best model ..\n",
            "Step 15800 of 1000000; Loss: 1.4361e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6724e+03; Test Loss: 1.2084e+03. (Time: 2.4s)updating best model ..\n",
            "Step 16000 of 1000000; Loss: 1.4359e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7593e+03; Test Loss: 1.2087e+03. (Time: 2.6s)updating best model ..\n",
            "Step 16200 of 1000000; Loss: 1.4358e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6924e+03; Test Loss: 1.2089e+03. (Time: 2.5s)updating best model ..\n",
            "Step 16400 of 1000000; Loss: 1.4357e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1319e+03; Test Loss: 1.2092e+03. (Time: 2.4s)updating best model ..\n",
            "Step 16600 of 1000000; Loss: 1.4357e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5075e+03; Test Loss: 1.2094e+03. (Time: 2.4s)Step 16800 of 1000000; Loss: 1.4357e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7496e+03; Test Loss: 1.2098e+03. (Time: 2.4s)Step 17000 of 1000000; Loss: 1.4358e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4709e+03; Test Loss: 1.2101e+03. (Time: 2.5s)Step 17200 of 1000000; Loss: 1.4359e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2106e+03; Test Loss: 1.2104e+03. (Time: 2.4s)Step 17400 of 1000000; Loss: 1.4360e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6427e+03; Test Loss: 1.2107e+03. (Time: 2.4s)Step 17600 of 1000000; Loss: 1.4361e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5189e+03; Test Loss: 1.2110e+03. (Time: 2.4s)Step 17800 of 1000000; Loss: 1.4363e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4486e+03; Test Loss: 1.2114e+03. (Time: 2.4s)Step 18000 of 1000000; Loss: 1.4365e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4963e+03; Test Loss: 1.2117e+03. (Time: 2.4s)Step 18200 of 1000000; Loss: 1.4366e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6789e+03; Test Loss: 1.2120e+03. (Time: 2.4s)Step 18400 of 1000000; Loss: 1.4368e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4804e+03; Test Loss: 1.2122e+03. (Time: 2.7s)Step 18600 of 1000000; Loss: 1.4370e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6642e+03; Test Loss: 1.2125e+03. (Time: 2.6s)Step 18800 of 1000000; Loss: 1.4371e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7592e+03; Test Loss: 1.2128e+03. (Time: 2.4s)Step 19000 of 1000000; Loss: 1.4373e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6908e+03; Test Loss: 1.2130e+03. (Time: 2.5s)Step 19200 of 1000000; Loss: 1.4375e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1230e+03; Test Loss: 1.2132e+03. (Time: 2.4s)Step 19400 of 1000000; Loss: 1.4376e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5005e+03; Test Loss: 1.2134e+03. (Time: 2.5s)Step 19600 of 1000000; Loss: 1.4377e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7422e+03; Test Loss: 1.2135e+03. (Time: 2.4s)Step 19800 of 1000000; Loss: 1.4379e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4639e+03; Test Loss: 1.2137e+03. (Time: 2.4s)Step 20000 of 1000000; Loss: 1.4380e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2070e+03; Test Loss: 1.2138e+03. (Time: 2.5s)Step 20200 of 1000000; Loss: 1.4381e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6359e+03; Test Loss: 1.2139e+03. (Time: 2.4s)Step 20400 of 1000000; Loss: 1.4382e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5129e+03; Test Loss: 1.2141e+03. (Time: 2.4s)Step 20600 of 1000000; Loss: 1.4383e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4415e+03; Test Loss: 1.2142e+03. (Time: 2.4s)Step 20800 of 1000000; Loss: 1.4383e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4981e+03; Test Loss: 1.2142e+03. (Time: 2.5s)Step 21000 of 1000000; Loss: 1.4384e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6788e+03; Test Loss: 1.2143e+03. (Time: 2.5s)Step 21200 of 1000000; Loss: 1.4385e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4749e+03; Test Loss: 1.2144e+03. (Time: 2.5s)Step 21400 of 1000000; Loss: 1.4385e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6613e+03; Test Loss: 1.2144e+03. (Time: 2.4s)Step 21600 of 1000000; Loss: 1.4386e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7599e+03; Test Loss: 1.2145e+03. (Time: 2.4s)Step 21800 of 1000000; Loss: 1.4386e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6897e+03; Test Loss: 1.2145e+03. (Time: 2.4s)Step 22000 of 1000000; Loss: 1.4387e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1211e+03; Test Loss: 1.2145e+03. (Time: 2.4s)Step 22200 of 1000000; Loss: 1.4387e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4982e+03; Test Loss: 1.2145e+03. (Time: 2.4s)Step 22400 of 1000000; Loss: 1.4387e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7394e+03; Test Loss: 1.2146e+03. (Time: 2.4s)Step 22600 of 1000000; Loss: 1.4388e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4627e+03; Test Loss: 1.2146e+03. (Time: 2.4s)Step 22800 of 1000000; Loss: 1.4388e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2056e+03; Test Loss: 1.2146e+03. (Time: 2.5s)Step 23000 of 1000000; Loss: 1.4388e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6338e+03; Test Loss: 1.2146e+03. (Time: 2.4s)Step 23200 of 1000000; Loss: 1.4388e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5107e+03; Test Loss: 1.2146e+03. (Time: 2.4s)Step 23400 of 1000000; Loss: 1.4389e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4398e+03; Test Loss: 1.2146e+03. (Time: 2.4s)Step 23600 of 1000000; Loss: 1.4389e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4992e+03; Test Loss: 1.2146e+03. (Time: 2.4s)Step 23800 of 1000000; Loss: 1.4389e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6782e+03; Test Loss: 1.2146e+03. (Time: 2.4s)Step 24000 of 1000000; Loss: 1.4389e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4742e+03; Test Loss: 1.2146e+03. (Time: 2.4s)Step 24200 of 1000000; Loss: 1.4390e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6596e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 24400 of 1000000; Loss: 1.4390e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7599e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 24600 of 1000000; Loss: 1.4390e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6884e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 24800 of 1000000; Loss: 1.4390e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1209e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 25000 of 1000000; Loss: 1.4390e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4971e+03; Test Loss: 1.2147e+03. (Time: 3.0s)Step 25200 of 1000000; Loss: 1.4390e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7377e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 25400 of 1000000; Loss: 1.4391e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4625e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 25600 of 1000000; Loss: 1.4391e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2048e+03; Test Loss: 1.2147e+03. (Time: 2.5s)Step 25800 of 1000000; Loss: 1.4391e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6330e+03; Test Loss: 1.2146e+03. (Time: 2.4s)Step 26000 of 1000000; Loss: 1.4391e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5098e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 26200 of 1000000; Loss: 1.4391e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4394e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 26400 of 1000000; Loss: 1.4391e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4999e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 26600 of 1000000; Loss: 1.4391e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6778e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 26800 of 1000000; Loss: 1.4392e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4744e+03; Test Loss: 1.2147e+03. (Time: 2.5s)Step 27000 of 1000000; Loss: 1.4392e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6585e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 27200 of 1000000; Loss: 1.4392e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7600e+03; Test Loss: 1.2147e+03. (Time: 2.5s)Step 27400 of 1000000; Loss: 1.4392e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6875e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 27600 of 1000000; Loss: 1.4392e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1211e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 27800 of 1000000; Loss: 1.4392e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4965e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 28000 of 1000000; Loss: 1.4392e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7367e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 28200 of 1000000; Loss: 1.4393e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4625e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 28400 of 1000000; Loss: 1.4393e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2044e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 28600 of 1000000; Loss: 1.4393e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6326e+03; Test Loss: 1.2147e+03. (Time: 2.5s)Step 28800 of 1000000; Loss: 1.4393e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5094e+03; Test Loss: 1.2147e+03. (Time: 2.5s)Step 29000 of 1000000; Loss: 1.4393e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4394e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 29200 of 1000000; Loss: 1.4393e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5005e+03; Test Loss: 1.2147e+03. (Time: 2.5s)Step 29400 of 1000000; Loss: 1.4393e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2147e+03. (Time: 2.5s)Step 29600 of 1000000; Loss: 1.4393e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4747e+03; Test Loss: 1.2147e+03. (Time: 2.5s)Step 29800 of 1000000; Loss: 1.4393e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6578e+03; Test Loss: 1.2147e+03. (Time: 2.5s)Step 30000 of 1000000; Loss: 1.4394e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7601e+03; Test Loss: 1.2147e+03. (Time: 2.5s)Step 30200 of 1000000; Loss: 1.4394e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6870e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 30400 of 1000000; Loss: 1.4394e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1213e+03; Test Loss: 1.2147e+03. (Time: 2.5s)Step 30600 of 1000000; Loss: 1.4394e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4962e+03; Test Loss: 1.2147e+03. (Time: 2.5s)Step 30800 of 1000000; Loss: 1.4394e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7361e+03; Test Loss: 1.2147e+03. (Time: 2.5s)Step 31000 of 1000000; Loss: 1.4394e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4626e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 31200 of 1000000; Loss: 1.4394e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2042e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 31400 of 1000000; Loss: 1.4394e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6324e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 31600 of 1000000; Loss: 1.4394e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5092e+03; Test Loss: 1.2147e+03. (Time: 2.5s)Step 31800 of 1000000; Loss: 1.4394e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4394e+03; Test Loss: 1.2147e+03. (Time: 2.5s)Step 32000 of 1000000; Loss: 1.4395e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5009e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 32200 of 1000000; Loss: 1.4395e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 32400 of 1000000; Loss: 1.4395e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4750e+03; Test Loss: 1.2147e+03. (Time: 2.5s)Step 32600 of 1000000; Loss: 1.4395e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6574e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 32800 of 1000000; Loss: 1.4395e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7602e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 33000 of 1000000; Loss: 1.4395e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6867e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 33200 of 1000000; Loss: 1.4395e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1214e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 33400 of 1000000; Loss: 1.4395e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4960e+03; Test Loss: 1.2147e+03. (Time: 2.5s)Step 33600 of 1000000; Loss: 1.4395e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7358e+03; Test Loss: 1.2147e+03. (Time: 2.5s)Step 33800 of 1000000; Loss: 1.4395e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4627e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 34000 of 1000000; Loss: 1.4395e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2041e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 34200 of 1000000; Loss: 1.4395e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6323e+03; Test Loss: 1.2147e+03. (Time: 2.4s)Step 34400 of 1000000; Loss: 1.4395e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5091e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 34600 of 1000000; Loss: 1.4395e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4394e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 34800 of 1000000; Loss: 1.4396e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5012e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 35000 of 1000000; Loss: 1.4396e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 35200 of 1000000; Loss: 1.4396e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4751e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 35400 of 1000000; Loss: 1.4396e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6572e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 35600 of 1000000; Loss: 1.4396e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7603e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 35800 of 1000000; Loss: 1.4396e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6866e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 36000 of 1000000; Loss: 1.4396e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1215e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 36200 of 1000000; Loss: 1.4396e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4960e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 36400 of 1000000; Loss: 1.4396e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7355e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 36600 of 1000000; Loss: 1.4396e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4627e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 36800 of 1000000; Loss: 1.4396e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2040e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 37000 of 1000000; Loss: 1.4396e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6322e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 37200 of 1000000; Loss: 1.4396e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5091e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 37400 of 1000000; Loss: 1.4396e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 37600 of 1000000; Loss: 1.4396e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5014e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 37800 of 1000000; Loss: 1.4396e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 3.4s)Step 38000 of 1000000; Loss: 1.4396e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4753e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 38200 of 1000000; Loss: 1.4396e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6570e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 38400 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7603e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 38600 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6864e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 38800 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1215e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 39000 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4959e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 39200 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7354e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 39400 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4628e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 39600 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 39800 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6322e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 40000 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 40200 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 40400 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5015e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 40600 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 40800 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4753e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 41000 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6569e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 41200 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7604e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 41400 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6864e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 41600 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1216e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 41800 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4959e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 42000 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7353e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 42200 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4628e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 42400 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 42600 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6322e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 42800 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 43000 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 43200 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5016e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 43400 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 43600 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4754e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 43800 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6568e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 44000 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7604e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 44200 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 44400 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1216e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 44600 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 44800 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7353e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 45000 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4628e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 45200 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 45400 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 45600 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 45800 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 46000 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5016e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 46200 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 46400 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4754e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 46600 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6568e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 46800 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7604e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 47000 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 47200 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1216e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 47400 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 47600 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 47800 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4628e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 48000 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 48200 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 48400 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 48600 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 48800 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 49000 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 49200 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 49400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6568e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 49600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7604e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 49800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 50000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1216e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 50200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 50400 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2148e+03. (Time: 3.2s)Step 50600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 50800 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 51000 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 51200 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 51400 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 51600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 51800 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 52000 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 52200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6568e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 52400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7604e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 52600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 52800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 53000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 53200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 53400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 53600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 53800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 54000 of 1000000; Loss: 1.4397e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 54200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 54400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 54600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 54800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 55000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 55200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7604e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 55400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 55600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 55800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 56000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 56200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 56400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 56600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 56800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 57000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.7s)Step 57200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 57400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 57600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 57800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 58000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7604e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 58200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 58400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 58600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 58800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 59000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 59200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 59400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 59600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 59800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 60000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 60200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 60400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 60600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 60800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 61000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 61200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 61400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 61600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 61800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 62000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 62200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 62400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 62600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 62800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 63000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 63200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.4s)Step 63400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 3.2s)Step 63600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 63800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 64000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.4s)Step 64200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 64400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 64600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 64800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 65000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 65200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 65400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 65600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 65800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 66000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 66200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 66400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 66600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 66800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 67000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 67200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 67400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 67600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 67800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 68000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 68200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 68400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 68600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 68800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 69000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 69200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 69400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 69600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 69800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 70000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 70200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 70400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 70600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 70800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 71000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 71200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 71400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 71600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 71800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 72000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 72200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 72400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 72600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 72800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 73000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 73200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 73400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 73600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 73800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 74000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 74200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 74400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 74600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 74800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 75000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 75200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 75400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 75600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 75800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 76000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 76200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 76400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 3.5s)Step 76600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 76800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 77000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 77200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 77400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 77600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 77800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 78000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 78200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 78400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 78600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 78800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 79000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 79200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 79400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 79600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 79800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 80000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 80200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 80400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 80600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 80800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 81000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 81200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 81400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 81600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 81800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 82000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 82200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 82400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 82600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 82800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 83000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.4s)Step 83200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 83400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 83600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 83800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 84000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 84200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 84400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 84600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 84800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 85000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 85200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 85400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 85600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 85800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 86000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.4s)Step 86200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.4s)Step 86400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 86600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 86800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 87000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 87200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 87400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 87600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 87800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 88000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 88200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 88400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 88600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 88800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 89000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 89200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 89400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 89600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 89800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 90000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 3.3s)Step 90200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 90400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 90600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.4s)Step 90800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 91000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 91200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 91400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 91600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 91800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 92000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 92200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 92400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.4s)Step 92600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 92800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 93000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 93200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 93400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 93600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 93800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 94000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 94200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 94400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 94600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 94800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 95000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 95200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 95400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 95600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 95800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 96000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 96200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 96400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 96600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 96800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 97000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 97200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 97400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 97600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 97800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 98000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 98200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 98400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 98600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 98800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 99000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 99200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 99400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 99600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 99800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 100000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 100200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 100400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 100600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 100800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 101000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 101200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 101400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 101600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 101800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 102000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 102200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 102400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 102600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 102800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 103000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 103200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 103400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 103600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 103800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 104000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 104200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 104400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 3.4s)Step 104600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 104800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 105000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 105200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 105400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 105600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 105800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 106000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 106200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 106400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 106600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 106800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 107000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 107200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 107400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.4s)Step 107600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 107800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 108000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 108200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 108400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 108600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 108800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 109000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 109200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 109400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 109600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 109800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 110000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 110200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 110400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 110600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 110800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 111000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 111200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 111400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 111600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 111800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 112000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 112200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 112400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 112600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 112800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 113000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 113200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 113400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 113600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 113800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 114000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 114200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 114400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 114600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 114800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 115000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 115200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 115400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 115600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 115800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 116000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 116200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 116400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 116600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 116800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 117000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 117200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 117400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 117600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 117800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 118000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 118200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 118400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 118600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 118800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 119000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 119200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 119400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 3.4s)Step 119600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 119800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 120000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 120200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 120400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 120600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 120800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 121000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 121200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 121400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 121600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 121800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 122000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 122200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 122400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 122600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 122800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 123000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 123200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 123400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 123600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 123800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 124000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 124200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 124400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 124600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 124800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 125000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 125200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 125400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 125600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 125800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 126000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 126200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 126400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 126600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 126800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 127000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 127200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 127400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 127600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 127800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 128000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 128200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 128400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 128600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 128800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 129000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 129200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 129400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 129600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 129800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 130000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 130200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 130400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 130600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 130800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 131000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 131200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 131400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 131600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 131800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 132000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 132200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 132400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 132600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 132800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 133000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 133200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 133400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 133600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 133800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 134000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 134200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 134400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 134600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 134800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 3.5s)Step 135000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 135200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 135400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 135600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 135800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 136000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 136200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 136400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 136600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 136800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 137000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 137200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 137400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 137600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 137800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 138000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 138200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 138400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 138600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 138800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 139000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 139200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 139400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 139600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 139800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 140000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 140200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 140400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 140600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 140800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 141000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 141200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 141400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 141600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 141800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 142000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 142200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 142400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 142600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 142800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 143000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 143200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 143400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 143600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 143800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 144000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 144200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 144400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 144600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 144800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 145000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 145200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 145400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 145600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 145800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 146000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 146200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 146400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 146600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.4s)Step 146800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 147000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 147200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 147400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 147600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 147800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 148000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 148200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 148400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 148600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.4s)Step 148800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 149000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 149200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 149400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 149600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 149800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 150000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 150200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.4s)Step 150400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 150600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 150800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 151000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 3.5s)Step 151200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 151400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 151600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 151800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 152000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 152200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 152400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 152600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 152800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 153000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 153200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 153400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 153600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 153800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 154000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 154200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 154400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 154600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 154800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 155000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 155200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 155400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 155600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 155800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 156000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 156200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 156400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 156600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 156800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 157000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 157200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 157400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 157600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 157800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 158000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 158200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 158400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 158600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 158800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 159000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 159200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 159400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 159600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 159800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 160000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 160200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 160400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 160600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 160800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 161000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 161200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.4s)Step 161400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 161600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 161800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 162000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 162200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 162400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 162600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 162800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 163000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 163200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 163400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 163600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 163800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 164000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 164200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 164400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 164600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 164800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 165000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 165200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 165400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 165600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 165800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 166000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 166200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 166400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 166600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 166800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 167000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 167200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 167400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 167600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 167800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 168000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 3.6s)Step 168200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 168400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 168600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 168800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 169000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 169200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 169400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 169600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 169800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 170000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 170200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 170400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 170600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 170800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 171000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 171200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 171400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 171600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 171800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 172000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 172200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.9s)Step 172400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 172600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 172800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 173000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 173200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 173400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 173600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 173800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 174000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 174200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 174400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 174600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 174800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 175000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 175200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 175400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 175600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 175800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 176000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 176200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 176400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 176600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 176800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 177000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 177200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 177400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 177600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 177800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 178000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 178200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 178400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 178600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 178800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 179000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 179200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 179400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 179600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 179800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 180000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 180200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 180400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 180600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 180800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 181000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 181200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 181400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 181600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 181800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 182000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 182200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 182400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 182600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 182800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 183000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 183200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 183400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 183600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 183800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 184000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 184200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 184400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 184600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 184800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 185000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 185200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 185400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 185600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 185800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 3.6s)Step 186000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 186200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.7s)Step 186400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 186600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 186800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 187000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 187200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 187400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 187600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 187800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 188000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 188200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 188400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 188600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 188800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 189000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 189200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 189400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 189600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 189800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 190000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 190200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 190400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 190600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 190800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 191000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 191200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 191400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 191600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 191800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 192000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 192200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.4s)Step 192400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 192600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 192800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 193000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 193200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 193400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 193600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 193800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 194000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 194200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 194400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 194600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 194800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 195000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.4s)Step 195200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 195400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 195600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 195800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 196000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 196200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 196400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 196600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 196800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 197000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 197200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 197400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 197600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.8s)Step 197800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 198000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 198200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 198400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 198600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 198800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 199000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 199200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 199400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 199600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 199800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 200000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.7s)Step 200200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 200400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.7s)Step 200600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 200800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 201000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 201200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.7s)Step 201400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 201600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 201800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 202000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 202200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 202400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 202600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 202800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 203000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 203200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 203400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 203600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 203800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 204000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 3.7s)Step 204200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 204400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 204600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 204800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 205000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 205200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 205400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 205600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 205800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 206000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 206200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 206400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 206600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 206800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 207000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 207200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 207400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.8s)Step 207600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 207800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 208000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 208200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.7s)Step 208400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.8s)Step 208600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.7s)Step 208800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.9s)Step 209000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 209200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 209400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 209600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.7s)Step 209800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 210000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 210200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 210400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.7s)Step 210600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.8s)Step 210800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 211000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 211200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 3.0s)Step 211400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 211600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 211800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 212000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 212200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 212400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 212600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 212800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 213000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 213200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 213400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 213600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 213800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 214000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.7s)Step 214200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.8s)Step 214400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 214600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 214800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 215000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 215200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 215400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 215600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 215800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 216000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 216200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 216400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 216600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 216800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 217000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 217200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 217400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 217600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 217800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 218000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 218200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 218400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 218600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 218800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 219000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 219200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 219400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 219600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 219800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 220000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 220200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 220400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 220600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 220800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 221000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 221200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 221400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 221600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 221800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 222000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 222200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 222400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 222600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 222800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 223000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 223200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 3.8s)Step 223400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 223600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 223800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 224000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 224200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 224400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 224600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 224800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 225000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 225200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 225400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 225600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 225800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 226000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 226200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 226400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 226600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 226800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 227000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 227200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 227400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 227600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 227800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 228000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 228200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 228400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 228600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 228800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 229000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 229200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 229400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 229600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 229800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 230000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 230200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 230400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 230600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 230800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 231000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 231200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 231400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 231600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 231800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 232000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 232200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 232400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 232600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 232800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 233000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 233200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 233400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 233600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 233800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 234000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 234200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 234400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 234600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 234800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 235000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 235200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 235400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 235600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 235800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 236000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 236200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 236400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 236600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 236800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 237000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 237200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 237400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 237600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 237800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 238000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 238200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 238400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 238600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 238800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 239000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 239200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 239400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 239600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 239800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 240000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 240200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 240400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 240600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 240800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 241000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 241200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 241400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 241600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 241800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 242000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 242200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 242400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 242600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 242800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 243000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 3.8s)Step 243200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 243400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 243600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 243800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 244000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 244200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 244400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 244600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 244800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 245000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 245200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 245400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 245600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 245800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 246000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 246200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 246400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 246600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 246800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 247000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 247200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 247400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 247600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 247800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 248000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 248200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 248400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 248600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 248800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 249000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 249200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 249400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 249600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 249800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 250000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 250200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 250400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 250600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 250800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 251000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 251200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 251400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 251600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 251800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 252000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 252200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 252400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 252600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 252800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 253000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 253200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 253400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 253600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 253800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 254000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 254200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 254400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 254600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 254800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 255000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 255200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 255400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 255600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 255800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.4s)Step 256000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 256200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 256400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 256600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 256800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 257000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 257200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 257400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 257600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 257800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 258000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 258200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 258400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 258600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 258800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 259000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 259200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 259400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 259600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 259800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 260000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 260200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 260400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 260600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 260800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 261000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 261200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 261400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 261600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 261800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 262000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 262200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 262400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 262600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 262800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 263000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 263200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 263400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 263600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 263800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 264000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 3.9s)Step 264200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 264400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 264600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 264800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 265000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 265200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 265400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 265600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 265800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 266000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 266200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 266400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 266600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 266800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 267000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 267200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 267400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 267600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 267800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 268000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 268200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 268400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 268600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 268800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 269000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 269200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 269400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 269600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 269800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 270000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 270200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 270400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 270600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 270800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 271000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 271200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 271400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 271600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 271800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 272000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 272200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 272400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 272600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 272800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 273000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 273200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 273400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 273600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 273800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 274000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 274200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 274400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 274600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 274800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 275000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 275200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 275400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 275600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 275800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 276000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 276200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 276400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 276600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 276800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 277000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 277200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 277400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 277600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 277800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 278000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 278200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 278400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 278600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 278800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 279000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 279200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.4s)Step 279400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 279600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 279800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 280000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 280200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 280400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 280600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 280800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 281000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 281200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 281400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 281600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 281800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 282000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 282200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 282400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 282600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 282800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 283000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 283200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 283400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 283600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 283800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 284000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 284200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 284400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 284600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 284800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 285000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 285200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 285400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 285600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 285800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 4.0s)Step 286000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 286200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 286400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 286600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 286800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 287000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 287200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 287400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 287600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 287800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 288000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 288200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 288400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 288600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 288800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 289000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 289200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 289400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 289600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 289800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 290000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 290200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 290400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 290600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 290800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 291000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 291200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 291400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 291600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 291800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 292000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 292200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 292400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 292600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 292800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 293000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 293200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 293400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 293600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 293800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 294000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 294200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 294400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 294600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 294800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 295000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 295200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 295400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 295600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 295800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 296000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 296200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 296400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 296600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 296800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 297000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 297200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 297400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 297600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 297800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 298000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 298200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 298400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 298600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 298800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 299000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 299200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 299400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 299600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 299800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.7s)Step 300000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 300200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 300400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 300600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 300800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 301000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 301200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 301400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 301600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 301800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 302000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 302200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 302400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 302600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 302800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 303000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 303200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 303400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 303600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 303800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 304000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 304200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 304400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 304600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 304800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 305000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 305200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 305400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 305600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 305800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 306000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 306200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 306400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 306600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 306800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 307000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 307200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 307400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 307600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 307800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 308000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 308200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 4.0s)Step 308400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 308600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 308800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 309000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 309200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 309400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 309600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 309800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 310000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 310200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 310400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 310600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 310800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 311000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 311200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 311400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 311600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 311800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 312000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 312200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 312400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 312600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 312800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 313000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 313200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 313400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 313600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 313800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 314000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 314200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 314400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 314600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 314800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 315000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 315200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 315400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 315600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 315800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 316000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 316200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 316400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 316600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 316800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 317000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.7s)Step 317200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 317400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 317600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 317800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 318000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 318200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 318400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 318600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.7s)Step 318800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 319000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 319200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 319400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 319600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 319800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 320000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 320200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 320400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 320600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 320800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 321000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.8s)Step 321200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 321400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 321600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 321800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 322000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 322200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 322400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 322600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 322800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 323000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 323200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 323400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 323600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 323800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 324000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 324200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 324400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 324600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 324800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 325000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 325200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 325400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 325600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 325800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 326000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 326200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 326400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 326600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 326800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 327000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 327200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 327400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 327600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 327800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 328000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 328200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 328400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 328600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 328800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 329000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 329200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 329400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 329600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 329800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 330000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 330200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 330400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 330600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 330800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 331000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 331200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 331400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 331600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 331800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 4.1s)Step 332000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 332200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.7s)Step 332400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 332600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 332800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 333000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 333200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 333400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 333600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 333800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 334000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 334200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 334400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 334600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 334800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 335000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 335200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 335400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 335600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 335800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 336000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 336200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 336400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 336600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 336800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 337000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 337200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 337400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.8s)Step 337600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 337800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 338000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 338200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 338400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 338600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 338800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 339000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 339200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 339400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 339600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 339800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 340000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 340200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 340400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 340600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 340800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 341000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 341200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 341400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 341600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 341800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 342000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 342200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 342400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 342600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 342800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 343000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 343200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 343400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 343600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 343800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 344000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 344200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 344400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 344600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 344800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 345000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 345200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 345400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 345600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 345800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 346000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 346200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 346400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 346600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 346800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 347000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 347200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 347400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 347600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 347800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 348000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 348200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 348400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 348600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 348800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 349000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 349200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 349400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 349600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 349800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 350000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 350200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 350400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 350600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 350800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 351000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 351200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 351400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 351600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 351800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 352000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 352200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 352400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 352600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 352800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 353000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 353200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 353400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 353600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 353800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 354000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 354200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 354400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 354600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 354800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 355000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 355200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 355400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 355600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 355800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 356000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 356200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 356400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 356600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 4.2s)Step 356800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 357000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 357200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 357400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 357600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 357800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 358000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 358200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 358400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 358600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 358800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 359000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 359200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 359400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 359600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 359800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 360000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 360200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 360400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 360600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 360800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 361000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.8s)Step 361200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.7s)Step 361400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 361600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 361800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 362000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 362200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 362400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 362600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 362800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 363000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 363200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 363400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 363600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 363800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 364000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 364200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 364400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 364600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 364800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 365000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 365200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 365400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 365600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 365800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 366000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 366200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 366400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 366600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 366800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 367000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 367200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 367400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 367600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 367800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 368000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 368200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 368400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 368600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 368800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 369000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 369200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 369400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 369600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 369800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 370000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 370200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 370400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 370600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 370800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 371000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 371200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 371400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 371600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 371800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 372000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 372200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 372400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 372600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 372800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 373000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 373200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 373400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 373600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 373800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 374000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 374200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 374400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 374600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 374800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 375000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 375200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.7s)Step 375400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 375600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.7s)Step 375800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.8s)Step 376000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 3.8s)Step 376200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 3.4s)Step 376400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 3.0s)Step 376600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 376800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 377000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.8s)Step 377200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.8s)Step 377400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 377600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 377800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 378000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 378200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 378400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 378600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.8s)Step 378800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 379000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 379200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 379400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 379600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 379800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 380000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 380200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 380400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 380600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 380800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 381000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 381200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 381400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 381600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 381800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 382000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 4.3s)Step 382200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 382400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 382600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 382800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 383000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 383200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 383400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 383600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 383800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 384000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 384200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 384400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 384600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 384800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 385000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 385200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.8s)Step 385400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 385600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 385800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 386000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 386200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 386400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 386600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 386800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 387000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 387200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 387400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.8s)Step 387600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.7s)Step 387800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 3.0s)Step 388000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.8s)Step 388200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.9s)Step 388400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.8s)Step 388600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 388800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 389000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 389200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 389400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 389600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 389800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 390000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 390200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.7s)Step 390400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 3.0s)Step 390600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.7s)Step 390800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 391000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 391200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 391400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 391600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 391800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 392000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 392200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 392400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 392600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 392800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 393000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 3.0s)Step 393200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 393400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 393600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 393800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 394000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 394200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 394400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 394600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 394800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 395000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 395200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 395400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 395600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 395800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 396000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 396200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 396400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 396600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.7s)Step 396800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 397000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 397200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 397400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 397600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 397800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 398000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 398200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 398400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 398600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.7s)Step 398800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 399000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 399200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 399400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 399600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 399800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 400000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 400200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 400400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 400600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 400800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 401000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 401200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 401400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 401600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 401800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 402000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 402200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 402400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 402600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 402800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.7s)Step 403000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 403200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 403400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 403600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 403800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 404000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 404200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 404400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 404600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 404800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 405000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 405200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 405400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 405600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 405800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 406000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 406200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 406400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 406600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 406800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 407000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 407200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 407400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.7s)Step 407600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 407800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 408000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 408200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 408400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 408600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 4.4s)Step 408800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 409000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 409200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 409400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 409600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 409800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 410000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 410200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 410400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.7s)Step 410600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 410800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 411000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 411200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 411400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 411600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 411800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 412000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 412200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 412400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 412600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 412800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 413000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 413200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 413400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 413600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 413800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 414000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 414200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 414400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 414600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 414800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 415000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 415200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 415400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 415600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 415800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 416000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 416200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 416400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 416600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 416800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 417000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 417200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 417400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 417600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 417800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 418000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 418200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 418400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 418600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 418800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 419000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 419200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 419400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 419600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 419800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 420000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 420200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 420400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 420600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 420800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 421000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 421200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 421400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 421600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 421800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 422000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 422200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 422400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 422600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 422800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 423000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 423200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 423400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 423600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 423800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 424000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 424200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 424400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 424600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 424800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 425000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 425200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 425400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 425600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 425800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 426000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 426200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 426400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 426600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 426800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 427000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 427200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 427400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 427600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 427800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 428000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 428200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 428400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 428600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 428800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 429000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 429200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 429400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 429600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 429800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 430000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 430200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 430400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 430600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 430800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 431000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 431200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 431400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 431600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 431800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 432000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.7s)Step 432200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4395e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 432400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5017e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 432600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6777e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 432800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4755e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 433000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6567e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 433200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7605e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 433400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6863e+03; Test Loss: 1.2149e+03. (Time: 2.6s)Step 433600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1217e+03; Test Loss: 1.2149e+03. (Time: 2.7s)Step 433800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4958e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 434000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7352e+03; Test Loss: 1.2149e+03. (Time: 2.5s)Step 434200 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4629e+03; Test Loss: 1.2148e+03. (Time: 2.6s)Step 434400 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2039e+03; Test Loss: 1.2148e+03. (Time: 2.5s)Step 434600 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6321e+03; Test Loss: 1.2148e+03. (Time: 2.7s)Step 434800 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5090e+03; Test Loss: 1.2148e+03. (Time: 2.8s)Step 435000 of 1000000; Loss: 1.4398e+03. (Time: 0.0s)\n"
          ]
        }
      ],
      "source": [
        "# 对认知模型\n",
        "n_folds = len(folds)\n",
        "avg_nlls = np.zeros(n_folds)\n",
        "\n",
        "\n",
        "for i, (train_ds, val_ds, test_ds) in enumerate(folds):\n",
        "    print(f\"=== Fold {i} ===\")\n",
        "    # 用 train/val 训练并选超参\n",
        "    params, _ = rnn_utils.fit_model(\n",
        "        model_fun=bandits.Hk_PreserveConAgentQ,\n",
        "        dataset_train=train_ds,\n",
        "        dataset_test=val_ds,      # 用 val_ds 做 early-stop\n",
        "        optimizer=optax.chain(\n",
        "            optax.add_decayed_weights(1e-4),\n",
        "            optax.adam(learning_rate=1e-4)\n",
        "        ),\n",
        "        n_steps_per_call=200,\n",
        "        n_steps_max=1000000,\n",
        "        early_stop_step=400,\n",
        "    )\n",
        "\n",
        "    # 在 test_ds 上计算平均 NLL\n",
        "    avg_nll = compute_negative_log_likelihood(test_ds, bandits.Hk_PreserveConAgentQ, params)\n",
        "    print(f\"Fold {i} test Avg NLL: {avg_nll:.4f}\\n\")\n",
        "    avg_nlls[i] = avg_nll\n",
        "\n",
        "# 汇总\n",
        "mean_nll = avg_nlls.mean()\n",
        "std_nll  = avg_nlls.std(ddof=1)    # 样本标准差\n",
        "se_nll   = std_nll / np.sqrt(n_folds)\n",
        "\n",
        "print(\"All folds Avg NLL:\", avg_nlls)\n",
        "print(f\"Mean NLL over folds: {mean_nll:.4f}\")\n",
        "print(f\"Stddev over folds : {std_nll:.4f}\")\n",
        "print(f\"Std. Error (SE)    : {se_nll:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('For RL model')\n",
        "print(\"All folds Avg NLL:\", avg_nlls)\n",
        "print(f\"Mean NLL over folds: {mean_nll:.4f}\")\n",
        "print(f\"Stddev over folds : {std_nll:.4f}\")\n",
        "print(f\"Std. Error (SE)    : {se_nll:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvGdnab8CdCN"
      },
      "source": [
        "## Fit LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db56B4IGCfuy"
      },
      "outputs": [],
      "source": [
        "# 设置 LSTM 模型\n",
        "n_hidden = 16  # 隐藏层的单元数\n",
        "\n",
        "def make_lstm():\n",
        "    model = hk.DeepRNN([\n",
        "        hk.LSTM(n_hidden),  # 使用 LSTM 而不是 GRU\n",
        "        hk.Linear(output_size=2)  # 输出层大小保持不变\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDur9ioEC-99",
        "outputId": "865d1646-3a1d-4fc3-b94f-b22c1604e367"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 500 of 500; Loss: 6.9076e+03; Test Loss: 6.4660e+02. (Time: 12.8s)Step 500 of 100000; Loss: 6.5286e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 6.1460e+03; Test Loss: 5.7758e+02. (Time: 12.5s)Step 1000 of 100000; Loss: 5.7942e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.8505e+03; Test Loss: 5.5253e+02. (Time: 12.9s)Step 1500 of 100000; Loss: 5.5328e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.6792e+03; Test Loss: 5.4233e+02. (Time: 12.5s)Step 2000 of 100000; Loss: 5.4272e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.5691e+03; Test Loss: 5.3511e+02. (Time: 12.7s)Step 2500 of 100000; Loss: 5.3546e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.4715e+03; Test Loss: 5.2846e+02. (Time: 12.7s)Step 3000 of 100000; Loss: 5.2875e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.3784e+03; Test Loss: 5.2423e+02. (Time: 12.7s)Step 3500 of 100000; Loss: 5.2439e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.2907e+03; Test Loss: 5.2027e+02. (Time: 14.5s)Step 4000 of 100000; Loss: 5.2046e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.2119e+03; Test Loss: 5.1714e+02. (Time: 13.3s)Step 4500 of 100000; Loss: 5.1735e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.1531e+03; Test Loss: 5.1490e+02. (Time: 13.5s)Step 5000 of 100000; Loss: 5.1489e+02. (Time: 0.0s)\n",
            "Step 500 of 500; Loss: 5.1053e+03; Test Loss: 5.1721e+02. (Time: 13.0s)\n",
            "Stopping early as the loss at step 500 did not improve over step 1.\n"
          ]
        }
      ],
      "source": [
        "optimizer = optax.adam(learning_rate=1e-4)\n",
        "lstm_params, _ = rnn_utils.fit_model(\n",
        "    model_fun=make_lstm,\n",
        "    dataset_train = dataset_m_train,\n",
        "    dataset_test = dataset_m_validate,\n",
        "    loss_fun='categorical',\n",
        "    optimizer= optax.chain(\n",
        "        optax.add_decayed_weights(1e-5),  # L2 正则化\n",
        "        optax.adam(learning_rate=1e-4)  # Adam 优化器\n",
        "    ),\n",
        "    n_steps_per_call=500,\n",
        "    n_steps_max=100000,\n",
        "    early_stop_step=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uzFJBoBDwGo",
        "outputId": "7b960ca6-4521-481e-f321-57190679f27e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalized Likelihoods for LSTM\n",
            "Training Dataset\n",
            "Average Normalized Likelihood: 67.9%\n",
            "Held-Out Dataset\n",
            "Average Normalized Likelihood: 67.9%\n"
          ]
        }
      ],
      "source": [
        "#@title Compute quality-of-fit: Held-out Normalized Likelihood\n",
        "# Compute log-likelihood\n",
        "print('Normalized Likelihoods for LSTM')\n",
        "print('Training Dataset')\n",
        "training_likelihood = compute_log_likelihood(dataset_m_train, make_lstm, lstm_params)\n",
        "print('Held-Out Dataset')\n",
        "testing_likelihood = compute_log_likelihood(dataset_m_test, make_lstm, lstm_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrmkwDvr500Q"
      },
      "source": [
        "## Fit HybridRNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "ZxqxHlpunmjq",
        "outputId": "eea7e289-5507-4996-aa12-0f1c1bf834c1"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-25-1447831139.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m#@title Fit the hybrid RNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m hybrnn_params, _ = rnn_utils.fit_model(\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mmodel_fun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_hybrnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mdataset_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CogModelingRNNsTutorial/rnn_utils.py\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model_fun, dataset_train, dataset_test, optimizer, loss_fun, random_key, n_steps_per_call, n_steps_max, return_all_losses, early_stop_step)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m     params, opt_state, losses = train_model(\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0mmodel_fun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CogModelingRNNsTutorial/rnn_utils.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_fun, dataset_train, dataset_test, optimizer, random_key, opt_state, params, n_steps, penalty_scale, loss_fun, do_plot, truncate_seq_length)\u001b[0m\n\u001b[1;32m    255\u001b[0m       \u001b[0mys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtruncate_seq_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m     \u001b[0mtraining_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36mcache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     (outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked, executable,\n\u001b[0;32m--> 341\u001b[0;31m      pgle_profiler) = _python_pjit_helper(fun, jit_info, *args, **kwargs)\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     maybe_fastpath_data = _get_fastpath_data(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_python_pjit_helper\u001b[0;34m(fun, jit_info, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_python_pjit_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjit_info\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPjitInfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m   \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjit_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs_flat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_infer_params\u001b[0;34m(fun, ji, args, kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m   \u001b[0mentry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_params_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mji\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpjit_mesh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpjit_params\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m     p, args_flat = _infer_params_impl(\n\u001b[0m\u001b[1;32m    731\u001b[0m         fun, ji, pjit_mesh, resource_env, dbg, args, kwargs, in_avals=avals)\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs_tracked\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# if attrs, don't popoulate the cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_infer_params_impl\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    621\u001b[0m   \u001b[0mattr_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_attr_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m   jaxpr, consts, out_avals, attrs_tracked = _create_pjit_jaxpr(\n\u001b[0m\u001b[1;32m    624\u001b[0m       flat_fun, in_type, attr_token, IgnoreKey(ji.inline))\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/linear_util.py\u001b[0m in \u001b[0;36mmemoized_fun\u001b[0;34m(fun, *args)\u001b[0m\n\u001b[1;32m    440\u001b[0m       \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopulate_stores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mexplain\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain_cache_misses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnew_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_create_pjit_jaxpr\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1297\u001b[0m       \u001b[0mattrs_tracked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m       jaxpr, global_out_avals, consts, attrs_tracked = pe.trace_to_jaxpr_dynamic(\n\u001b[0m\u001b[1;32m   1300\u001b[0m           fun, in_type)\n\u001b[1;32m   1301\u001b[0m       \u001b[0;31m# assert attr_data is sentinel or attr_data matches attrs_tracked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_jaxpr_dynamic\u001b[0;34m(fun, in_avals, keep_inputs)\u001b[0m\n\u001b[1;32m   2170\u001b[0m     \u001b[0min_tracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_inputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2171\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_current_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2172\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0min_tracers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2174\u001b[0m     \u001b[0mout_tracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_jaxpr_tracer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;34m\"\"\"Calls the transformed function\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_transformed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/api_util.py\u001b[0m in \u001b[0;36m_argnums_partial\u001b[0;34m(_fun, _dyn_argnums, _fixed_args, *dyn_args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m   \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed_args_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0msentinel\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed_args_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentinel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m def argnames_partial_except(f: lu.WrappedFun, static_argnames: tuple[str, ...],\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/api_util.py\u001b[0m in \u001b[0;36mflatten_fun\u001b[0;34m(f, store, in_tree, *args_flat)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 in_tree: PyTreeDef, *args_flat):\n\u001b[1;32m     72\u001b[0m   \u001b[0mpy_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpy_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m   \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpy_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpy_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m   \u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m   \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/linear_util.py\u001b[0m in \u001b[0;36m_get_result_paths_thunk\u001b[0;34m(_fun, _store, *args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtransformation_with_aux2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_result_paths_thunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_fun\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_store\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mStore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m   \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m   \u001b[0mresult_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_clean_keystr_arg_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerate_key_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_store\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CogModelingRNNsTutorial/rnn_utils.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(params, opt_state, xs, ys, random_key)\u001b[0m\n\u001b[1;32m    236\u001b[0m       \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m   ) -> Tuple[float, Any, Any]:\n\u001b[0;32m--> 238\u001b[0;31m     loss, grads = jax.value_and_grad(compute_loss, argnums=0)(\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     )\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mvalue_and_grad_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    468\u001b[0m       \u001b[0m_check_input_dtype_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mholomorphic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m       \u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvjp_py\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_vjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_partial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdyn_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m       ans, vjp_py, aux = _vjp(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36m_vjp\u001b[0;34m(fun, has_aux, *primals)\u001b[0m\n\u001b[1;32m   2013\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2014\u001b[0m     \u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_fun_nokwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2015\u001b[0;31m     \u001b[0mout_primals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvjp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimals_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2016\u001b[0m     \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2017\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/ad.py\u001b[0m in \u001b[0;36mvjp\u001b[0;34m(traceable, primals, has_aux)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mvjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceable\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWrappedFun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0mout_primals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinearize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mprimals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mout_primals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinearize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mprimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/ad.py\u001b[0m in \u001b[0;36mlinearize\u001b[0;34m(traceable, *primals, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m   \u001b[0mjvpfun_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjvpfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m   \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_pvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_to_jaxpr_nounits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjvpfun_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_pvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m   \u001b[0mout_primals_pvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tangents_pvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_pvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mout_primal_pval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_known\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mout_primal_pval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout_primals_pvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_jaxpr_nounits\u001b[0;34m(fun, pvals, instantiate)\u001b[0m\n\u001b[1;32m    574\u001b[0m       \u001b[0mfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_to_subjaxpr_nounits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstantiate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_current_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m         \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout_pvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;34m\"\"\"Calls the transformed function\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_transformed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_subjaxpr_nounits\u001b[0;34m(f, trace, instantiate, debug_info, in_pvals)\u001b[0m\n\u001b[1;32m    588\u001b[0m     in_pvals: Sequence[PartialVal]):\n\u001b[1;32m    589\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPartialVal\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0min_pvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_pvals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m   out_tracers, jaxpr, out_consts, env = _trace_to_subjaxpr_nounits(\n\u001b[0m\u001b[1;32m    591\u001b[0m       f, trace, instantiate, in_pvals, debug_info)\n\u001b[1;32m    592\u001b[0m   \u001b[0mout_pvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout_tracers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/partial_eval.py\u001b[0m in \u001b[0;36m_trace_to_subjaxpr_nounits\u001b[0;34m(f, trace, instantiate, in_pvals, debug_info)\u001b[0m\n\u001b[1;32m    621\u001b[0m   \u001b[0min_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_lists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_knowns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_consts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_current_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0min_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m   assert isinstance(ans, (list, tuple)), (\n\u001b[1;32m    625\u001b[0m       f\"Got unexpected return type when tracing function to jaxpr: {ans}\")\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/api_util.py\u001b[0m in \u001b[0;36mflatten_fun\u001b[0;34m(f, store, in_tree, *args_flat)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 in_tree: PyTreeDef, *args_flat):\n\u001b[1;32m     72\u001b[0m   \u001b[0mpy_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpy_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m   \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpy_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpy_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m   \u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m   \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/ad.py\u001b[0m in \u001b[0;36mjvpfun\u001b[0;34m(f, instantiate, transform_stack, primals, tangents)\u001b[0m\n\u001b[1;32m     78\u001b[0m          else contextlib.nullcontext())\n\u001b[1;32m     79\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mout_primals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tangents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtangents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstantiate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0minstantiate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minstantiate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tangents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/ad.py\u001b[0m in \u001b[0;36mjvp_subtrace\u001b[0;34m(f, tag, primals, tangents)\u001b[0m\n\u001b[1;32m    118\u001b[0m                   for x, t in zip(primals, tangents)]\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_current_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0min_tracers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munzip2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_primal_tangent_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/api_util.py\u001b[0m in \u001b[0;36mflatten_fun_nokwargs\u001b[0;34m(f, store, in_tree, *args_flat)\u001b[0m\n\u001b[1;32m     88\u001b[0m                          in_tree: PyTreeDef, *args_flat):\n\u001b[1;32m     89\u001b[0m   \u001b[0mpy_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m   \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpy_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m   \u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m   \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/api_util.py\u001b[0m in \u001b[0;36m_argnums_partial\u001b[0;34m(_fun, _dyn_argnums, _fixed_args, *dyn_args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m   \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed_args_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0msentinel\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed_args_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentinel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m def argnames_partial_except(f: lu.WrappedFun, static_argnames: tuple[str, ...],\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/linear_util.py\u001b[0m in \u001b[0;36m_get_result_paths_thunk\u001b[0;34m(_fun, _store, *args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtransformation_with_aux2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_result_paths_thunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_fun\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_store\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mStore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m   \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m   \u001b[0mresult_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_clean_keystr_arg_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenerate_key_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_store\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36mcache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     (outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked, executable,\n\u001b[0;32m--> 341\u001b[0;31m      pgle_profiler) = _python_pjit_helper(fun, jit_info, *args, **kwargs)\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     maybe_fastpath_data = _get_fastpath_data(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_python_pjit_helper\u001b[0;34m(fun, jit_info, *args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m       \u001b[0mout_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompiled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pjit_call_impl_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m       \u001b[0mout_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpjit_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m       \u001b[0mcompiled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m       \u001b[0mprofiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    500\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_canonicalization\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanonicalize_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_true_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_true_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36m_true_bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0mtrace_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m       \u001b[0mtrace_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdef_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/ad.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_current_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m       \u001b[0mprimal_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtangent_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimals_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtangents_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_pjit_jvp\u001b[0;34m(primals_in, tangents_in, jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, compiler_options_kvs)\u001b[0m\n\u001b[1;32m   2041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m   \u001b[0mis_nz_tangents_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZero\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtangents_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m   jaxpr_jvp, is_nz_tangents_out = ad.jvp_jaxpr(\n\u001b[0m\u001b[1;32m   2044\u001b[0m       jaxpr, is_nz_tangents_in, instantiate=False)\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/ad.py\u001b[0m in \u001b[0;36mjvp_jaxpr\u001b[0;34m(jaxpr, nonzeros, instantiate)\u001b[0m\n\u001b[1;32m   1076\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstantiate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[0minstantiate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minstantiate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_avals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_jvp_jaxpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnonzeros\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstantiate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mweakref_lru_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/ad.py\u001b[0m in \u001b[0;36m_jvp_jaxpr\u001b[0;34m(jaxpr, nonzeros, instantiate)\u001b[0m\n\u001b[1;32m   1088\u001b[0m   \u001b[0mtangent_avals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tangent_aval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0maval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_avals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonzeros\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m   \u001b[0mavals_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_avals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtangent_avals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m   jaxpr_out, avals_out, literals_out, () = pe.trace_to_jaxpr_dynamic(\n\u001b[0m\u001b[1;32m   1091\u001b[0m       f_jvp, avals_in)\n\u001b[1;32m   1092\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClosedJaxpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaxpr_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mliterals_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_nonzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_jaxpr_dynamic\u001b[0;34m(fun, in_avals, keep_inputs)\u001b[0m\n\u001b[1;32m   2170\u001b[0m     \u001b[0min_tracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_inputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2171\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_current_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2172\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0min_tracers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2174\u001b[0m     \u001b[0mout_tracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_jaxpr_tracer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;34m\"\"\"Calls the transformed function\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_transformed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/ad.py\u001b[0m in \u001b[0;36mf_jvp_traceable\u001b[0;34m(f, store, nonzeros, *primals_and_nztangents)\u001b[0m\n\u001b[1;32m   1099\u001b[0m   tangents = [next(nonzero_tangents) if nz else Zero.from_primal_value(p)\n\u001b[1;32m   1100\u001b[0m               for p, nz in zip(primals, nonzeros)]\n\u001b[0;32m-> 1101\u001b[0;31m   \u001b[0mprimals_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtangents_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtangents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m   \u001b[0mout_nonzeros\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mZero\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtangents_out\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m   \u001b[0mnonzero_tangents_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtangents_out\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mZero\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/ad.py\u001b[0m in \u001b[0;36mjvpfun\u001b[0;34m(f, instantiate, transform_stack, primals, tangents)\u001b[0m\n\u001b[1;32m     78\u001b[0m          else contextlib.nullcontext())\n\u001b[1;32m     79\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mout_primals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tangents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtangents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstantiate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0minstantiate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minstantiate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tangents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/ad.py\u001b[0m in \u001b[0;36mjvp_subtrace\u001b[0;34m(f, tag, primals, tangents)\u001b[0m\n\u001b[1;32m    118\u001b[0m                   for x, t in zip(primals, tangents)]\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_current_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0min_tracers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munzip2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_primal_tangent_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mjaxpr_as_fun\u001b[0;34m(closed_jaxpr, *args)\u001b[0m\n\u001b[1;32m    274\u001b[0m   \u001b[0;31m# round-trip prevents those ops producing spurious errors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug_nans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0meval_jaxpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosed_jaxpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosed_jaxpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36meval_jaxpr\u001b[0;34m(jaxpr, consts, propagate_source_info, *args)\u001b[0m\n\u001b[1;32m    588\u001b[0m     with source_info_util.user_context(\n\u001b[1;32m    589\u001b[0m         traceback, name_stack=name_stack), eqn.ctx.manager:\n\u001b[0;32m--> 590\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msubfuns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mbind_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m       \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutvars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    500\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_canonicalization\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanonicalize_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_true_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_true_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36m_true_bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0mtrace_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m       \u001b[0mtrace_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdef_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/ad.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_current_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m       \u001b[0mprimal_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtangent_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimals_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtangents_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_pjit_jvp\u001b[0;34m(primals_in, tangents_in, jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, compiler_options_kvs)\u001b[0m\n\u001b[1;32m   2041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m   \u001b[0mis_nz_tangents_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZero\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtangents_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m   jaxpr_jvp, is_nz_tangents_out = ad.jvp_jaxpr(\n\u001b[0m\u001b[1;32m   2044\u001b[0m       jaxpr, is_nz_tangents_in, instantiate=False)\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/ad.py\u001b[0m in \u001b[0;36mjvp_jaxpr\u001b[0;34m(jaxpr, nonzeros, instantiate)\u001b[0m\n\u001b[1;32m   1076\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstantiate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m     \u001b[0minstantiate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minstantiate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_avals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_jvp_jaxpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnonzeros\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstantiate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mweakref_lru_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/ad.py\u001b[0m in \u001b[0;36m_jvp_jaxpr\u001b[0;34m(jaxpr, nonzeros, instantiate)\u001b[0m\n\u001b[1;32m   1088\u001b[0m   \u001b[0mtangent_avals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tangent_aval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0maval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_avals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonzeros\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m   \u001b[0mavals_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_avals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtangent_avals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1090\u001b[0;31m   jaxpr_out, avals_out, literals_out, () = pe.trace_to_jaxpr_dynamic(\n\u001b[0m\u001b[1;32m   1091\u001b[0m       f_jvp, avals_in)\n\u001b[1;32m   1092\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClosedJaxpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjaxpr_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mliterals_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_nonzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_jaxpr_dynamic\u001b[0;34m(fun, in_avals, keep_inputs)\u001b[0m\n\u001b[1;32m   2170\u001b[0m     \u001b[0min_tracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_inputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2171\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_current_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2172\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0min_tracers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2174\u001b[0m     \u001b[0mout_tracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_jaxpr_tracer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;34m\"\"\"Calls the transformed function\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_transformed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/ad.py\u001b[0m in \u001b[0;36mf_jvp_traceable\u001b[0;34m(f, store, nonzeros, *primals_and_nztangents)\u001b[0m\n\u001b[1;32m   1099\u001b[0m   tangents = [next(nonzero_tangents) if nz else Zero.from_primal_value(p)\n\u001b[1;32m   1100\u001b[0m               for p, nz in zip(primals, nonzeros)]\n\u001b[0;32m-> 1101\u001b[0;31m   \u001b[0mprimals_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtangents_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtangents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m   \u001b[0mout_nonzeros\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mZero\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtangents_out\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m   \u001b[0mnonzero_tangents_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtangents_out\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mZero\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/ad.py\u001b[0m in \u001b[0;36mjvpfun\u001b[0;34m(f, instantiate, transform_stack, primals, tangents)\u001b[0m\n\u001b[1;32m     78\u001b[0m          else contextlib.nullcontext())\n\u001b[1;32m     79\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mout_primals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tangents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtangents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstantiate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0minstantiate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minstantiate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tangents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/ad.py\u001b[0m in \u001b[0;36mjvp_subtrace\u001b[0;34m(f, tag, primals, tangents)\u001b[0m\n\u001b[1;32m    118\u001b[0m                   for x, t in zip(primals, tangents)]\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_current_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0min_tracers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munzip2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_primal_tangent_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mjaxpr_as_fun\u001b[0;34m(closed_jaxpr, *args)\u001b[0m\n\u001b[1;32m    274\u001b[0m   \u001b[0;31m# round-trip prevents those ops producing spurious errors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug_nans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0meval_jaxpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosed_jaxpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosed_jaxpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36meval_jaxpr\u001b[0;34m(jaxpr, consts, propagate_source_info, *args)\u001b[0m\n\u001b[1;32m    588\u001b[0m     with source_info_util.user_context(\n\u001b[1;32m    589\u001b[0m         traceback, name_stack=name_stack), eqn.ctx.manager:\n\u001b[0;32m--> 590\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msubfuns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mbind_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m       \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutvars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    500\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_canonicalization\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcanonicalize_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_true_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_true_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36m_true_bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0mtrace_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m       \u001b[0mtrace_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36mbind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdef_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/interpreters/ad.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_current_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m       \u001b[0mprimal_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtangent_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjvp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimals_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtangents_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/pjit.py\u001b[0m in \u001b[0;36m_pjit_jvp\u001b[0;34m(primals_in, tangents_in, jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, compiler_options_kvs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m   tangents_in = [ad_util.zeros_like_aval(a) if isinstance(a, AbstractRef) else x\n\u001b[0;32m-> 2040\u001b[0;31m                  for x, a in zip(tangents_in, jaxpr.in_avals)]\n\u001b[0m\u001b[1;32m   2041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m   \u001b[0mis_nz_tangents_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZero\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtangents_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/core.py\u001b[0m in \u001b[0;36min_avals\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0min_avals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjaxpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 在这里就是定义几种不同的hybridRNN了，我主要是通过控制use_hidden_state和use_previous_values来控制的\n",
        "# RL-ANN: 这是最简单的HybRNN，use_hidden_state = 'False', use_previous_values = 'False'。 我们只是把rescolar的学习率用MLP替代了，不给他额外的输入。\n",
        "# Context-ANN: use_hidden_state = 'False', use_previous_values = 'True'。这里我们相当于给RL-ANN额外加一个，另外选项的Value作为额外的输入，补足context信息。\n",
        "# Memory-ANN: use_hidden_state = 'True', use_previous_values = 'False'。这里给RL-ANN额外加上一步所有的hidden-units的激活，作为额外的输入。\n",
        "# 其他的参数都不用改！！\n",
        "use_hidden_state = 'False'\n",
        "use_previous_values = 'False'\n",
        "fit_forget = \"False\"\n",
        "habit_weight = 1\n",
        "\n",
        "value_weight = 1.  # This is needed for it to be doing RL\n",
        "\n",
        "rnn_rl_params = {\n",
        "    's': use_hidden_state == 'True',\n",
        "    'o': use_previous_values == 'True',\n",
        "    'fit_forget': fit_forget == 'True',\n",
        "    'forget': 0.,\n",
        "    'w_h': habit_weight,\n",
        "    'w_v': value_weight}\n",
        "network_params = {'n_actions': 2, 'hidden_size': 8}\n",
        "\n",
        "def make_hybrnn():\n",
        "  model = hybrnn.BiRNN(rl_params=rnn_rl_params, network_params=network_params)\n",
        "  return model\n",
        "\n",
        "\n",
        "#从这里就是运行训练代码了\n",
        "hybrnn_params, _ = rnn_utils.fit_model(\n",
        "    model_fun=make_hybrnn,\n",
        "    dataset_train=dataset_train,\n",
        "    dataset_test=dataset_validate,\n",
        "    loss_fun='categorical',\n",
        "    optimizer=optax.chain(\n",
        "        optax.add_decayed_weights(1e-5),  # L2 正则化\n",
        "        optax.adam(learning_rate=1e-4)  # Adam 优化器\n",
        "    ),\n",
        "    n_steps_per_call=200,\n",
        "    n_steps_max=200000,\n",
        "    early_stop_step=200\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rz4n-sUi8F63"
      },
      "outputs": [],
      "source": [
        "# 这个是计算模型里面有多少参数用的\n",
        "def count_parameters_jax(params):\n",
        "    \"\"\"\n",
        "    Count the number of parameters in a JAX model.\n",
        "\n",
        "    Args:\n",
        "        params: A nested dictionary of model parameters.\n",
        "\n",
        "    Returns:\n",
        "        Total number of parameters.\n",
        "    \"\"\"\n",
        "    total_params = 0\n",
        "\n",
        "    # 递归遍历字典，计算所有参数的数量\n",
        "    def count_nested_params(p):\n",
        "        nonlocal total_params\n",
        "        if isinstance(p, dict):\n",
        "            for v in p.values():\n",
        "                count_nested_params(v)\n",
        "        else:\n",
        "            total_params += np.prod(p.shape)  # 累加参数的形状\n",
        "    count_nested_params(params)\n",
        "    return total_params\n",
        "\n",
        "# Example usage\n",
        "# Assuming `model_params` is your model's parameter dictionary\n",
        "# num_parameters = count_parameters_jax(conann_params)\n",
        "# print(f\"Number of Parameters: {num_parameters}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzLEK1MF8vn9"
      },
      "outputs": [],
      "source": [
        "# AIC指标，我们不需要，不用跑\n",
        "def compute_aic(dataset, model_fun, params, num_parameters):\n",
        "    \"\"\"\n",
        "    Compute the Akaike Information Criterion (AIC) for a given model.\n",
        "\n",
        "    Args:\n",
        "        dataset: The dataset generator.\n",
        "        model_fun: The model function.\n",
        "        params: The model parameters.\n",
        "        num_parameters: The number of free parameters in the model.\n",
        "\n",
        "    Returns:\n",
        "        aic_score: The AIC score for the entire dataset.\n",
        "    \"\"\"\n",
        "    xs, actual_choices = next(dataset)\n",
        "    n_trials_per_session, n_sessions = actual_choices.shape[:2]\n",
        "    model_outputs, model_states = rnn_utils.eval_model(model_fun, params, xs)\n",
        "\n",
        "    predicted_log_choice_probabilities = np.array(jax.nn.log_softmax(model_outputs[:, :, :2]))\n",
        "\n",
        "    total_log_likelihood = 0\n",
        "    total_n = 0  # Total number of trials across all sessions\n",
        "\n",
        "    for sess_i in range(n_sessions):\n",
        "        session_log_likelihood = 0\n",
        "        session_n = 0  # Number of valid trials in this session\n",
        "        for trial_i in range(n_trials_per_session):\n",
        "            actual_choice = int(actual_choices[trial_i, sess_i])\n",
        "            if actual_choice >= 0:  # Ignore invalid trials\n",
        "                session_log_likelihood += predicted_log_choice_probabilities[trial_i, sess_i, actual_choice]\n",
        "                session_n += 1\n",
        "\n",
        "        # Update totals across sessions\n",
        "        total_log_likelihood += session_log_likelihood\n",
        "        total_n += session_n\n",
        "\n",
        "    # Compute AIC using the total log-likelihood\n",
        "    aic_score = 2 * num_parameters - 2 * total_log_likelihood\n",
        "\n",
        "    print(f'AIC Score: {aic_score:.2f}')\n",
        "    return aic_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zr6BXRVBJlzH"
      },
      "outputs": [],
      "source": [
        "#@title Compute quality-of-fit: Held-out Normalized Likelihood\n",
        "# Compute log-likelihood\n",
        "print('Normalized Likelihoods for LSTM')\n",
        "print('Training Dataset')\n",
        "training_likelihood = compute_log_likelihood(dataset_m_train, make_hybrnn, hybrnn_params)\n",
        "print('Held-Out Dataset')\n",
        "testing_likelihood = compute_log_likelihood(dataset_m_test, make_hybrnn, hybrnn_params)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "v8sai1OhWYy3",
        "WmgO7HywNl5-",
        "GvGdnab8CdCN",
        "mrmkwDvr500Q"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
