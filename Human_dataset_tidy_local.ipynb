{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7x3Ff2xWTdb"
      },
      "source": [
        "# Import and preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mYuMvSuvyGuR"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "import gdown\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CPp79Z477SU8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotnine as gg\n",
        "gg.theme_set(gg.theme_classic)  # for nicer-looking plots\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "import optax\n",
        "import scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CpuDevice(id=0)]\n"
          ]
        }
      ],
      "source": [
        "devices = jax.devices()\n",
        "print(devices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWQY_uJK7Uyx",
        "outputId": "983c0b42-ee55-48ed-e507-ccba22a8289f"
      },
      "outputs": [],
      "source": [
        "# !pip install -U dm-haiku\n",
        "import haiku as hk\n",
        "rng_seq = hk.PRNGSequence(np.random.randint(2**32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX2twY1NNT1r",
        "outputId": "c73fd2f4-31d7-470b-caf5-2d9dc712511d"
      },
      "outputs": [],
      "source": [
        "# #@title Install required packages.\n",
        "# try:\n",
        "#     from google.colab import files\n",
        "#     _ON_COLAB = True\n",
        "# except:\n",
        "#     _ON_COLAB = False\n",
        "\n",
        "# if _ON_COLAB:\n",
        "#   !rm -rf CogModelingRNNsTutorial\n",
        "#   !git clone https://github.com/YifeiCAO/CogModelingRNNsTutorial\n",
        "#   !pip install -e CogModelingRNNsTutorial/CogModelingRNNsTutorial\n",
        "#   !cp CogModelingRNNsTutorial/CogModelingRNNsTutorial/*py CogModelingRNNsTutorial\n",
        "# else:\n",
        "#   !pip install CogModelingRNNsTutorial/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_2jivtclNVjS"
      },
      "outputs": [],
      "source": [
        "#@title Imports + defaults settings.\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "# for reload\n",
        "# %reload_ext autoreload\n",
        "\n",
        "# import haiku as hk\n",
        "# import jax\n",
        "# import jax.numpy as jnp\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optax\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# try:\n",
        "#     from google.colab import files\n",
        "#     _ON_COLAB = True\n",
        "# except:\n",
        "#     _ON_COLAB = False\n",
        "\n",
        "from CogModelingRNNsTutorial import bandits\n",
        "from CogModelingRNNsTutorial import disrnn\n",
        "from CogModelingRNNsTutorial import hybrnn\n",
        "from CogModelingRNNsTutorial import hybconrnn\n",
        "from CogModelingRNNsTutorial import hybrnn_direct_con\n",
        "from CogModelingRNNsTutorial import plotting\n",
        "from CogModelingRNNsTutorial import rat_data\n",
        "from CogModelingRNNsTutorial import rnn_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyjWR7YmyPQB",
        "outputId": "0101e98d-62cc-42d5-a6d9-80fd8c89a044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File downloaded and read successfully!\n",
            "xs.shape = (60, 206, 2)\n",
            "ys.shape = (60, 206, 1)\n"
          ]
        }
      ],
      "source": [
        "osf_url = 'https://osf.io/xe6yu/download?direct=1'\n",
        "response = requests.get(osf_url)\n",
        "\n",
        "# Check the request\n",
        "if response.status_code == 200:\n",
        "    # Read as pandas dataframe\n",
        "    qasim_data = pd.read_csv(StringIO(response.text))\n",
        "    print('File downloaded and read successfully!')\n",
        "else:\n",
        "    print('Failed to download file. Status code:', response.status_code)\n",
        "\n",
        "qasim_data.head()\n",
        "\n",
        "# # read data\n",
        "selected_columns = ['participant', 'trials_gamble', 'gamble', 'prob', 'reward']\n",
        "\n",
        "qasim = qasim_data[selected_columns]\n",
        "qasim_filtered = qasim[qasim['trials_gamble'].notna()]\n",
        "qasim_sorted = qasim_filtered.groupby('participant', group_keys=False).apply(lambda x: x.sort_values('trials_gamble'))\n",
        "qasim_sorted = qasim_sorted.reset_index(drop=True)\n",
        "qasim_sorted['participant'] = qasim_sorted.groupby(['participant']).ngroup() + 1\n",
        "qasim_sorted['action'] = qasim_sorted['gamble']\n",
        "qasim_sorted\n",
        "\n",
        "# —— 2) 把缺失的 action 填成 -1 —— #\n",
        "qasim_sorted['action'] = qasim_sorted['action'].fillna(-1).astype(int)\n",
        "\n",
        "# —— 3) 如果需要，把 reward 映射到 0/1 —— #\n",
        "# （如果已经是 0/1 可跳过；否则取消下面注释并调整映射字典）\n",
        "# qasim_sorted['reward'] = (\n",
        "#     qasim_sorted['reward']\n",
        "#     .map({-1: 0, 1: 1})\n",
        "#     .fillna(-1)\n",
        "#     .astype(int)\n",
        "# )\n",
        "\n",
        "# —— 4) 排序，确保 trial 顺序 —— #\n",
        "qasim_sorted = qasim_sorted.sort_values(\n",
        "    ['participant', 'trials_gamble']\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# —— 5) 生成下一步动作 action_n —— #\n",
        "qasim_sorted['action_n'] = (\n",
        "    qasim_sorted\n",
        "    .groupby('participant')['action']\n",
        "    .shift(-1)\n",
        ")\n",
        "# 每个 participant 最后一 trial 的 action_n 设为 -1\n",
        "last_idxs = qasim_sorted.groupby('participant').tail(1).index\n",
        "qasim_sorted.loc[last_idxs, 'action_n'] = -1\n",
        "\n",
        "# —— 6) 按 participant 构造 xs_list, ys_list —— #\n",
        "xs_list, ys_list = [], []\n",
        "for pid, grp in qasim_sorted.groupby('participant'):\n",
        "    grp = grp.sort_values('trials_gamble')\n",
        "    x = grp[['prob', 'reward']].to_numpy().astype(float)    # 输入特征\n",
        "    y = grp[['action_n']].to_numpy().astype(int)             # 下一步动作\n",
        "    xs_list.append(x)\n",
        "    ys_list.append(y)\n",
        "\n",
        "# —— 7) 堆成三维数组 —— #\n",
        "xs_qa = np.stack(xs_list, axis=1)  # (n_sessions, n_trials, 2)\n",
        "ys_qa = np.stack(ys_list, axis=1)  # (n_sessions, n_trials, 1)\n",
        "\n",
        "print(\"xs.shape =\", xs_qa.shape)\n",
        "print(\"ys.shape =\", ys_qa.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNCQ2haXljHL"
      },
      "source": [
        "### Sidarus dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zetxo6c_lgFd",
        "outputId": "f8f0f5e1-0f46-452d-ce7e-87a314963918"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1TSV6CdyClKz831qD2ln4z3WjLLcOgG1n\n",
            "To: /Users/yifei/Desktop/DeepMind_project/CogModelingRNNsTutorial/sidarus_data.csv\n",
            "100%|██████████| 653k/653k [00:00<00:00, 12.6MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "xs.shape = (800, 40, 2)\n",
            "ys.shape = (800, 40, 1)\n"
          ]
        }
      ],
      "source": [
        "# 修改后的 file_id\n",
        "file_id = '1TSV6CdyClKz831qD2ln4z3WjLLcOgG1n'\n",
        "download_url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "\n",
        "# 下载并保存为 'downloaded_file.csv'\n",
        "output_file = 'sidarus_data.csv'\n",
        "gdown.download(download_url, output_file, quiet=False, fuzzy=True)\n",
        "\n",
        "# 读取 CSV 数据\n",
        "sida_data = pd.read_csv(output_file)\n",
        "sida_data\n",
        "\n",
        "# 1) action 从 {1,2} → {0,1}，并把所有缺失值填成 -1\n",
        "sida_data['action'] = (\n",
        "    sida_data['action']\n",
        "    .map({1: 0, 2: 1})    # 映射\n",
        "    .fillna(-1)           # 缺失值设为 -1\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "# 2) outcome 从 {-1,1} → {0,1}，缺失也填成 -1\n",
        "sida_data['reward'] = (\n",
        "    sida_data['outcome']\n",
        "    .map({-1: 0, 1: 1})\n",
        "    .fillna(-1)\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "# 3) 给每个被试×session 分配一个连续的 session_id\n",
        "sida_data['session_id'] = (\n",
        "    sida_data\n",
        "    .groupby(['subj', 'session'], sort=False)\n",
        "    .ngroup()\n",
        "    + 1\n",
        ")\n",
        "\n",
        "# 4) 排序，保证 trial 顺序\n",
        "sida_data = sida_data.sort_values(\n",
        "    ['session_id', 'epN', 'epTrialN']\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# 5) 生成下一步动作 action_n；每个 session 末尾设为 -1\n",
        "sida_data['action_n'] = (\n",
        "    sida_data\n",
        "    .groupby('session_id')['action']\n",
        "    .shift(-1)\n",
        ")\n",
        "last_idx = sida_data.groupby('session_id').tail(1).index\n",
        "sida_data.loc[last_idx, 'action_n'] = -1\n",
        "\n",
        "# 6) 按 session_id 抽取序列，并堆成 xs, ys\n",
        "xs_list, ys_list = [], []\n",
        "for sid, grp in sida_data.groupby('session_id'):\n",
        "    grp = grp.sort_values(['epN', 'epTrialN'])\n",
        "    x = grp[['hiRewAct', 'reward']].to_numpy().astype(float)  # 特征\n",
        "    y = grp[['action_n']].to_numpy().astype(int)              # Label\n",
        "    xs_list.append(x)\n",
        "    ys_list.append(y)\n",
        "\n",
        "# 最终结果：xs.shape == (n_sessions, n_trials, 2)，ys.shape == (n_sessions, n_trials, 1)\n",
        "xs_sida = np.stack(xs_list, axis=1)\n",
        "ys_sida = np.stack(ys_list, axis=1)\n",
        "\n",
        "print(\"xs.shape =\", xs_sida.shape)\n",
        "print(\"ys.shape =\", ys_sida.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "147k59EcluMe"
      },
      "source": [
        "### Schaaf dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRUXDOuEltlO",
        "outputId": "4d607a65-5003-4439-f314-daa193744c6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rJFmDhCE3fdXtSHSSvtre_7V49gGsg7P\n",
            "To: /Users/yifei/Desktop/DeepMind_project/CogModelingRNNsTutorial/schaaf_data.csv\n",
            "100%|██████████| 374k/374k [00:00<00:00, 3.52MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "xs.shape = (249, 94, 2)\n",
            "ys.shape = (249, 94, 1)\n"
          ]
        }
      ],
      "source": [
        "# 修改后的 file_id\n",
        "file_id = '1rJFmDhCE3fdXtSHSSvtre_7V49gGsg7P'\n",
        "download_url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "\n",
        "# 下载并保存为 'downloaded_file.csv'\n",
        "output_file = 'schaaf_data.csv'\n",
        "gdown.download(download_url, output_file, quiet=False, fuzzy=True)\n",
        "\n",
        "# 读取 CSV 数据\n",
        "schaaf_data = pd.read_csv(output_file)\n",
        "schaaf_data\n",
        "\n",
        "df1 = schaaf_data[['pp', 'trial1', 'response1', 'outcome1']].copy()\n",
        "df1.columns = ['pp', 'trial_in_session', 'response', 'reward']\n",
        "df1['session'] = 1\n",
        "\n",
        "df2 = schaaf_data[['pp', 'trial2', 'response2', 'outcome2']].copy()\n",
        "df2.columns = ['pp', 'trial_in_session', 'response', 'reward']\n",
        "df2['session'] = 2\n",
        "\n",
        "# —— 上面拆分、concat、sort 的部分保持不变 —— #\n",
        "\n",
        "# 合并成长表\n",
        "df_long = pd.concat([df1, df2], ignore_index=True)\n",
        "df_long = df_long.sort_values(['pp', 'session', 'trial_in_session']).reset_index(drop=True)\n",
        "\n",
        "# —— 映射 outcome，再填 missing response —— #\n",
        "# 把原来的 response1/response2 改名后是 `response`\n",
        "# 把 outcome1/outcome2 改名后是 `reward`\n",
        "# 把 outcome 映射成 1/0，然后把原本缺失的 reward 补成 -1\n",
        "df_long['reward'] = (\n",
        "    df_long['reward']\n",
        "    .map({1: 1, -1: 0})\n",
        "    .fillna(-1)\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "# 把缺失的 response 一样补成 -1\n",
        "df_long['response'] = (\n",
        "    df_long['response']\n",
        "    .fillna(-1)\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "\n",
        "# 接下来再做 session_id、shift action_n、以及 stack xs/ys 的流程……\n",
        "df_long['session_id'] = (df_long['pp'] - 1) * 2 + df_long['session']\n",
        "df_long['action_n'] = df_long.groupby('session_id')['response'].shift(-1)\n",
        "last_idx = df_long.groupby('session_id').tail(1).index\n",
        "df_long.loc[last_idx, 'action_n'] = -1\n",
        "\n",
        "# 生成 xs, ys\n",
        "session_ids = df_long['session_id'].unique()\n",
        "xs_list, ys_list = [], []\n",
        "for sid in session_ids:\n",
        "    sd = df_long[df_long['session_id'] == sid].sort_values('trial_in_session')\n",
        "    x = sd[['response', 'reward']].to_numpy().astype(float)\n",
        "    y = sd[['action_n']].to_numpy().astype(int)\n",
        "    xs_list.append(x)\n",
        "    ys_list.append(y)\n",
        "\n",
        "xs = np.stack(xs_list, axis=0)   # (n_sessions, n_trials, 2)\n",
        "ys = np.stack(ys_list, axis=0)   # (n_sessions, n_trials, 1)\n",
        "\n",
        "\n",
        "# 或者第 0 维是 trials，第 1 维是 sessions：\n",
        "xs_sch = np.stack(xs_list, axis=1)  # (n_trials, n_sessions, 2)\n",
        "ys_sch = np.stack(ys_list, axis=1)  # (n_trials, n_sessions, 1)\n",
        "\n",
        "print(\"xs.shape =\", xs_sch.shape)\n",
        "print(\"ys.shape =\", ys_sch.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8KQplq3tHC7"
      },
      "source": [
        "### Maria Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDBAI_AwD1PF",
        "outputId": "a54a9388-a125-4948-b27e-e97bc0ba8ab5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1N_zAy-qrbfjvF8Kbb504IH2JNhR5KI-P\n",
            "To: /Users/yifei/Desktop/DeepMind_project/CogModelingRNNsTutorial/maria_data.csv\n",
            "100%|██████████| 2.57M/2.57M [00:00<00:00, 15.1MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "xs_ma.shape = (120, 305, 2)\n",
            "ys_ma.shape = (120, 305, 1)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gdown\n",
        "\n",
        "# —— 1) 下载并读入原始数据 —— #\n",
        "file_id = '1N_zAy-qrbfjvF8Kbb504IH2JNhR5KI-P'\n",
        "url     = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "gdown.download(url, 'maria_data.csv', quiet=False, fuzzy=True)\n",
        "eck_data = pd.read_csv('maria_data.csv')\n",
        "\n",
        "# —— 2) 筛选＆重命名列 —— #\n",
        "sel = ['sID','TrialID','selected_box','reward']\n",
        "eck_sorted = eck_data[sel].copy()\n",
        "eck_sorted['participant'] = eck_sorted.groupby('sID').ngroup()+1\n",
        "eck_sorted['action']      = eck_sorted['selected_box']\n",
        "\n",
        "# 只保留至少做够 120 试次的被试\n",
        "max_trial = eck_sorted.groupby('participant')['TrialID'].transform('max')\n",
        "eck_sorted = eck_sorted[max_trial>=120]\n",
        "\n",
        "# 只用前 120 试次\n",
        "eck_sorted = eck_sorted[eck_sorted['TrialID']<=120].reset_index(drop=True)\n",
        "\n",
        "# —— 3) 生成下一步动作 action_n —— #\n",
        "def generate_action_n(group):\n",
        "    group = group.sort_values('TrialID')\n",
        "    group['action_n'] = group['action'].shift(-1).fillna(-1).astype(int)\n",
        "    return group\n",
        "\n",
        "eck_sorted = (\n",
        "    eck_sorted\n",
        "    .groupby('participant', group_keys=False)\n",
        "    .apply(generate_action_n)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# —— 4) 按 participant 构造 xs_list/ys_list —— #\n",
        "xs_list, ys_list = [], []\n",
        "for pid, grp in eck_sorted.groupby('participant'):\n",
        "    grp = grp.sort_values('TrialID').iloc[:120]\n",
        "    x = grp[['action','reward']].to_numpy().astype(float)  # (120,2)\n",
        "    y = grp[['action_n']].to_numpy().astype(int)           # (120,1)\n",
        "    xs_list.append(x)\n",
        "    ys_list.append(y)\n",
        "\n",
        "# —— 5) stack 成 (n_sessions, n_trials, feat_dim) —— #\n",
        "xs_ma = np.stack(xs_list, axis=1)  # (305,120,2)\n",
        "ys_ma = np.stack(ys_list, axis=1)  # (305,120,1)\n",
        "\n",
        "print(\"xs_ma.shape =\", xs_ma.shape)\n",
        "print(\"ys_ma.shape =\", ys_ma.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbm2178XrKVd"
      },
      "source": [
        "### Generate a big human dataset with all experiments, session length is 130 trial per session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ9SycOgtbsK",
        "outputId": "9650c98e-c381-4581-eda2-0f4849543525"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total segments: 979\n",
            "First xs segment shape: (130, 2)\n",
            "First ys segment shape: (130, 1)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def segment_and_pad(x: np.ndarray,\n",
        "                    y: np.ndarray,\n",
        "                    seg_len: int = 130,\n",
        "                    pad_x: float = 0.,\n",
        "                    pad_y: int = -1):\n",
        "    T, D = x.shape\n",
        "    n_segs = int(np.ceil(T / seg_len))\n",
        "    x_segs, y_segs = [], []\n",
        "    for i in range(n_segs):\n",
        "        start = i * seg_len\n",
        "        end = start + seg_len\n",
        "        x_part = x[start : min(end, T)]\n",
        "        y_part = y[start : min(end, T)]\n",
        "        pad = end - min(end, T)\n",
        "        if pad > 0:\n",
        "            x_part = np.pad(x_part,\n",
        "                            pad_width=((0, pad), (0, 0)),\n",
        "                            constant_values=pad_x)\n",
        "            y_part = np.pad(y_part,\n",
        "                            pad_width=((0, pad), (0, 0)),\n",
        "                            constant_values=pad_y)\n",
        "        x_segs.append(x_part)\n",
        "        y_segs.append(y_part)\n",
        "    return x_segs, y_segs\n",
        "\n",
        "# —— 假设你已经有这几组 (xs, ys) —— #\n",
        "# xs_qa   (60,  206, 2), ys_qa  (60,206, 1)\n",
        "# xs_sida (800, 40,  2), ys_sida(800,40, 1)\n",
        "# xs_sch  (249, 44,  2), ys_sch (249,44, 1)\n",
        "# xs_ma   (120,305, 2), ys_ma  (120,305,1)\n",
        "\n",
        "all_xs, all_ys = [], []\n",
        "\n",
        "for xs, ys in [(xs_qa, ys_qa),\n",
        "               (xs_sida, ys_sida),\n",
        "               (xs_sch, ys_sch),\n",
        "               (xs_ma, ys_ma)\n",
        "               ]:\n",
        "\n",
        "    # 如果是 (n_trials, n_sessions, feat) 维度，就直接：\n",
        "    # T, N, D = xs.shape\n",
        "    # 否则若是 (n_sessions, n_trials, feat)，先转：\n",
        "    # xs = xs.transpose(1,0,2)\n",
        "    # ys = ys.transpose(1,0,2)\n",
        "\n",
        "    T, N, D = xs.shape\n",
        "    for sess in range(N):\n",
        "        x_seq = xs[:, sess, :]  # (T, D)\n",
        "        y_seq = ys[:, sess, :]  # (T, 1)\n",
        "        x_segs, y_segs = segment_and_pad(\n",
        "            x_seq, y_seq,\n",
        "            seg_len=130,\n",
        "            pad_x=0., pad_y=-1\n",
        "        )\n",
        "        all_xs.extend(x_segs)\n",
        "        all_ys.extend(y_segs)\n",
        "\n",
        "# —— 修改在这里：不要 np.stack，直接输出列表 —— #\n",
        "# all_xs 是一个 Python list，长度 = 总片段数，每个元素 shape=(130, D)\n",
        "# all_ys 是一个 Python list，长度 = 总片段数，每个元素 shape=(130, 1)\n",
        "\n",
        "print(f\"Total segments: {len(all_xs)}\")\n",
        "print(f\"First xs segment shape: {all_xs[0].shape}\")\n",
        "print(f\"First ys segment shape: {all_ys[0].shape}\")\n",
        "\n",
        "# 如果你需要把它们返回成变量：\n",
        "xs_segment_list = all_xs\n",
        "ys_segment_list = all_ys\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Y-JHKuQv48Ur"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def format_into_datasets_multi_source(\n",
        "    xs_list: list[np.ndarray],\n",
        "    ys_list: list[np.ndarray],\n",
        "    dataset_constructor,\n",
        "    n_train_sessions: int,\n",
        "    n_test_sessions: int,\n",
        "    n_validate_sessions: int,\n",
        "    batch_size: int = None,\n",
        "    random_seed: int = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    按照 QA、SIDA、SCH、MA 这 4 个来源的数据源比例，\n",
        "    在它们各自内部抽取 train/test/val session，\n",
        "    最后拼成全局的 DatasetRNN。\n",
        "\n",
        "    xs_list, ys_list:\n",
        "      长度 4 的 list，每个元素形状是 (timesteps, n_sessions_i, feat)\n",
        "    n_*_sessions:\n",
        "      全局希望 train/test/val 一共要多少 session\n",
        "    \"\"\"\n",
        "    if random_seed is not None:\n",
        "        rng = np.random.RandomState(random_seed)\n",
        "    else:\n",
        "        rng = np.random\n",
        "\n",
        "    # 1) 计算每个来源各有多少 session\n",
        "    sess_counts = np.array([xs.shape[1] for xs in xs_list])  # e.g. [206, 40, 44, 305]\n",
        "    total_sessions = sess_counts.sum()\n",
        "\n",
        "    # 2) 按比例分配到每个来源的 train/test/val 数目\n",
        "    def proportional_alloc(total, counts):\n",
        "        floats = counts / counts.sum() * total\n",
        "        floors = np.floor(floats).astype(int)\n",
        "        rem = total - floors.sum()\n",
        "        # 剩余的按余数最大的那些来源补齐\n",
        "        remainders = floats - floors\n",
        "        for idx in np.argsort(remainders)[-rem:]:\n",
        "            floors[idx] += 1\n",
        "        return floors\n",
        "\n",
        "    n_train_per = proportional_alloc(n_train_sessions,    sess_counts)\n",
        "    n_test_per  = proportional_alloc(n_test_sessions,     sess_counts)\n",
        "    n_val_per   = proportional_alloc(n_validate_sessions, sess_counts)\n",
        "\n",
        "    # 3) 在每个来源内部随机打乱并切分\n",
        "    train_idx_list, test_idx_list, val_idx_list = [], [], []\n",
        "    for cnt, n_tr, n_te, n_va in zip(sess_counts,\n",
        "                                     n_train_per,\n",
        "                                     n_test_per,\n",
        "                                     n_val_per):\n",
        "        all_idx = np.arange(cnt)\n",
        "        rng.shuffle(all_idx)\n",
        "        train_idx_list.append(all_idx[:n_tr])\n",
        "        test_idx_list.append( all_idx[n_tr:n_tr+n_te] )\n",
        "        val_idx_list.append(  all_idx[n_tr+n_te:n_tr+n_te+n_va] )\n",
        "\n",
        "    # 4) 汇总抽到的 sessions：concat 出全局 xs/ys\n",
        "    def gather(xs_list, ys_list, idx_lists):\n",
        "        parts_x, parts_y = [], []\n",
        "        for xs, ys, idx in zip(xs_list, ys_list, idx_lists):\n",
        "            # xs: (timesteps, n_sessions_i, feat)\n",
        "            parts_x.append(xs[:, idx, :])\n",
        "            parts_y.append(ys[:, idx, :])\n",
        "        return np.concatenate(parts_x, axis=1), np.concatenate(parts_y, axis=1)\n",
        "\n",
        "    xs_train, ys_train = gather(xs_list, ys_list, train_idx_list)\n",
        "    xs_test,  ys_test  = gather(xs_list, ys_list, test_idx_list)\n",
        "    xs_val,   ys_val   = gather(xs_list, ys_list, val_idx_list)\n",
        "\n",
        "    # 5) 构造 DatasetRNN\n",
        "    ds_train = dataset_constructor(xs_train, ys_train, batch_size=batch_size)\n",
        "    ds_test  = dataset_constructor(xs_test,  ys_test,  batch_size=batch_size)\n",
        "    ds_val   = dataset_constructor(xs_val,   ys_val,   batch_size=batch_size)\n",
        "\n",
        "    return ds_train, ds_test, ds_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VnupWYiP5vW6"
      },
      "outputs": [],
      "source": [
        "# 假设你已经有原始的：\n",
        "#   xs_qa   (T_qa,   N_qa,   D),   ys_qa   (T_qa,   N_qa,   1)\n",
        "#   xs_sida (T_sida, N_sida, D),   ys_sida (T_sida, N_sida, 1)\n",
        "#   xs_sch  (T_sch,  N_sch,  D),   ys_sch  (T_sch,  N_sch,  1)\n",
        "#   xs_ma   (T_ma,   N_ma,   D),   ys_ma   (T_ma,   N_ma,   1)\n",
        "\n",
        "def make_segmented_array(xs, ys, seg_len=130, pad_x=0., pad_y=-1):\n",
        "    all_xs, all_ys = [], []\n",
        "    T, N, D = xs.shape\n",
        "    for sess in range(N):\n",
        "        x_seq = xs[:, sess, :]    # (T, D)\n",
        "        y_seq = ys[:, sess, :]    # (T, 1)\n",
        "        x_segs, y_segs = segment_and_pad(x_seq, y_seq, seg_len, pad_x, pad_y)\n",
        "        all_xs.extend(x_segs)     # list of (130, D)\n",
        "        all_ys.extend(y_segs)     # list of (130, 1)\n",
        "    # 把 list 再拼成一个三维 array (130, n_segments, D)\n",
        "    xs_seg = np.stack(all_xs, axis=1)\n",
        "    ys_seg = np.stack(all_ys, axis=1)\n",
        "    return xs_seg, ys_seg\n",
        "\n",
        "# 针对四个源分别做一次\n",
        "xs_qa_seg,   ys_qa_seg   = make_segmented_array(xs_qa,   ys_qa)\n",
        "xs_sida_seg,ys_sida_seg = make_segmented_array(xs_sida, ys_sida)\n",
        "xs_sch_seg, ys_sch_seg  = make_segmented_array(xs_sch,  ys_sch)\n",
        "xs_ma_seg,  ys_ma_seg   = make_segmented_array(xs_ma,   ys_ma)\n",
        "\n",
        "# 然后再把它们送入多源拼分函数\n",
        "dataset_train, dataset_test, dataset_validate = format_into_datasets_multi_source(\n",
        "    xs_list   = [\n",
        "        xs_qa_seg,   \n",
        "        xs_sida_seg,   \n",
        "        xs_sch_seg,   \n",
        "        xs_ma_seg\n",
        "        ],\n",
        "    ys_list   = [\n",
        "        ys_qa_seg,   \n",
        "        ys_sida_seg,   \n",
        "        ys_sch_seg,   \n",
        "        ys_ma_seg\n",
        "        ],\n",
        "    dataset_constructor = rnn_utils.DatasetRNN,\n",
        "    n_train_sessions   = 783,\n",
        "    n_test_sessions    = 98,\n",
        "    n_validate_sessions= 98,\n",
        "    batch_size=64,\n",
        "    random_seed=42,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "otkhuOQyx7hB"
      },
      "outputs": [],
      "source": [
        "def compute_log_likelihood(dataset, model_fun, params):\n",
        "    xs, actual_choices = next(dataset)\n",
        "    n_trials_per_session, n_sessions = actual_choices.shape[:2]\n",
        "    model_outputs, model_states = rnn_utils.eval_model(model_fun, params, xs)\n",
        "\n",
        "    # predicted log-probs for the first two actions\n",
        "    predicted_log_choice_probabilities = np.array(\n",
        "        jax.nn.log_softmax(model_outputs[:, :, :2], axis=-1)\n",
        "    )\n",
        "\n",
        "    n_actions = predicted_log_choice_probabilities.shape[2]\n",
        "    log_likelihoods = []\n",
        "\n",
        "    for sess_i in range(n_sessions):\n",
        "        log_likelihood = 0.0\n",
        "        n = 0\n",
        "        for trial_i in range(n_trials_per_session):\n",
        "            actual_choice = int(actual_choices[trial_i, sess_i])\n",
        "            # ignore invalid trials (<0 or ≥n_actions)\n",
        "            if 0 <= actual_choice < n_actions:\n",
        "                log_likelihood += predicted_log_choice_probabilities[\n",
        "                    trial_i, sess_i, actual_choice\n",
        "                ]\n",
        "                n += 1\n",
        "\n",
        "        if n > 0:\n",
        "            normalized_likelihood = np.exp(log_likelihood / n)\n",
        "            log_likelihoods.append(normalized_likelihood)\n",
        "\n",
        "    mean_likelihood = np.mean(log_likelihoods)\n",
        "    std_likelihood  = np.std(log_likelihoods)\n",
        "\n",
        "    print(f'Average Normalized Likelihood: {100 * mean_likelihood:.1f}%')\n",
        "    return mean_likelihood, std_likelihood\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCzihu0NJO6R",
        "outputId": "94f1db8b-0a3f-436a-d233-470f384a6c11"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 挂载 Google Drive\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# import pickle\n",
        "\n",
        "# # 挂载 Google Drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8sai1OhWYy3"
      },
      "source": [
        "# Fitting different models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmgO7HywNl5-"
      },
      "source": [
        "# Fit Vanilla RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZQyXrnBlvJaZ",
        "outputId": "527bb074-2a42-4519-bac2-c88a6f866cee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 200 of 200; Loss: 6.8754e+03; Test Loss: 7.5290e+03. (Time: 4.2s)updating best model ..\n",
            "Step 200 of 200000; Loss: 1.0303e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.0230e+03; Test Loss: 7.7661e+03. (Time: 2.9s)updating best model ..\n",
            "Step 400 of 200000; Loss: 1.0171e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4142e+04; Test Loss: 7.6360e+03. (Time: 3.0s)updating best model ..\n",
            "Step 600 of 200000; Loss: 1.0055e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6319e+04; Test Loss: 7.8186e+03. (Time: 3.1s)Step 800 of 200000; Loss: 1.0551e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9790e+04; Test Loss: 8.2054e+03. (Time: 3.0s)Step 1000 of 200000; Loss: 1.0842e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5133e+04; Test Loss: 7.6148e+03. (Time: 3.0s)updating best model ..\n",
            "Step 1200 of 200000; Loss: 9.9237e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2449e+04; Test Loss: 1.0396e+04. (Time: 3.0s)Step 1400 of 200000; Loss: 1.3505e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8491e+04; Test Loss: 1.0269e+04. (Time: 3.3s)Step 1600 of 200000; Loss: 1.5326e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6988e+04; Test Loss: 1.0165e+04. (Time: 3.0s)Step 1800 of 200000; Loss: 1.5111e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7580e+04; Test Loss: 1.0043e+04. (Time: 3.0s)Step 2000 of 200000; Loss: 1.4869e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.1381e+03; Test Loss: 9.8727e+03. (Time: 3.0s)Step 2200 of 200000; Loss: 1.4599e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3853e+04; Test Loss: 9.6251e+03. (Time: 3.2s)Step 2400 of 200000; Loss: 1.4283e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.3913e+03; Test Loss: 9.3284e+03. (Time: 3.2s)Step 2600 of 200000; Loss: 1.3928e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7786e+04; Test Loss: 9.0318e+03. (Time: 3.0s)Step 2800 of 200000; Loss: 1.3578e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.8286e+03; Test Loss: 8.8182e+03. (Time: 3.1s)Step 3000 of 200000; Loss: 1.3286e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3588e+04; Test Loss: 8.6232e+03. (Time: 3.1s)Step 3200 of 200000; Loss: 1.3053e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6154e+04; Test Loss: 8.4436e+03. (Time: 3.2s)Step 3400 of 200000; Loss: 1.2835e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2822e+04; Test Loss: 8.2801e+03. (Time: 3.3s)Step 3600 of 200000; Loss: 1.2625e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4220e+04; Test Loss: 8.2264e+03. (Time: 3.2s)Step 3800 of 200000; Loss: 1.2942e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.8102e+04; Test Loss: 8.0017e+03. (Time: 3.2s)Step 4000 of 200000; Loss: 1.2814e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4391e+04; Test Loss: 7.9077e+03. (Time: 3.1s)Step 4200 of 200000; Loss: 1.2850e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5151e+04; Test Loss: 7.8475e+03. (Time: 3.1s)Step 4400 of 200000; Loss: 1.3001e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2282e+04; Test Loss: 7.7754e+03. (Time: 3.6s)Step 4600 of 200000; Loss: 1.3438e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.3162e+03; Test Loss: 7.6718e+03. (Time: 3.2s)Step 4800 of 200000; Loss: 1.3280e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.2709e+03; Test Loss: 7.7322e+03. (Time: 4.1s)Step 5000 of 200000; Loss: 1.2897e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.1330e+03; Test Loss: 7.5772e+03. (Time: 3.9s)Step 5200 of 200000; Loss: 1.2873e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7739e+04; Test Loss: 7.7498e+03. (Time: 3.9s)Step 5400 of 200000; Loss: 1.2516e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3306e+03; Test Loss: 7.6236e+03. (Time: 4.1s)Step 5600 of 200000; Loss: 1.2614e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2004e+04; Test Loss: 7.5263e+03. (Time: 3.6s)Step 5800 of 200000; Loss: 1.2674e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3992e+04; Test Loss: 7.4718e+03. (Time: 3.9s)Step 6000 of 200000; Loss: 1.2851e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3245e+04; Test Loss: 7.2867e+03. (Time: 3.7s)Step 6200 of 200000; Loss: 1.2664e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2762e+04; Test Loss: 7.3054e+03. (Time: 3.6s)Step 6400 of 200000; Loss: 1.1940e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.8358e+04; Test Loss: 7.1490e+03. (Time: 4.1s)Step 6600 of 200000; Loss: 1.2346e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3155e+04; Test Loss: 7.1711e+03. (Time: 3.6s)Step 6800 of 200000; Loss: 1.2443e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4972e+04; Test Loss: 7.1365e+03. (Time: 4.2s)Step 7000 of 200000; Loss: 1.2410e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1351e+04; Test Loss: 6.9553e+03. (Time: 4.3s)Step 7200 of 200000; Loss: 1.2221e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.4335e+03; Test Loss: 6.8979e+03. (Time: 3.9s)Step 7400 of 200000; Loss: 1.2017e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.8917e+03; Test Loss: 7.0533e+03. (Time: 4.2s)Step 7600 of 200000; Loss: 1.2434e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.1896e+03; Test Loss: 7.0111e+03. (Time: 5.6s)Step 7800 of 200000; Loss: 1.2572e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6733e+04; Test Loss: 6.8905e+03. (Time: 4.0s)Step 8000 of 200000; Loss: 1.2332e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2504e+03; Test Loss: 6.9166e+03. (Time: 3.8s)Step 8200 of 200000; Loss: 1.1743e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1537e+04; Test Loss: 7.0059e+03. (Time: 3.4s)Step 8400 of 200000; Loss: 1.2146e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2872e+04; Test Loss: 6.8611e+03. (Time: 3.2s)Step 8600 of 200000; Loss: 1.2112e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1961e+04; Test Loss: 6.1521e+03. (Time: 3.3s)Step 8800 of 200000; Loss: 1.1276e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2221e+04; Test Loss: 6.1878e+03. (Time: 3.0s)Step 9000 of 200000; Loss: 1.0685e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6179e+04; Test Loss: 6.2532e+03. (Time: 2.9s)Step 9200 of 200000; Loss: 1.0598e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2330e+04; Test Loss: 6.2545e+03. (Time: 2.9s)Step 9400 of 200000; Loss: 1.0692e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3321e+04; Test Loss: 6.2190e+03. (Time: 2.9s)Step 9600 of 200000; Loss: 1.0663e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.6691e+03; Test Loss: 6.1980e+03. (Time: 3.0s)Step 9800 of 200000; Loss: 1.0507e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.7383e+03; Test Loss: 6.1851e+03. (Time: 2.9s)Step 10000 of 200000; Loss: 1.0484e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.9443e+03; Test Loss: 6.1649e+03. (Time: 2.9s)Step 10200 of 200000; Loss: 1.0439e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.0515e+03; Test Loss: 6.1425e+03. (Time: 3.2s)Step 10400 of 200000; Loss: 1.0391e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1785e+04; Test Loss: 6.1295e+03. (Time: 3.5s)Step 10600 of 200000; Loss: 1.0353e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3589e+03; Test Loss: 6.1716e+03. (Time: 3.0s)Step 10800 of 200000; Loss: 1.0332e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3877e+04; Test Loss: 6.8538e+03. (Time: 3.0s)Step 11000 of 200000; Loss: 1.0983e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2875e+04; Test Loss: 7.1889e+03. (Time: 3.0s)Step 11200 of 200000; Loss: 1.2981e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.8079e+04; Test Loss: 7.0871e+03. (Time: 3.0s)Step 11400 of 200000; Loss: 1.3571e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2534e+04; Test Loss: 6.6970e+03. (Time: 3.0s)Step 11600 of 200000; Loss: 1.3064e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.8899e+04; Test Loss: 6.6441e+03. (Time: 3.0s)Step 11800 of 200000; Loss: 1.2679e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3024e+04; Test Loss: 6.5224e+03. (Time: 3.6s)Step 12000 of 200000; Loss: 1.2848e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4913e+04; Test Loss: 6.7842e+03. (Time: 3.7s)Step 12200 of 200000; Loss: 1.2539e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0803e+04; Test Loss: 6.7176e+03. (Time: 3.0s)Step 12400 of 200000; Loss: 1.2400e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.0747e+03; Test Loss: 6.5754e+03. (Time: 4.0s)Step 12600 of 200000; Loss: 1.2392e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0215e+04; Test Loss: 6.7165e+03. (Time: 2.9s)Step 12800 of 200000; Loss: 1.2321e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.7337e+03; Test Loss: 6.0019e+03. (Time: 3.0s)Step 13000 of 200000; Loss: 1.1924e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 8.6224e+03; Test Loss: 6.3274e+03. (Time: 3.0s)Step 13200 of 200000; Loss: 9.9434e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2993e+03; Test Loss: 6.3633e+03. (Time: 3.0s)updating best model ..\n",
            "Step 13400 of 200000; Loss: 9.6746e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9989e+04; Test Loss: 6.3776e+03. (Time: 3.2s)Step 13600 of 200000; Loss: 9.6900e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2984e+04; Test Loss: 6.4126e+03. (Time: 3.0s)Step 13800 of 200000; Loss: 9.7788e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2825e+04; Test Loss: 6.6093e+03. (Time: 3.0s)Step 14000 of 200000; Loss: 1.0738e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1893e+04; Test Loss: 6.5940e+03. (Time: 2.9s)Step 14200 of 200000; Loss: 1.1392e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6808e+04; Test Loss: 6.4801e+03. (Time: 3.0s)Step 14400 of 200000; Loss: 1.1022e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1896e+04; Test Loss: 6.5590e+03. (Time: 2.9s)Step 14600 of 200000; Loss: 1.1491e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3438e+04; Test Loss: 6.3447e+03. (Time: 2.9s)Step 14800 of 200000; Loss: 1.1612e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0319e+04; Test Loss: 6.2983e+03. (Time: 3.1s)Step 15000 of 200000; Loss: 1.1621e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.4981e+03; Test Loss: 6.5609e+03. (Time: 2.7s)Step 15200 of 200000; Loss: 1.2327e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1459e+04; Test Loss: 6.4761e+03. (Time: 2.7s)Step 15400 of 200000; Loss: 1.2506e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.1535e+03; Test Loss: 6.5218e+03. (Time: 2.7s)Step 15600 of 200000; Loss: 1.2311e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3875e+04; Test Loss: 6.4493e+03. (Time: 3.5s)Step 15800 of 200000; Loss: 1.2194e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3452e+03; Test Loss: 6.3143e+03. (Time: 3.0s)Step 16000 of 200000; Loss: 1.1930e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3318e+04; Test Loss: 6.3991e+03. (Time: 3.0s)Step 16200 of 200000; Loss: 1.1964e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2594e+04; Test Loss: 6.5782e+03. (Time: 3.0s)Step 16400 of 200000; Loss: 1.2354e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7925e+04; Test Loss: 6.5365e+03. (Time: 3.2s)Step 16600 of 200000; Loss: 1.2735e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1941e+04; Test Loss: 6.1581e+03. (Time: 3.2s)Step 16800 of 200000; Loss: 1.2555e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.0151e+04; Test Loss: 6.1581e+03. (Time: 3.6s)Step 17000 of 200000; Loss: 1.2453e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2172e+04; Test Loss: 6.3485e+03. (Time: 3.7s)Step 17200 of 200000; Loss: 1.2579e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.4028e+04; Test Loss: 6.2492e+03. (Time: 4.9s)Step 17400 of 200000; Loss: 1.2685e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.9631e+03; Test Loss: 5.8723e+03. (Time: 4.6s)Step 17600 of 200000; Loss: 1.1285e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.9823e+03; Test Loss: 5.8738e+03. (Time: 5.2s)Step 17800 of 200000; Loss: 1.1257e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 7.4456e+03; Test Loss: 5.8708e+03. (Time: 5.8s)Step 18000 of 200000; Loss: 1.1219e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.4206e+03; Test Loss: 6.0425e+03. (Time: 4.2s)Step 18200 of 200000; Loss: 1.1171e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2050e+04; Test Loss: 6.1085e+03. (Time: 4.2s)Step 18400 of 200000; Loss: 1.1115e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3403e+03; Test Loss: 6.0855e+03. (Time: 3.7s)Step 18600 of 200000; Loss: 1.1050e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1866e+04; Test Loss: 6.0594e+03. (Time: 3.6s)Step 18800 of 200000; Loss: 1.1004e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2109e+04; Test Loss: 6.0451e+03. (Time: 3.7s)Step 19000 of 200000; Loss: 1.0962e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.4997e+04; Test Loss: 6.0395e+03. (Time: 3.4s)Step 19200 of 200000; Loss: 1.0966e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1830e+04; Test Loss: 6.0318e+03. (Time: 3.6s)Step 19400 of 200000; Loss: 1.0960e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5006e+04; Test Loss: 6.0213e+03. (Time: 3.5s)Step 19600 of 200000; Loss: 1.0928e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2038e+04; Test Loss: 6.2000e+03. (Time: 4.6s)Step 19800 of 200000; Loss: 1.0895e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2064e+04; Test Loss: 6.1145e+03. (Time: 4.8s)Step 20000 of 200000; Loss: 1.0409e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.8994e+03; Test Loss: 5.9423e+03. (Time: 7.4s)Step 20200 of 200000; Loss: 1.0426e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.7071e+03; Test Loss: 5.9706e+03. (Time: 3.9s)Step 20400 of 200000; Loss: 1.0606e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.6966e+03; Test Loss: 5.9701e+03. (Time: 4.0s)Step 20600 of 200000; Loss: 1.0507e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3903e+03; Test Loss: 6.0932e+03. (Time: 4.1s)Step 20800 of 200000; Loss: 1.0260e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1259e+04; Test Loss: 6.0027e+03. (Time: 3.6s)Step 21000 of 200000; Loss: 1.0442e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3424e+03; Test Loss: 6.1438e+03. (Time: 3.4s)Step 21200 of 200000; Loss: 1.0789e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1875e+04; Test Loss: 6.1440e+03. (Time: 3.3s)Step 21400 of 200000; Loss: 1.1074e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2519e+04; Test Loss: 6.1272e+03. (Time: 3.5s)Step 21600 of 200000; Loss: 1.1059e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3052e+04; Test Loss: 6.1104e+03. (Time: 3.9s)Step 21800 of 200000; Loss: 1.1033e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1771e+04; Test Loss: 6.0902e+03. (Time: 3.4s)Step 22000 of 200000; Loss: 1.1010e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5975e+04; Test Loss: 6.0782e+03. (Time: 3.9s)Step 22200 of 200000; Loss: 1.0989e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1594e+04; Test Loss: 6.1119e+03. (Time: 3.4s)Step 22400 of 200000; Loss: 1.0895e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.3166e+04; Test Loss: 6.0876e+03. (Time: 3.7s)Step 22600 of 200000; Loss: 1.0892e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0983e+04; Test Loss: 6.2457e+03. (Time: 3.4s)Step 22800 of 200000; Loss: 1.0768e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.7833e+03; Test Loss: 6.2435e+03. (Time: 3.4s)Step 23000 of 200000; Loss: 1.0822e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1699e+04; Test Loss: 6.4976e+03. (Time: 4.1s)Step 23200 of 200000; Loss: 1.1726e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.6220e+03; Test Loss: 6.0796e+03. (Time: 3.2s)Step 23400 of 200000; Loss: 1.2229e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.5710e+04; Test Loss: 6.0491e+03. (Time: 3.2s)Step 23600 of 200000; Loss: 1.2299e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1434e+03; Test Loss: 6.0338e+03. (Time: 3.3s)Step 23800 of 200000; Loss: 1.2253e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.3946e+04; Test Loss: 6.0217e+03. (Time: 3.2s)Step 24000 of 200000; Loss: 1.2194e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1837e+04; Test Loss: 6.0062e+03. (Time: 3.5s)Step 24200 of 200000; Loss: 1.2159e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7065e+04; Test Loss: 5.9844e+03. (Time: 3.2s)Step 24400 of 200000; Loss: 1.2124e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1235e+04; Test Loss: 5.9860e+03. (Time: 3.3s)Step 24600 of 200000; Loss: 1.2146e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.8009e+04; Test Loss: 5.9785e+03. (Time: 3.3s)Step 24800 of 200000; Loss: 1.2236e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1599e+04; Test Loss: 5.9660e+03. (Time: 3.2s)Step 25000 of 200000; Loss: 1.2198e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.2793e+04; Test Loss: 5.9568e+03. (Time: 4.7s)Step 25200 of 200000; Loss: 1.2185e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1846e+04; Test Loss: 5.9481e+03. (Time: 3.9s)Step 25400 of 200000; Loss: 1.2177e+04. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 6.7178e+03; Test Loss: 5.9397e+03. (Time: 3.5s)Step 25600 of 200000; Loss: 1.2154e+04. (Time: 0.0s)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/interpreters/mlir.py:2154\u001b[39m, in \u001b[36m_lower_jaxpr_to_fun_cached\u001b[39m\u001b[34m(ctx, fn_name, call_jaxpr, effects, name_stack, arg_names, result_names)\u001b[39m\n\u001b[32m   2153\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2154\u001b[39m   func_op = \u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcached_primitive_lowerings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   2155\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
            "\u001b[31mKeyError\u001b[39m: (None, { lambda ; a:i32[] b:i32[] c:f32[1,2] d:f32[1,2] e:f32[12,8] f:f32[64,8] g:f32[8,1]\n    h:f32[64,1] i:f32[1,2] j:f32[13,8] k:f32[64,8] l:f32[8,2] m:f32[64,2] n:f32[64,8]\n    o:f32[64,8] p:f32[64,2] q:f32[64,2] r:f32[64,2]. let\n    s:f32[64,2] = mul 1.0 q\n    t:f32[64,2] = add s 0.0\n    u:f32[64,1] = dynamic_slice[slice_sizes=(64, 1)] r a b\n    v:f32[64] = squeeze[dimensions=(1,)] u\n    w:f32[64,1] = broadcast_in_dim[\n      broadcast_dimensions=(0,)\n      shape=(64, 1)\n      sharding=None\n    ] v\n    x:f32[64,1] = broadcast_in_dim[\n      broadcast_dimensions=(0,)\n      shape=(64, 1)\n      sharding=None\n    ] v\n    y:f32[64,1] = slice[limit_indices=(64, 1) start_indices=(0, 0) strides=None] r\n    z:f32[64] = squeeze[dimensions=(1,)] y\n    ba:f32[64,2] = pjit[\n      name=_one_hot\n      jaxpr={ lambda ; bb:f32[64] bc:f32[1,2]. let\n          bd:f32[64,1] = broadcast_in_dim[\n            broadcast_dimensions=(0,)\n            shape=(64, 1)\n            sharding=None\n          ] bb\n          be:bool[64,2] = eq bd bc\n          bf:f32[64,2] = convert_element_type[new_dtype=float32 weak_type=False] be\n        in (bf,) }\n    ] z c\n    bg:f32[64,2] = mul q ba\n    bh:f32[64] = reduce_sum[axes=(1,)] bg\n    bi:f32[64,1] = broadcast_in_dim[\n      broadcast_dimensions=(0,)\n      shape=(64, 1)\n      sharding=None\n    ] bh\n    bj:f32[64,2] = sub d ba\n    bk:f32[64,2] = mul q bj\n    bl:f32[64] = reduce_sum[axes=(1,)] bk\n    bm:f32[64,1] = broadcast_in_dim[\n      broadcast_dimensions=(0,)\n      shape=(64, 1)\n      sharding=None\n    ] bl\n    bn:f32[64,1] = add bi bm\n    bo:f32[64,1] = add bn 9.99999993922529e-09\n    bp:f32[64,1] = div bi bo\n    bq:f32[64,1] = sub x bp\n    br:f32[64,2] = mul q bq\n    bs:f32[64,1] = sub 1.0 bq\n    bt:f32[64,8] = mul o bs\n    bu:f32[64,12] = concatenate[dimension=1] bi w br bt\n    bv:f32[64,8] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] bu e\n    bw:f32[64,8] = add bv f\n    bx:f32[64,8] = tanh bw\n    by:f32[64,1] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] bx g\n    bz:f32[64,1] = add by h\n    ca:f32[64,2] = mul ba bz\n    cb:f32[64,2] = add t ca\n    cc:f32[64,2] = mul 1.0 cb\n    cd:f32[64,1] = broadcast_in_dim[\n      broadcast_dimensions=(0,)\n      shape=(64, 1)\n      sharding=None\n    ] v\n    ce:f32[64,1] = broadcast_in_dim[\n      broadcast_dimensions=(0,)\n      shape=(64, 1)\n      sharding=None\n    ] v\n    cf:f32[64,2] = mul q ba\n    cg:f32[64] = reduce_sum[axes=(1,)] cf\n    ch:f32[64,1] = broadcast_in_dim[\n      broadcast_dimensions=(0,)\n      shape=(64, 1)\n      sharding=None\n    ] cg\n    ci:f32[64,2] = sub i ba\n    cj:f32[64,2] = mul q ci\n    ck:f32[64] = reduce_sum[axes=(1,)] cj\n    cl:f32[64,1] = broadcast_in_dim[\n      broadcast_dimensions=(0,)\n      shape=(64, 1)\n      sharding=None\n    ] ck\n    cm:f32[64,1] = add ch cl\n    cn:f32[64,1] = add cm 9.99999993922529e-09\n    co:f32[64,1] = div ch cn\n    cp:f32[64,1] = sub ce co\n    cq:f32[64,2] = mul p cp\n    cr:f32[64,1] = sub 1.0 cp\n    cs:f32[64,8] = mul n cr\n    ct:f32[64,13] = concatenate[dimension=1] ba cd cq cs\n    cu:f32[64,8] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] ct j\n    cv:f32[64,8] = add cu k\n    cw:f32[64,8] = tanh cv\n    cx:f32[64,2] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] cw l\n    cy:f32[64,2] = add cx m\n    cz:f32[64,2] = mul 1.0 cy\n    da:f32[64,2] = add cc cz\n    db:f32[64,1] = integer_pow[y=-2] bo\n    dc:f32[64,8] = sub 1.0 bx\n    dd:f32[64,1] = integer_pow[y=-2] cn\n    de:f32[64,8] = sub 1.0 cw\n  in (cw, bx, cy, cb, da, ba, bq, q, bo, bj, bi, db, bs, o, bu, bx, dc, cp, p, cn,\n    ci, ch, dd, cr, n, ct, cw, de) }, ())",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     25\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m#从这里就是运行训练代码了\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m hybrnn_params, _ = \u001b[43mrnn_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_fun\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_hybrnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_validate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_fun\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcategorical\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptax\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_decayed_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# L2 正则化\u001b[39;49;00m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptax\u001b[49m\u001b[43m.\u001b[49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adam 优化器\u001b[39;49;00m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_steps_per_call\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_steps_max\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stop_step\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\n\u001b[32m     41\u001b[39m \u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/CogModelingRNNsTutorial/rnn_utils.py:558\u001b[39m, in \u001b[36mfit_model\u001b[39m\u001b[34m(model_fun, dataset_train, dataset_test, optimizer, loss_fun, random_key, n_steps_per_call, n_steps_max, return_all_losses, early_stop_step, if_early_stop, if_mean)\u001b[39m\n\u001b[32m    555\u001b[39m   early_stop_triggered = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m continue_training:\n\u001b[32m--> \u001b[39m\u001b[32m558\u001b[39m   params, opt_state, losses = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel_fun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m      \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m      \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m      \u001b[49m\u001b[43mloss_fun\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_fun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdo_plot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m      \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_steps_per_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m      \u001b[49m\u001b[43mif_mean\u001b[49m\u001b[43m=\u001b[49m\u001b[43mif_mean\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m   n_calls_to_train_model += \u001b[32m1\u001b[39m\n\u001b[32m    571\u001b[39m   t_start = time.time()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/CogModelingRNNsTutorial/rnn_utils.py:274\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model_fun, dataset_train, dataset_test, optimizer, random_key, opt_state, params, n_steps, penalty_scale, loss_fun, do_plot, truncate_seq_length, if_mean)\u001b[39m\n\u001b[32m    271\u001b[39m   xs = xs[:truncate_seq_length]\n\u001b[32m    272\u001b[39m   ys = ys[:truncate_seq_length]\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m loss, params, opt_state = \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m training_loss.append(\u001b[38;5;28mfloat\u001b[39m(loss))\n\u001b[32m    277\u001b[39m \u001b[38;5;66;03m# Compute testing loss\u001b[39;00m\n",
            "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/pjit.py:337\u001b[39m, in \u001b[36m_cpp_pjit.<locals>.cache_miss\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.no_tracing.value:\n\u001b[32m    333\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mre-tracing function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjit_info.fun_sourceinfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    334\u001b[39m                      \u001b[33m\"\u001b[39m\u001b[33m`jit`, but \u001b[39m\u001b[33m'\u001b[39m\u001b[33mno_tracing\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is set\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    336\u001b[39m (outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked, executable,\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m  pgle_profiler) = \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m maybe_fastpath_data = _get_fastpath_data(\n\u001b[32m    340\u001b[39m     executable, out_tree, args_flat, out_flat, attrs_tracked, jaxpr.effects,\n\u001b[32m    341\u001b[39m     jaxpr.consts, jit_info.abstracted_axes,\n\u001b[32m    342\u001b[39m     pgle_profiler)\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outs, maybe_fastpath_data, _need_to_rebuild_with_fdo(pgle_profiler)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/pjit.py:195\u001b[39m, in \u001b[36m_python_pjit_helper\u001b[39m\u001b[34m(fun, jit_info, *args, **kwargs)\u001b[39m\n\u001b[32m    193\u001b[39m   args_flat = \u001b[38;5;28mmap\u001b[39m(core.full_lower, args_flat)\n\u001b[32m    194\u001b[39m   core.check_eval_args(args_flat)\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m   out_flat, compiled, profiler = \u001b[43m_pjit_call_impl_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    197\u001b[39m   out_flat = pjit_p.bind(*args_flat, **p.params)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/pjit.py:1663\u001b[39m, in \u001b[36m_pjit_call_impl_python\u001b[39m\u001b[34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, compiler_options_kvs, *args)\u001b[39m\n\u001b[32m   1660\u001b[39m compiler_options_kvs = compiler_options_kvs + \u001b[38;5;28mtuple\u001b[39m(pgle_compile_options.items())\n\u001b[32m   1661\u001b[39m \u001b[38;5;66;03m# Passing mutable PGLE profile here since it should be extracted by JAXPR to\u001b[39;00m\n\u001b[32m   1662\u001b[39m \u001b[38;5;66;03m# initialize the fdo_profile compile option.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1663\u001b[39m compiled = \u001b[43m_resolve_and_lower\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1664\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1665\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1666\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_env\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresource_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1667\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1668\u001b[39m \u001b[43m    \u001b[49m\u001b[43minline\u001b[49m\u001b[43m=\u001b[49m\u001b[43minline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowering_platforms\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1669\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlowering_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlir\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLoweringParameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1672\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.compile()\n\u001b[32m   1674\u001b[39m \u001b[38;5;66;03m# This check is expensive so only do it if enable_checks is on.\u001b[39;00m\n\u001b[32m   1675\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compiled._auto_spmd_lowering \u001b[38;5;129;01mand\u001b[39;00m config.enable_checks.value:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/pjit.py:1630\u001b[39m, in \u001b[36m_resolve_and_lower\u001b[39m\u001b[34m(args, jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, lowering_platforms, lowering_parameters, pgle_profiler, compiler_options_kvs)\u001b[39m\n\u001b[32m   1627\u001b[39m in_shardings = _resolve_in_shardings(args, in_shardings)\n\u001b[32m   1628\u001b[39m in_layouts = _resolve_in_layouts(args, in_layouts, in_shardings,\n\u001b[32m   1629\u001b[39m                                  jaxpr.in_avals)\n\u001b[32m-> \u001b[39m\u001b[32m1630\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pjit_lower\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1631\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1632\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1633\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlowering_platforms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlowering_platforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1634\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlowering_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlowering_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1635\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/pjit.py:1795\u001b[39m, in \u001b[36m_pjit_lower\u001b[39m\u001b[34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, name, keep_unused, inline, compiler_options_kvs, lowering_platforms, lowering_parameters, pgle_profiler)\u001b[39m\n\u001b[32m   1792\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1793\u001b[39m   mesh, api_name = ((resource_env.physical_mesh, \u001b[33m'\u001b[39m\u001b[33mpjit\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   1794\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m resource_env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m'\u001b[39m\u001b[33mjit\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m-> \u001b[39m\u001b[32m1795\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpxla\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower_sharding_computation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1797\u001b[39m \u001b[43m    \u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1799\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1800\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlowering_platforms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlowering_platforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1801\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlowering_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlowering_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/profiler.py:333\u001b[39m, in \u001b[36mannotate_function.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    332\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, **decorator_kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    334\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m wrapper\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:2323\u001b[39m, in \u001b[36mlower_sharding_computation\u001b[39m\u001b[34m(closed_jaxpr, api_name, fun_name, in_shardings, out_shardings, in_layouts, out_layouts, donated_invars, keep_unused, context_mesh, compiler_options_kvs, lowering_platforms, lowering_parameters, pgle_profiler)\u001b[39m\n\u001b[32m   2317\u001b[39m semantic_in_shardings = SemanticallyEqualShardings(\n\u001b[32m   2318\u001b[39m     in_shardings, global_in_avals)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   2319\u001b[39m semantic_out_shardings = SemanticallyEqualShardings(\n\u001b[32m   2320\u001b[39m     out_shardings, global_out_avals)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   2322\u001b[39m (module, keepalive, host_callbacks, unordered_effects, ordered_effects,\n\u001b[32m-> \u001b[39m\u001b[32m2323\u001b[39m  nreps, tuple_args, shape_poly_state) = \u001b[43m_cached_lowering_to_hlo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2324\u001b[39m \u001b[43m     \u001b[49m\u001b[43mclosed_jaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msemantic_in_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2325\u001b[39m \u001b[43m     \u001b[49m\u001b[43msemantic_out_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_devices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m     \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mda_object\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprim_requires_devices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m     \u001b[49m\u001b[43mname_stack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_default_mem_kind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minout_aliases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m     \u001b[49m\u001b[43mpropagated_out_mem_kinds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplatforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m     \u001b[49m\u001b[43mlowering_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlowering_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m     \u001b[49m\u001b[43mabstract_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mabstract_mesh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[38;5;66;03m# backend and device_assignment is passed through to MeshExecutable because\u001b[39;00m\n\u001b[32m   2333\u001b[39m \u001b[38;5;66;03m# if keep_unused=False and all in_shardings are pruned, then there is no way\u001b[39;00m\n\u001b[32m   2334\u001b[39m \u001b[38;5;66;03m# to get the device_assignment and backend. So pass it to MeshExecutable\u001b[39;00m\n\u001b[32m   2335\u001b[39m \u001b[38;5;66;03m# because we calculate the device_assignment and backend before in_shardings,\u001b[39;00m\n\u001b[32m   2336\u001b[39m \u001b[38;5;66;03m# etc are pruned.\u001b[39;00m\n\u001b[32m   2337\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m MeshComputation(\n\u001b[32m   2338\u001b[39m     \u001b[38;5;28mstr\u001b[39m(name_stack),\n\u001b[32m   2339\u001b[39m     module,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2369\u001b[39m     intermediate_shardings=unique_intermediate_shardings,\n\u001b[32m   2370\u001b[39m     context_mesh=context_mesh)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/interpreters/pxla.py:1953\u001b[39m, in \u001b[36m_cached_lowering_to_hlo\u001b[39m\u001b[34m(closed_jaxpr, api_name, fun_name, backend, semantic_in_shardings, semantic_out_shardings, in_layouts, out_layouts, num_devices, device_assignment, donated_invars, name_stack, all_default_mem_kind, inout_aliases, propagated_out_mem_kinds, platforms, lowering_parameters, abstract_mesh)\u001b[39m\n\u001b[32m   1949\u001b[39m ordered_effects = \u001b[38;5;28mlist\u001b[39m(effects.ordered_effects.filter_in(closed_jaxpr.effects))\n\u001b[32m   1950\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m dispatch.log_elapsed_time(\n\u001b[32m   1951\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mFinished jaxpr to MLIR module conversion \u001b[39m\u001b[38;5;132;01m{fun_name}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{elapsed_time:.9f}\u001b[39;00m\u001b[33m sec\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1952\u001b[39m       fun_name=\u001b[38;5;28mstr\u001b[39m(name_stack), event=dispatch.JAXPR_TO_MLIR_MODULE_EVENT):\n\u001b[32m-> \u001b[39m\u001b[32m1953\u001b[39m   lowering_result = \u001b[43mmlir\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower_jaxpr_to_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1954\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1955\u001b[39m \u001b[43m      \u001b[49m\u001b[43mclosed_jaxpr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1956\u001b[39m \u001b[43m      \u001b[49m\u001b[43mordered_effects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mordered_effects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1957\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbackend_or_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1958\u001b[39m \u001b[43m      \u001b[49m\u001b[43mplatforms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplatforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1959\u001b[39m \u001b[43m      \u001b[49m\u001b[43maxis_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis_ctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1960\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname_stack\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname_stack\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1961\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdonated_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1962\u001b[39m \u001b[43m      \u001b[49m\u001b[43mreplicated_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreplicated_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1963\u001b[39m \u001b[43m      \u001b[49m\u001b[43marg_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_mlir_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1964\u001b[39m \u001b[43m      \u001b[49m\u001b[43mresult_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_mlir_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1965\u001b[39m \u001b[43m      \u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1966\u001b[39m \u001b[43m      \u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1967\u001b[39m \u001b[43m      \u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdebug_info\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdebug_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1968\u001b[39m \u001b[43m      \u001b[49m\u001b[43mresult_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdebug_info\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdebug_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1969\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_replicas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnreps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1970\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_partitions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_partitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1971\u001b[39m \u001b[43m      \u001b[49m\u001b[43mall_default_mem_kind\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_default_mem_kind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1972\u001b[39m \u001b[43m      \u001b[49m\u001b[43minput_output_aliases\u001b[49m\u001b[43m=\u001b[49m\u001b[43minout_aliases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1973\u001b[39m \u001b[43m      \u001b[49m\u001b[43mpropagated_out_mem_kinds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpropagated_out_mem_kinds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1974\u001b[39m \u001b[43m      \u001b[49m\u001b[43mlowering_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlowering_parameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1975\u001b[39m tuple_args = dispatch.should_tuple_args(\u001b[38;5;28mlen\u001b[39m(global_in_avals), backend.platform)\n\u001b[32m   1976\u001b[39m unordered_effects = \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m   1977\u001b[39m     effects.ordered_effects.filter_not_in(closed_jaxpr.effects))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/interpreters/mlir.py:1171\u001b[39m, in \u001b[36mlower_jaxpr_to_module\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1169\u001b[39m attrs[\u001b[33m\"\u001b[39m\u001b[33mmhlo.num_replicas\u001b[39m\u001b[33m\"\u001b[39m] = i32_attr(num_replicas)\n\u001b[32m   1170\u001b[39m attrs[\u001b[33m\"\u001b[39m\u001b[33mmhlo.num_partitions\u001b[39m\u001b[33m\"\u001b[39m] = i32_attr(num_partitions)\n\u001b[32m-> \u001b[39m\u001b[32m1171\u001b[39m \u001b[43mlower_jaxpr_to_fun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mordered_effects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname_stack\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname_stack\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpublic\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreplicated_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreplicated_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m    \u001b[49m\u001b[43marg_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43marg_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresult_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresult_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_output_aliases\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_output_aliases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxla_donated_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxla_donated_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresult_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresult_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m    \u001b[49m\u001b[43marg_memory_kinds\u001b[49m\u001b[43m=\u001b[49m\u001b[43marg_memory_kinds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresult_memory_kinds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresult_memory_kinds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m    \u001b[49m\u001b[43marg_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresult_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpropagated_out_mem_kinds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpropagated_out_mem_kinds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.use_shardy_partitioner.value:\n\u001b[32m   1188\u001b[39m   pipeline = passmanager.PassManager.parse(\n\u001b[32m   1189\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mbuiltin.module(sdy-lift-inlined-meshes)\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/interpreters/mlir.py:1646\u001b[39m, in \u001b[36mlower_jaxpr_to_fun\u001b[39m\u001b[34m(ctx, name, jaxpr, effects, name_stack, public, replicated_args, arg_shardings, result_shardings, use_sharding_annotations, input_output_aliases, xla_donated_args, api_name, arg_names, result_names, arg_memory_kinds, result_memory_kinds, arg_layouts, result_layouts, propagated_out_mem_kinds)\u001b[39m\n\u001b[32m   1644\u001b[39m   callee_name_stack = name_stack\n\u001b[32m   1645\u001b[39m consts = [ir_constant(xla.canonicalize_dtype(x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m jaxpr.consts]\n\u001b[32m-> \u001b[39m\u001b[32m1646\u001b[39m out_vals, tokens_out = \u001b[43mjaxpr_subcomp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallee_name_stack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconsts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_var_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdim_var_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1649\u001b[39m outs: \u001b[38;5;28mlist\u001b[39m[IrValues] = []\n\u001b[32m   1650\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m eff \u001b[38;5;129;01min\u001b[39;00m effects:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/interpreters/mlir.py:1905\u001b[39m, in \u001b[36mjaxpr_subcomp\u001b[39m\u001b[34m(ctx, jaxpr, name_stack, tokens, consts, dim_var_values, *args)\u001b[39m\n\u001b[32m   1902\u001b[39m   rule_ctx = rule_ctx.replace(axis_size_env=axis_size_env)\n\u001b[32m   1904\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_ir_values(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m in_nodes), (eqn, in_nodes)\n\u001b[32m-> \u001b[39m\u001b[32m1905\u001b[39m ans = \u001b[43mlower_per_platform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrule_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43meqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprimitive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mplatform_rules\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_rule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m                         \u001b[49m\u001b[43meqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43meffects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m                         \u001b[49m\u001b[43m*\u001b[49m\u001b[43min_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43meqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m effects:\n\u001b[32m   1911\u001b[39m   \u001b[38;5;66;03m# If there were ordered effects in the primitive, there should be output\u001b[39;00m\n\u001b[32m   1912\u001b[39m   \u001b[38;5;66;03m# tokens we need for subsequent ordered effects.\u001b[39;00m\n\u001b[32m   1913\u001b[39m   tokens_out = rule_ctx.tokens_out\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/interpreters/mlir.py:2023\u001b[39m, in \u001b[36mlower_per_platform\u001b[39m\u001b[34m(ctx, description, platform_rules, default_rule, effects, *rule_args, **rule_kwargs)\u001b[39m\n\u001b[32m   2021\u001b[39m \u001b[38;5;66;03m# If there is a single rule left just apply the rule, without conditionals.\u001b[39;00m\n\u001b[32m   2022\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kept_rules) == \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2023\u001b[39m   output = \u001b[43mkept_rules\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrule_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2024\u001b[39m   \u001b[38;5;28mmap\u001b[39m(\n\u001b[32m   2025\u001b[39m       \u001b[38;5;28;01mlambda\u001b[39;00m o: wrap_compute_type_in_place(ctx, o.owner),\n\u001b[32m   2026\u001b[39m       \u001b[38;5;28mfilter\u001b[39m(_is_not_block_argument, flatten_ir_values(output)),\n\u001b[32m   2027\u001b[39m   )\n\u001b[32m   2028\u001b[39m   \u001b[38;5;28mmap\u001b[39m(\n\u001b[32m   2029\u001b[39m       \u001b[38;5;28;01mlambda\u001b[39;00m o: wrap_xla_metadata_in_place(ctx, o.owner),\n\u001b[32m   2030\u001b[39m       flatten_ir_values(output),\n\u001b[32m   2031\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/pjit.py:1950\u001b[39m, in \u001b[36m_pjit_lowering\u001b[39m\u001b[34m(ctx, name, jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, resource_env, donated_invars, keep_unused, inline, compiler_options_kvs, *args)\u001b[39m\n\u001b[32m   1947\u001b[39m output_types = [mlir.token_type()] * \u001b[38;5;28mlen\u001b[39m(effects) + output_types\n\u001b[32m   1948\u001b[39m flat_output_types = mlir.flatten_ir_types(output_types)\n\u001b[32m-> \u001b[39m\u001b[32m1950\u001b[39m func = \u001b[43m_pjit_cached_lower_jaxpr_to_fun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43meffects\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1953\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mjit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresource_env\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpjit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1955\u001b[39m tokens_in = [ctx.tokens_in.get(eff) \u001b[38;5;28;01mfor\u001b[39;00m eff \u001b[38;5;129;01min\u001b[39;00m effects]\n\u001b[32m   1956\u001b[39m args = (*ctx.dim_var_values, *tokens_in, *args)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/pjit.py:1933\u001b[39m, in \u001b[36m_pjit_cached_lower_jaxpr_to_fun\u001b[39m\u001b[34m(ctx, name, jaxpr, effects, in_shardings, out_shardings, in_layouts, out_layouts, api_name)\u001b[39m\n\u001b[32m   1929\u001b[39m   result_shardings = [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, UnspecifiedValue) \u001b[38;5;28;01melse\u001b[39;00m o \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m out_shardings]\n\u001b[32m   1930\u001b[39m   \u001b[38;5;66;03m# TODO(b/228598865): inlined calls cannot have shardings set directly on the\u001b[39;00m\n\u001b[32m   1931\u001b[39m   \u001b[38;5;66;03m# inputs or outputs because they are lost during MLIR->HLO conversion.\u001b[39;00m\n\u001b[32m   1932\u001b[39m   \u001b[38;5;66;03m# using_sharding_annotation=False means we add an identity operation instead.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1933\u001b[39m   func = \u001b[43mmlir\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower_jaxpr_to_fun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1934\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmod_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname_stack\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1935\u001b[39m \u001b[43m      \u001b[49m\u001b[43marg_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43marg_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_shardings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresult_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1936\u001b[39m \u001b[43m      \u001b[49m\u001b[43muse_sharding_annotations\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1937\u001b[39m \u001b[43m      \u001b[49m\u001b[43marg_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_layouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_layouts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1938\u001b[39m   mod_ctx.cached_primitive_lowerings[key] = func\n\u001b[32m   1939\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/interpreters/mlir.py:1646\u001b[39m, in \u001b[36mlower_jaxpr_to_fun\u001b[39m\u001b[34m(ctx, name, jaxpr, effects, name_stack, public, replicated_args, arg_shardings, result_shardings, use_sharding_annotations, input_output_aliases, xla_donated_args, api_name, arg_names, result_names, arg_memory_kinds, result_memory_kinds, arg_layouts, result_layouts, propagated_out_mem_kinds)\u001b[39m\n\u001b[32m   1644\u001b[39m   callee_name_stack = name_stack\n\u001b[32m   1645\u001b[39m consts = [ir_constant(xla.canonicalize_dtype(x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m jaxpr.consts]\n\u001b[32m-> \u001b[39m\u001b[32m1646\u001b[39m out_vals, tokens_out = \u001b[43mjaxpr_subcomp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallee_name_stack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconsts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_var_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdim_var_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1649\u001b[39m outs: \u001b[38;5;28mlist\u001b[39m[IrValues] = []\n\u001b[32m   1650\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m eff \u001b[38;5;129;01min\u001b[39;00m effects:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/interpreters/mlir.py:1905\u001b[39m, in \u001b[36mjaxpr_subcomp\u001b[39m\u001b[34m(ctx, jaxpr, name_stack, tokens, consts, dim_var_values, *args)\u001b[39m\n\u001b[32m   1902\u001b[39m   rule_ctx = rule_ctx.replace(axis_size_env=axis_size_env)\n\u001b[32m   1904\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_ir_values(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m in_nodes), (eqn, in_nodes)\n\u001b[32m-> \u001b[39m\u001b[32m1905\u001b[39m ans = \u001b[43mlower_per_platform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrule_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43meqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprimitive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mplatform_rules\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_rule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m                         \u001b[49m\u001b[43meqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43meffects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m                         \u001b[49m\u001b[43m*\u001b[49m\u001b[43min_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43meqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m effects:\n\u001b[32m   1911\u001b[39m   \u001b[38;5;66;03m# If there were ordered effects in the primitive, there should be output\u001b[39;00m\n\u001b[32m   1912\u001b[39m   \u001b[38;5;66;03m# tokens we need for subsequent ordered effects.\u001b[39;00m\n\u001b[32m   1913\u001b[39m   tokens_out = rule_ctx.tokens_out\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/interpreters/mlir.py:2023\u001b[39m, in \u001b[36mlower_per_platform\u001b[39m\u001b[34m(ctx, description, platform_rules, default_rule, effects, *rule_args, **rule_kwargs)\u001b[39m\n\u001b[32m   2021\u001b[39m \u001b[38;5;66;03m# If there is a single rule left just apply the rule, without conditionals.\u001b[39;00m\n\u001b[32m   2022\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kept_rules) == \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2023\u001b[39m   output = \u001b[43mkept_rules\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrule_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2024\u001b[39m   \u001b[38;5;28mmap\u001b[39m(\n\u001b[32m   2025\u001b[39m       \u001b[38;5;28;01mlambda\u001b[39;00m o: wrap_compute_type_in_place(ctx, o.owner),\n\u001b[32m   2026\u001b[39m       \u001b[38;5;28mfilter\u001b[39m(_is_not_block_argument, flatten_ir_values(output)),\n\u001b[32m   2027\u001b[39m   )\n\u001b[32m   2028\u001b[39m   \u001b[38;5;28mmap\u001b[39m(\n\u001b[32m   2029\u001b[39m       \u001b[38;5;28;01mlambda\u001b[39;00m o: wrap_xla_metadata_in_place(ctx, o.owner),\n\u001b[32m   2030\u001b[39m       flatten_ir_values(output),\n\u001b[32m   2031\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/interpreters/mlir.py:2138\u001b[39m, in \u001b[36mlower_fun.<locals>.f_lowered\u001b[39m\u001b[34m(ctx, *args, **params)\u001b[39m\n\u001b[32m   2136\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2137\u001b[39m   sub_context = ctx.module_context\n\u001b[32m-> \u001b[39m\u001b[32m2138\u001b[39m out, tokens = \u001b[43mjaxpr_subcomp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2139\u001b[39m \u001b[43m    \u001b[49m\u001b[43msub_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname_stack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokens_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2140\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_ir_consts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconsts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdim_var_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdim_var_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2142\u001b[39m ctx.set_tokens_out(tokens)\n\u001b[32m   2143\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/interpreters/mlir.py:1905\u001b[39m, in \u001b[36mjaxpr_subcomp\u001b[39m\u001b[34m(ctx, jaxpr, name_stack, tokens, consts, dim_var_values, *args)\u001b[39m\n\u001b[32m   1902\u001b[39m   rule_ctx = rule_ctx.replace(axis_size_env=axis_size_env)\n\u001b[32m   1904\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_ir_values(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m in_nodes), (eqn, in_nodes)\n\u001b[32m-> \u001b[39m\u001b[32m1905\u001b[39m ans = \u001b[43mlower_per_platform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrule_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43meqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprimitive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mplatform_rules\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_rule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m                         \u001b[49m\u001b[43meqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43meffects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m                         \u001b[49m\u001b[43m*\u001b[49m\u001b[43min_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43meqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m effects:\n\u001b[32m   1911\u001b[39m   \u001b[38;5;66;03m# If there were ordered effects in the primitive, there should be output\u001b[39;00m\n\u001b[32m   1912\u001b[39m   \u001b[38;5;66;03m# tokens we need for subsequent ordered effects.\u001b[39;00m\n\u001b[32m   1913\u001b[39m   tokens_out = rule_ctx.tokens_out\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/interpreters/mlir.py:2023\u001b[39m, in \u001b[36mlower_per_platform\u001b[39m\u001b[34m(ctx, description, platform_rules, default_rule, effects, *rule_args, **rule_kwargs)\u001b[39m\n\u001b[32m   2021\u001b[39m \u001b[38;5;66;03m# If there is a single rule left just apply the rule, without conditionals.\u001b[39;00m\n\u001b[32m   2022\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kept_rules) == \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2023\u001b[39m   output = \u001b[43mkept_rules\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrule_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2024\u001b[39m   \u001b[38;5;28mmap\u001b[39m(\n\u001b[32m   2025\u001b[39m       \u001b[38;5;28;01mlambda\u001b[39;00m o: wrap_compute_type_in_place(ctx, o.owner),\n\u001b[32m   2026\u001b[39m       \u001b[38;5;28mfilter\u001b[39m(_is_not_block_argument, flatten_ir_values(output)),\n\u001b[32m   2027\u001b[39m   )\n\u001b[32m   2028\u001b[39m   \u001b[38;5;28mmap\u001b[39m(\n\u001b[32m   2029\u001b[39m       \u001b[38;5;28;01mlambda\u001b[39;00m o: wrap_xla_metadata_in_place(ctx, o.owner),\n\u001b[32m   2030\u001b[39m       flatten_ir_values(output),\n\u001b[32m   2031\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/lax/control_flow/loops.py:1787\u001b[39m, in \u001b[36m_while_lowering\u001b[39m\u001b[34m(ctx, cond_jaxpr, body_jaxpr, cond_nconsts, body_nconsts, *args)\u001b[39m\n\u001b[32m   1784\u001b[39m body_name_stack = name_stack.extend(\u001b[33m'\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   1785\u001b[39m body_consts = [mlir.ir_constant(xla.canonicalize_dtype(x))\n\u001b[32m   1786\u001b[39m                \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m body_jaxpr.consts]\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m new_z, tokens_out = \u001b[43mmlir\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjaxpr_subcomp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodule_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody_jaxpr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody_name_stack\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokens_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody_consts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_var_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdim_var_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1790\u001b[39m out_tokens = [tokens_out.get(eff) \u001b[38;5;28;01mfor\u001b[39;00m eff \u001b[38;5;129;01min\u001b[39;00m body_effects]\n\u001b[32m   1791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batched:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/interpreters/mlir.py:1905\u001b[39m, in \u001b[36mjaxpr_subcomp\u001b[39m\u001b[34m(ctx, jaxpr, name_stack, tokens, consts, dim_var_values, *args)\u001b[39m\n\u001b[32m   1902\u001b[39m   rule_ctx = rule_ctx.replace(axis_size_env=axis_size_env)\n\u001b[32m   1904\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_ir_values(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m in_nodes), (eqn, in_nodes)\n\u001b[32m-> \u001b[39m\u001b[32m1905\u001b[39m ans = \u001b[43mlower_per_platform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrule_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43meqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprimitive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mplatform_rules\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_rule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m                         \u001b[49m\u001b[43meqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43meffects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m                         \u001b[49m\u001b[43m*\u001b[49m\u001b[43min_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43meqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m effects:\n\u001b[32m   1911\u001b[39m   \u001b[38;5;66;03m# If there were ordered effects in the primitive, there should be output\u001b[39;00m\n\u001b[32m   1912\u001b[39m   \u001b[38;5;66;03m# tokens we need for subsequent ordered effects.\u001b[39;00m\n\u001b[32m   1913\u001b[39m   tokens_out = rule_ctx.tokens_out\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/interpreters/mlir.py:2023\u001b[39m, in \u001b[36mlower_per_platform\u001b[39m\u001b[34m(ctx, description, platform_rules, default_rule, effects, *rule_args, **rule_kwargs)\u001b[39m\n\u001b[32m   2021\u001b[39m \u001b[38;5;66;03m# If there is a single rule left just apply the rule, without conditionals.\u001b[39;00m\n\u001b[32m   2022\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kept_rules) == \u001b[32m1\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2023\u001b[39m   output = \u001b[43mkept_rules\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrule_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2024\u001b[39m   \u001b[38;5;28mmap\u001b[39m(\n\u001b[32m   2025\u001b[39m       \u001b[38;5;28;01mlambda\u001b[39;00m o: wrap_compute_type_in_place(ctx, o.owner),\n\u001b[32m   2026\u001b[39m       \u001b[38;5;28mfilter\u001b[39m(_is_not_block_argument, flatten_ir_values(output)),\n\u001b[32m   2027\u001b[39m   )\n\u001b[32m   2028\u001b[39m   \u001b[38;5;28mmap\u001b[39m(\n\u001b[32m   2029\u001b[39m       \u001b[38;5;28;01mlambda\u001b[39;00m o: wrap_xla_metadata_in_place(ctx, o.owner),\n\u001b[32m   2030\u001b[39m       flatten_ir_values(output),\n\u001b[32m   2031\u001b[39m   )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/interpreters/mlir.py:2212\u001b[39m, in \u001b[36mcore_call_lowering\u001b[39m\u001b[34m(ctx, name, backend, call_jaxpr, *args)\u001b[39m\n\u001b[32m   2210\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcore_call_lowering\u001b[39m(ctx: LoweringRuleContext,\n\u001b[32m   2211\u001b[39m                        *args, name, backend=\u001b[38;5;28;01mNone\u001b[39;00m, call_jaxpr):\n\u001b[32m-> \u001b[39m\u001b[32m2212\u001b[39m   out_nodes, tokens = \u001b[43mcall_lowering\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2213\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname_stack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall_jaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodule_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2214\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mavals_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mavals_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokens_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2215\u001b[39m \u001b[43m      \u001b[49m\u001b[43mdim_var_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdim_var_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2216\u001b[39m   ctx.set_tokens_out(tokens)\n\u001b[32m   2217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m out_nodes\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/interpreters/mlir.py:2197\u001b[39m, in \u001b[36mcall_lowering\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   2195\u001b[39m output_types = [token_type()] * \u001b[38;5;28mlen\u001b[39m(effects) + output_types\n\u001b[32m   2196\u001b[39m flat_output_types = flatten_ir_types(output_types)\n\u001b[32m-> \u001b[39m\u001b[32m2197\u001b[39m symbol_name = \u001b[43m_lower_jaxpr_to_fun_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2198\u001b[39m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall_jaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_stack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresult_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresult_names\u001b[49m\u001b[43m)\u001b[49m.name.value\n\u001b[32m   2200\u001b[39m tokens = [tokens_in.get(eff) \u001b[38;5;28;01mfor\u001b[39;00m eff \u001b[38;5;129;01min\u001b[39;00m effects]\n\u001b[32m   2201\u001b[39m args = (*dim_var_values, *tokens, *args)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/interpreters/mlir.py:2156\u001b[39m, in \u001b[36m_lower_jaxpr_to_fun_cached\u001b[39m\u001b[34m(ctx, fn_name, call_jaxpr, effects, name_stack, arg_names, result_names)\u001b[39m\n\u001b[32m   2154\u001b[39m     func_op = ctx.cached_primitive_lowerings[key]\n\u001b[32m   2155\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2156\u001b[39m     func_op = \u001b[43mlower_jaxpr_to_fun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall_jaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname_stack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresult_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresult_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2159\u001b[39m     ctx.cached_primitive_lowerings[key] = func_op\n\u001b[32m   2160\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/interpreters/mlir.py:1646\u001b[39m, in \u001b[36mlower_jaxpr_to_fun\u001b[39m\u001b[34m(ctx, name, jaxpr, effects, name_stack, public, replicated_args, arg_shardings, result_shardings, use_sharding_annotations, input_output_aliases, xla_donated_args, api_name, arg_names, result_names, arg_memory_kinds, result_memory_kinds, arg_layouts, result_layouts, propagated_out_mem_kinds)\u001b[39m\n\u001b[32m   1644\u001b[39m   callee_name_stack = name_stack\n\u001b[32m   1645\u001b[39m consts = [ir_constant(xla.canonicalize_dtype(x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m jaxpr.consts]\n\u001b[32m-> \u001b[39m\u001b[32m1646\u001b[39m out_vals, tokens_out = \u001b[43mjaxpr_subcomp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallee_name_stack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconsts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_var_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdim_var_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1649\u001b[39m outs: \u001b[38;5;28mlist\u001b[39m[IrValues] = []\n\u001b[32m   1650\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m eff \u001b[38;5;129;01min\u001b[39;00m effects:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/interpreters/mlir.py:1872\u001b[39m, in \u001b[36mjaxpr_subcomp\u001b[39m\u001b[34m(ctx, jaxpr, name_stack, tokens, consts, dim_var_values, *args)\u001b[39m\n\u001b[32m   1869\u001b[39m in_nodes = \u001b[38;5;28mmap\u001b[39m(read, eqn.invars)\n\u001b[32m   1870\u001b[39m source_info = eqn.source_info.replace(\n\u001b[32m   1871\u001b[39m     name_stack=name_stack + eqn.source_info.name_stack)\n\u001b[32m-> \u001b[39m\u001b[32m1872\u001b[39m loc = \u001b[43m_source_info_to_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprimitive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1873\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m source_info_util.user_context(eqn.source_info.traceback), loc:\n\u001b[32m   1874\u001b[39m   override_rule = get_override_lowering_rule(eqn.primitive)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/interpreters/mlir.py:511\u001b[39m, in \u001b[36m_source_info_to_location\u001b[39m\u001b[34m(ctx, primitive, source_info)\u001b[39m\n\u001b[32m    509\u001b[39m     loc = ir.Location.unknown()\n\u001b[32m    510\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m     loc = \u001b[43m_traceback_to_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    513\u001b[39m   frame = source_info_util.user_frame(source_info)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/DeepMind_project/CogModelingRNNsTutorial/.venv/lib/python3.12/site-packages/jax/_src/interpreters/mlir.py:482\u001b[39m, in \u001b[36m_traceback_to_location\u001b[39m\u001b[34m(ctx, tb)\u001b[39m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    481\u001b[39m   frame = source_info_util.raw_frame_to_frame(code, lasti)\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m   file_loc = \u001b[43mir\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLocation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m      \u001b[49m\u001b[43mget_canonical_source_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraceback_caches\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m      \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart_line\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m      \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    487\u001b[39m   loc = ir.Location.name(frame.function_name, childLoc=file_loc)\n\u001b[32m    488\u001b[39m   ctx.traceback_caches.location_cache[code_lasti] = loc\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# 这个相当于是最好的认知模型，在rescolar里面加了一些参数\n",
        "# 在这里就是定义几种不同的hybridRNN了，我主要是通过控制use_hidden_state和use_previous_values来控制的\n",
        "# RL-ANN: 这是最简单的HybRNN，use_hidden_state = 'False', use_previous_values = 'False'。 我们只是把rescolar的学习率用MLP替代了，不给他额外的输入。\n",
        "# Context-ANN: use_hidden_state = 'False', use_previous_values = 'True'。这里我们相当于给RL-ANN额外加一个，另外选项的Value作为额外的输入，补足context信息。\n",
        "# Memory-ANN: use_hidden_state = 'True', use_previous_values = 'False'。这里给RL-ANN额外加上一步所有的hidden-units的激活，作为额外的输入。\n",
        "# 其他的参数都不用改！！\n",
        "use_hidden_state = 'False'\n",
        "use_previous_values = 'False'\n",
        "fit_forget = \"False\"\n",
        "habit_weight = 1\n",
        "\n",
        "value_weight = 1.  # This is needed for it to be doing RL\n",
        "\n",
        "rnn_rl_params = {\n",
        "    's': use_hidden_state == 'True',\n",
        "    'o': use_previous_values == 'True',\n",
        "    'fit_forget': fit_forget == 'True',\n",
        "    'forget': 0.,\n",
        "    'w_h': habit_weight,\n",
        "    'w_v': value_weight}\n",
        "network_params = {'n_actions': 2, 'hidden_size': 8}\n",
        "\n",
        "def make_hybrnn():\n",
        "  model = hybrnn.BiControlRNN(rl_params=rnn_rl_params, network_params=network_params)\n",
        "  return model\n",
        "\n",
        "\n",
        "#从这里就是运行训练代码了\n",
        "hybrnn_params, _ = rnn_utils.fit_model(\n",
        "    model_fun=make_hybrnn,\n",
        "    dataset_train=dataset_train,\n",
        "    dataset_test=dataset_validate,\n",
        "    loss_fun='categorical',\n",
        "    optimizer=optax.chain(\n",
        "        optax.add_decayed_weights(1e-5),  # L2 正则化\n",
        "        optax.adam(learning_rate=1e-4)  # Adam 优化器\n",
        "    ),\n",
        "    n_steps_per_call=200,\n",
        "    n_steps_max=200000,\n",
        "    early_stop_step=200\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJIf1uLXwYLM",
        "outputId": "613e2379-28cc-4a12-8402-5b82183e9405"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Normalized Likelihood: 48.5%\n"
          ]
        }
      ],
      "source": [
        "mean,std = compute_log_likelihood(dataset_test, bandits.Hk_PreserveConAgentQ, rl_pc_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKUz7-bxNvub"
      },
      "outputs": [],
      "source": [
        "#@title Set up the RNN (Vanilla RNN) Model\n",
        "n_hidden = 8\n",
        "def make_vanilla_rnn():\n",
        "    model = hk.DeepRNN(\n",
        "        [hk.VanillaRNN(n_hidden), hk.Linear(output_size=2)]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "#@title Set up the RNN (Vanilla RNN) Model\n",
        "n_hidden = 8\n",
        "def make_lstm():\n",
        "    model = hk.DeepRNN(\n",
        "        [hk.LSTM(n_hidden), hk.Linear(output_size=2)]\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "khylH5F1OKHK",
        "outputId": "4eaafe01-db93-4ae3-bdc1-06e810217d6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 200 of 200; Loss: 2.6275e+03; Test Loss: 2.5221e+03. (Time: 4.7s)updating best model ..\n",
            "Step 200 of 100000; Loss: 3.0262e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1956e+03; Test Loss: 2.5076e+03. (Time: 3.9s)updating best model ..\n",
            "Step 400 of 100000; Loss: 3.0114e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.0060e+03; Test Loss: 2.4942e+03. (Time: 4.3s)updating best model ..\n",
            "Step 600 of 100000; Loss: 2.9995e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.0375e+03; Test Loss: 2.4818e+03. (Time: 4.8s)updating best model ..\n",
            "Step 800 of 100000; Loss: 2.9901e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2896e+03; Test Loss: 2.4699e+03. (Time: 3.8s)updating best model ..\n",
            "Step 1000 of 100000; Loss: 2.9821e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.0236e+03; Test Loss: 2.4578e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1200 of 100000; Loss: 2.9748e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.1469e+03; Test Loss: 2.4449e+03. (Time: 4.8s)updating best model ..\n",
            "Step 1400 of 100000; Loss: 2.9673e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.9505e+03; Test Loss: 2.4300e+03. (Time: 3.8s)updating best model ..\n",
            "Step 1600 of 100000; Loss: 2.9588e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6482e+03; Test Loss: 2.4133e+03. (Time: 3.8s)updating best model ..\n",
            "Step 1800 of 100000; Loss: 2.9494e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6579e+03; Test Loss: 2.3955e+03. (Time: 4.6s)updating best model ..\n",
            "Step 2000 of 100000; Loss: 2.9394e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6355e+03; Test Loss: 2.3780e+03. (Time: 3.8s)updating best model ..\n",
            "Step 2200 of 100000; Loss: 2.9297e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9643e+03; Test Loss: 2.3620e+03. (Time: 3.6s)updating best model ..\n",
            "Step 2400 of 100000; Loss: 2.9206e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6343e+03; Test Loss: 2.3475e+03. (Time: 4.5s)updating best model ..\n",
            "Step 2600 of 100000; Loss: 2.9125e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2995e+03; Test Loss: 2.3353e+03. (Time: 3.6s)updating best model ..\n",
            "Step 2800 of 100000; Loss: 2.9054e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1132e+03; Test Loss: 2.3244e+03. (Time: 3.6s)updating best model ..\n",
            "Step 3000 of 100000; Loss: 2.8993e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.0844e+03; Test Loss: 2.3150e+03. (Time: 4.1s)updating best model ..\n",
            "Step 3200 of 100000; Loss: 2.8938e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.6416e+03; Test Loss: 2.3065e+03. (Time: 3.6s)updating best model ..\n",
            "Step 3400 of 100000; Loss: 2.8890e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3279e+03; Test Loss: 2.2987e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3600 of 100000; Loss: 2.8845e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.6887e+03; Test Loss: 2.2914e+03. (Time: 4.0s)updating best model ..\n",
            "Step 3800 of 100000; Loss: 2.8803e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.1835e+03; Test Loss: 2.2847e+03. (Time: 3.6s)updating best model ..\n",
            "Step 4000 of 100000; Loss: 2.8765e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.6189e+03; Test Loss: 2.2784e+03. (Time: 3.6s)updating best model ..\n",
            "Step 4200 of 100000; Loss: 2.8728e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6547e+03; Test Loss: 2.2721e+03. (Time: 4.0s)updating best model ..\n",
            "Step 4400 of 100000; Loss: 2.8693e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.4394e+03; Test Loss: 2.2666e+03. (Time: 3.6s)updating best model ..\n",
            "Step 4600 of 100000; Loss: 2.8660e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6320e+03; Test Loss: 2.2607e+03. (Time: 3.6s)updating best model ..\n",
            "Step 4800 of 100000; Loss: 2.8629e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8360e+03; Test Loss: 2.2559e+03. (Time: 3.8s)updating best model ..\n",
            "Step 5000 of 100000; Loss: 2.8599e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6120e+03; Test Loss: 2.2506e+03. (Time: 4.0s)updating best model ..\n",
            "Step 5200 of 100000; Loss: 2.8572e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2125e+03; Test Loss: 2.2464e+03. (Time: 3.7s)updating best model ..\n",
            "Step 5400 of 100000; Loss: 2.8546e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0758e+03; Test Loss: 2.2418e+03. (Time: 3.7s)updating best model ..\n",
            "Step 5600 of 100000; Loss: 2.8522e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.1448e+03; Test Loss: 2.2379e+03. (Time: 4.5s)updating best model ..\n",
            "Step 5800 of 100000; Loss: 2.8498e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4851e+03; Test Loss: 2.2340e+03. (Time: 3.6s)updating best model ..\n",
            "Step 6000 of 100000; Loss: 2.8477e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3665e+03; Test Loss: 2.2303e+03. (Time: 3.6s)updating best model ..\n",
            "Step 6200 of 100000; Loss: 2.8457e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5532e+03; Test Loss: 2.2268e+03. (Time: 4.5s)updating best model ..\n",
            "Step 6400 of 100000; Loss: 2.8438e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2168e+03; Test Loss: 2.2234e+03. (Time: 3.6s)updating best model ..\n",
            "Step 6600 of 100000; Loss: 2.8421e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5063e+03; Test Loss: 2.2204e+03. (Time: 3.7s)updating best model ..\n",
            "Step 6800 of 100000; Loss: 2.8404e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6546e+03; Test Loss: 2.2172e+03. (Time: 4.2s)updating best model ..\n",
            "Step 7000 of 100000; Loss: 2.8389e+03. (Time: 0.0s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _xla_gc_callback at 0x7d103f43cfe0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "    \n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 200 of 200; Loss: 3.3487e+03; Test Loss: 2.2146e+03. (Time: 4.0s)updating best model ..\n",
            "Step 7200 of 100000; Loss: 2.8375e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6303e+03; Test Loss: 2.2115e+03. (Time: 3.7s)updating best model ..\n",
            "Step 7400 of 100000; Loss: 2.8362e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7915e+03; Test Loss: 2.2091e+03. (Time: 4.1s)updating best model ..\n",
            "Step 7600 of 100000; Loss: 2.8350e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6021e+03; Test Loss: 2.2060e+03. (Time: 3.7s)updating best model ..\n",
            "Step 7800 of 100000; Loss: 2.8340e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1777e+03; Test Loss: 2.2033e+03. (Time: 3.7s)updating best model ..\n",
            "Step 8000 of 100000; Loss: 2.8327e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0539e+03; Test Loss: 2.1991e+03. (Time: 3.9s)updating best model ..\n",
            "Step 8200 of 100000; Loss: 2.8297e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.1424e+03; Test Loss: 2.1942e+03. (Time: 3.6s)updating best model ..\n",
            "Step 8400 of 100000; Loss: 2.8250e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4035e+03; Test Loss: 2.1899e+03. (Time: 3.6s)updating best model ..\n",
            "Step 8600 of 100000; Loss: 2.8194e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3696e+03; Test Loss: 2.1861e+03. (Time: 3.9s)updating best model ..\n",
            "Step 8800 of 100000; Loss: 2.8164e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4715e+03; Test Loss: 2.1830e+03. (Time: 3.8s)updating best model ..\n",
            "Step 9000 of 100000; Loss: 2.8092e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2051e+03; Test Loss: 2.1794e+03. (Time: 3.7s)updating best model ..\n",
            "Step 9200 of 100000; Loss: 2.8057e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4275e+03; Test Loss: 2.1765e+03. (Time: 3.7s)updating best model ..\n",
            "Step 9400 of 100000; Loss: 2.8032e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6944e+03; Test Loss: 2.1738e+03. (Time: 4.3s)updating best model ..\n",
            "Step 9600 of 100000; Loss: 2.8014e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2116e+03; Test Loss: 2.1717e+03. (Time: 3.6s)updating best model ..\n",
            "Step 9800 of 100000; Loss: 2.7998e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6479e+03; Test Loss: 2.1694e+03. (Time: 3.7s)updating best model ..\n",
            "Step 10000 of 100000; Loss: 2.7984e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6075e+03; Test Loss: 2.1676e+03. (Time: 4.5s)updating best model ..\n",
            "Step 10200 of 100000; Loss: 2.7970e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6094e+03; Test Loss: 2.1656e+03. (Time: 3.7s)updating best model ..\n",
            "Step 10400 of 100000; Loss: 2.7958e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9776e+03; Test Loss: 2.1641e+03. (Time: 3.7s)updating best model ..\n",
            "Step 10600 of 100000; Loss: 2.7945e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0370e+03; Test Loss: 2.1623e+03. (Time: 4.5s)updating best model ..\n",
            "Step 10800 of 100000; Loss: 2.7933e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2220e+03; Test Loss: 2.1603e+03. (Time: 4.1s)updating best model ..\n",
            "Step 11000 of 100000; Loss: 2.7919e+03. (Time: 0.0s)\n",
            "Step 30 of 200; Loss: 4.4276e+03; Test Loss: 2.1604e+03. (Time: 1.6s)"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3664668.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#n_steps_max = 1000000 #@param\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m gru_params, _, all_losses = rnn_utils.fit_model(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_fun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_lstm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CogModelingRNNsTutorial/rnn_utils.py\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model_fun, dataset_train, dataset_test, optimizer, loss_fun, random_key, n_steps_per_call, n_steps_max, return_all_losses, early_stop_step, if_early_stop)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m     params, opt_state, losses = train_model(\n\u001b[0m\u001b[1;32m    549\u001b[0m         \u001b[0mmodel_fun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CogModelingRNNsTutorial/rnn_utils.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_fun, dataset_train, dataset_test, optimizer, random_key, opt_state, params, n_steps, penalty_scale, loss_fun, do_plot, truncate_seq_length)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;31m# Log every 10th step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m       print((f'\\rStep {step + 1} of {n_steps}; '\n\u001b[1;32m    282\u001b[0m              \u001b[0;34mf'Loss: {loss:.4e}; Test Loss: {test_loss:.4e}. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/numpy/array_methods.py\u001b[0m in \u001b[0;36mdeferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mswap\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_accepted_binop_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m     \u001b[0;31m# Note: don't use isinstance here, because we don't want to raise for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;31m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title Fit the RNN (GRU) model\n",
        "#n_steps_max = 1000000 #@param\n",
        "\n",
        "gru_params, _, all_losses = rnn_utils.fit_model(\n",
        "    model_fun=make_lstm,\n",
        "    dataset_train = dataset_train,\n",
        "    dataset_test = dataset_validate,\n",
        "    loss_fun='categorical',\n",
        "    optimizer= optax.chain(\n",
        "        optax.add_decayed_weights(1e-5),  # L2 正则化\n",
        "        optax.adam(learning_rate=1e-4)  # Adam 优化器\n",
        "    ),\n",
        "    n_steps_per_call=200,\n",
        "    n_steps_max=100000,\n",
        "    return_all_losses=True,\n",
        "    early_stop_step=200,\n",
        "    if_early_stop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOpDYABeOaEM"
      },
      "outputs": [],
      "source": [
        "# #@title Compute log-likelihood\n",
        "# def compute_log_likelihood(dataset, model_fun, params):\n",
        "\n",
        "#   xs, actual_choices = next(dataset)\n",
        "#   n_trials_per_session, n_sessions = actual_choices.shape[:2]\n",
        "#   model_outputs, model_states = rnn_utils.eval_model(model_fun, params, xs)\n",
        "\n",
        "#   predicted_log_choice_probabilities = np.array(jax.nn.log_softmax(model_outputs[:, :, :2]))\n",
        "\n",
        "#   log_likelihood = 0\n",
        "#   n = 0  # Total number of trials across sessions.\n",
        "#   for sess_i in range(n_sessions):\n",
        "#     for trial_i in range(n_trials_per_session):\n",
        "#       actual_choice = int(actual_choices[trial_i, sess_i])\n",
        "#       if actual_choice >= 0:  # values < 0 are invalid trials which we ignore.\n",
        "#         log_likelihood += predicted_log_choice_probabilities[trial_i, sess_i, actual_choice]\n",
        "#         n += 1\n",
        "\n",
        "#   normalized_likelihood = np.exp(log_likelihood / n)\n",
        "\n",
        "#   print(f'Normalized Likelihood: {100 * normalized_likelihood:.1f}%')\n",
        "\n",
        "#   return normalized_likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ute8Yj_xOMd8",
        "outputId": "ca85be09-e42c-41a5-dd3e-13c9a6bc0d28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalized Likelihoods for GRU\n",
            "Training Dataset\n",
            "Average Normalized Likelihood: 61.3%\n",
            "Held-Out Dataset\n",
            "Average Normalized Likelihood: 62.0%\n"
          ]
        }
      ],
      "source": [
        "#@title Compute quality-of-fit: Held-out Normalized Likelihood\n",
        "# Compute log-likelihood\n",
        "print('Normalized Likelihoods for GRU')\n",
        "print('Training Dataset')\n",
        "training_likelihood = compute_log_likelihood(dataset_train, make_lstm, gru_params)\n",
        "print('Held-Out Dataset')\n",
        "testing_likelihood = compute_log_likelihood(dataset_validate, make_lstm, gru_params)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "i7x3Ff2xWTdb",
        "YOGiNjm4FUAx",
        "WmgO7HywNl5-",
        "GvGdnab8CdCN",
        "mrmkwDvr500Q",
        "9WygdkV_tSUA",
        "EYkrrFeXOqG2"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
