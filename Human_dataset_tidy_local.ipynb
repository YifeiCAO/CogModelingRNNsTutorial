{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7x3Ff2xWTdb"
      },
      "source": [
        "# Import and preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mYuMvSuvyGuR"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "import gdown\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CPp79Z477SU8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotnine as gg\n",
        "gg.theme_set(gg.theme_classic)  # for nicer-looking plots\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "import optax\n",
        "import scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CudaDevice(id=0)]\n"
          ]
        }
      ],
      "source": [
        "devices = jax.devices()\n",
        "print(devices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWQY_uJK7Uyx",
        "outputId": "983c0b42-ee55-48ed-e507-ccba22a8289f"
      },
      "outputs": [],
      "source": [
        "# !pip install -U dm-haiku\n",
        "import haiku as hk\n",
        "rng_seq = hk.PRNGSequence(np.random.randint(2**32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX2twY1NNT1r",
        "outputId": "c73fd2f4-31d7-470b-caf5-2d9dc712511d"
      },
      "outputs": [],
      "source": [
        "# #@title Install required packages.\n",
        "# try:\n",
        "#     from google.colab import files\n",
        "#     _ON_COLAB = True\n",
        "# except:\n",
        "#     _ON_COLAB = False\n",
        "\n",
        "# if _ON_COLAB:\n",
        "#   !rm -rf CogModelingRNNsTutorial\n",
        "#   !git clone https://github.com/YifeiCAO/CogModelingRNNsTutorial\n",
        "#   !pip install -e CogModelingRNNsTutorial/CogModelingRNNsTutorial\n",
        "#   !cp CogModelingRNNsTutorial/CogModelingRNNsTutorial/*py CogModelingRNNsTutorial\n",
        "# else:\n",
        "#   !pip install CogModelingRNNsTutorial/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_2jivtclNVjS"
      },
      "outputs": [],
      "source": [
        "#@title Imports + defaults settings.\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "# for reload\n",
        "# %reload_ext autoreload\n",
        "\n",
        "# import haiku as hk\n",
        "# import jax\n",
        "# import jax.numpy as jnp\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optax\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# try:\n",
        "#     from google.colab import files\n",
        "#     _ON_COLAB = True\n",
        "# except:\n",
        "#     _ON_COLAB = False\n",
        "\n",
        "from CogModelingRNNsTutorial import bandits\n",
        "from CogModelingRNNsTutorial import disrnn\n",
        "from CogModelingRNNsTutorial import hybrnn\n",
        "from CogModelingRNNsTutorial import hybconrnn\n",
        "from CogModelingRNNsTutorial import hybrnn_direct_con\n",
        "from CogModelingRNNsTutorial import plotting\n",
        "from CogModelingRNNsTutorial import rat_data\n",
        "from CogModelingRNNsTutorial import rnn_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyjWR7YmyPQB",
        "outputId": "0101e98d-62cc-42d5-a6d9-80fd8c89a044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File downloaded and read successfully!\n",
            "xs.shape = (60, 206, 2)\n",
            "ys.shape = (60, 206, 1)\n"
          ]
        }
      ],
      "source": [
        "osf_url = 'https://osf.io/xe6yu/download?direct=1'\n",
        "response = requests.get(osf_url)\n",
        "\n",
        "# Check the request\n",
        "if response.status_code == 200:\n",
        "    # Read as pandas dataframe\n",
        "    qasim_data = pd.read_csv(StringIO(response.text))\n",
        "    print('File downloaded and read successfully!')\n",
        "else:\n",
        "    print('Failed to download file. Status code:', response.status_code)\n",
        "\n",
        "qasim_data.head()\n",
        "\n",
        "# # read data\n",
        "selected_columns = ['participant', 'trials_gamble', 'gamble', 'prob', 'reward']\n",
        "\n",
        "qasim = qasim_data[selected_columns]\n",
        "qasim_filtered = qasim[qasim['trials_gamble'].notna()]\n",
        "qasim_sorted = qasim_filtered.groupby('participant', group_keys=False).apply(lambda x: x.sort_values('trials_gamble'))\n",
        "qasim_sorted = qasim_sorted.reset_index(drop=True)\n",
        "qasim_sorted['participant'] = qasim_sorted.groupby(['participant']).ngroup() + 1\n",
        "qasim_sorted['action'] = qasim_sorted['gamble']\n",
        "qasim_sorted\n",
        "\n",
        "# —— 2) 把缺失的 action 填成 -1 —— #\n",
        "qasim_sorted['action'] = qasim_sorted['action'].fillna(-1).astype(int)\n",
        "\n",
        "# —— 3) 如果需要，把 reward 映射到 0/1 —— #\n",
        "# （如果已经是 0/1 可跳过；否则取消下面注释并调整映射字典）\n",
        "# qasim_sorted['reward'] = (\n",
        "#     qasim_sorted['reward']\n",
        "#     .map({-1: 0, 1: 1})\n",
        "#     .fillna(-1)\n",
        "#     .astype(int)\n",
        "# )\n",
        "\n",
        "# —— 4) 排序，确保 trial 顺序 —— #\n",
        "qasim_sorted = qasim_sorted.sort_values(\n",
        "    ['participant', 'trials_gamble']\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# —— 5) 生成下一步动作 action_n —— #\n",
        "qasim_sorted['action_n'] = (\n",
        "    qasim_sorted\n",
        "    .groupby('participant')['action']\n",
        "    .shift(-1)\n",
        ")\n",
        "# 每个 participant 最后一 trial 的 action_n 设为 -1\n",
        "last_idxs = qasim_sorted.groupby('participant').tail(1).index\n",
        "qasim_sorted.loc[last_idxs, 'action_n'] = -1\n",
        "\n",
        "# —— 6) 按 participant 构造 xs_list, ys_list —— #\n",
        "xs_list, ys_list = [], []\n",
        "for pid, grp in qasim_sorted.groupby('participant'):\n",
        "    grp = grp.sort_values('trials_gamble')\n",
        "    x = grp[['prob', 'reward']].to_numpy().astype(float)    # 输入特征\n",
        "    y = grp[['action_n']].to_numpy().astype(int)             # 下一步动作\n",
        "    xs_list.append(x)\n",
        "    ys_list.append(y)\n",
        "\n",
        "# —— 7) 堆成三维数组 —— #\n",
        "xs_qa = np.stack(xs_list, axis=1)  # (n_sessions, n_trials, 2)\n",
        "ys_qa = np.stack(ys_list, axis=1)  # (n_sessions, n_trials, 1)\n",
        "\n",
        "print(\"xs.shape =\", xs_qa.shape)\n",
        "print(\"ys.shape =\", ys_qa.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNCQ2haXljHL"
      },
      "source": [
        "### Sidarus dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zetxo6c_lgFd",
        "outputId": "f8f0f5e1-0f46-452d-ce7e-87a314963918"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'action'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m~/anaconda3/envs/CogModel/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'action'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m sida_data\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 1) action 从 {1,2} → {0,1}，并把所有缺失值填成 -1\u001b[39;00m\n\u001b[1;32m     14\u001b[0m sida_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m---> 15\u001b[0m     \u001b[43msida_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m.\u001b[39mmap({\u001b[38;5;241m1\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m: \u001b[38;5;241m1\u001b[39m})    \u001b[38;5;66;03m# 映射\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)           \u001b[38;5;66;03m# 缺失值设为 -1\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 2) outcome 从 {-1,1} → {0,1}，缺失也填成 -1\u001b[39;00m\n\u001b[1;32m     22\u001b[0m sida_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     23\u001b[0m     sida_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutcome\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;241m.\u001b[39mmap({\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;241m1\u001b[39m})\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     27\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/envs/CogModel/lib/python3.10/site-packages/pandas/core/frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[0;32m~/anaconda3/envs/CogModel/lib/python3.10/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[0;31mKeyError\u001b[0m: 'action'"
          ]
        }
      ],
      "source": [
        "# 修改后的 file_id\n",
        "file_id = '1TSV6CdyClKz831qD2ln4z3WjLLcOgG1n'\n",
        "download_url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "\n",
        "# 下载并保存为 'downloaded_file.csv'\n",
        "output_file = 'downloaded_file.csv'\n",
        "gdown.download(download_url, output_file, quiet=False, fuzzy=True)\n",
        "\n",
        "# 读取 CSV 数据\n",
        "sida_data = pd.read_csv(output_file)\n",
        "sida_data\n",
        "\n",
        "# 1) action 从 {1,2} → {0,1}，并把所有缺失值填成 -1\n",
        "sida_data['action'] = (\n",
        "    sida_data['action']\n",
        "    .map({1: 0, 2: 1})    # 映射\n",
        "    .fillna(-1)           # 缺失值设为 -1\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "# 2) outcome 从 {-1,1} → {0,1}，缺失也填成 -1\n",
        "sida_data['reward'] = (\n",
        "    sida_data['outcome']\n",
        "    .map({-1: 0, 1: 1})\n",
        "    .fillna(-1)\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "# 3) 给每个被试×session 分配一个连续的 session_id\n",
        "sida_data['session_id'] = (\n",
        "    sida_data\n",
        "    .groupby(['subj', 'session'], sort=False)\n",
        "    .ngroup()\n",
        "    + 1\n",
        ")\n",
        "\n",
        "# 4) 排序，保证 trial 顺序\n",
        "sida_data = sida_data.sort_values(\n",
        "    ['session_id', 'epN', 'epTrialN']\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# 5) 生成下一步动作 action_n；每个 session 末尾设为 -1\n",
        "sida_data['action_n'] = (\n",
        "    sida_data\n",
        "    .groupby('session_id')['action']\n",
        "    .shift(-1)\n",
        ")\n",
        "last_idx = sida_data.groupby('session_id').tail(1).index\n",
        "sida_data.loc[last_idx, 'action_n'] = -1\n",
        "\n",
        "# 6) 按 session_id 抽取序列，并堆成 xs, ys\n",
        "xs_list, ys_list = [], []\n",
        "for sid, grp in sida_data.groupby('session_id'):\n",
        "    grp = grp.sort_values(['epN', 'epTrialN'])\n",
        "    x = grp[['hiRewAct', 'reward']].to_numpy().astype(float)  # 特征\n",
        "    y = grp[['action_n']].to_numpy().astype(int)              # Label\n",
        "    xs_list.append(x)\n",
        "    ys_list.append(y)\n",
        "\n",
        "# 最终结果：xs.shape == (n_sessions, n_trials, 2)，ys.shape == (n_sessions, n_trials, 1)\n",
        "xs_sida = np.stack(xs_list, axis=1)\n",
        "ys_sida = np.stack(ys_list, axis=1)\n",
        "\n",
        "print(\"xs.shape =\", xs_sida.shape)\n",
        "print(\"ys.shape =\", ys_sida.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "147k59EcluMe"
      },
      "source": [
        "### Schaaf dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRUXDOuEltlO",
        "outputId": "4d607a65-5003-4439-f314-daa193744c6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "xs.shape = (249, 94, 2)\n",
            "ys.shape = (249, 94, 1)\n"
          ]
        }
      ],
      "source": [
        "# # 修改后的 file_id\n",
        "# file_id = '1rJFmDhCE3fdXtSHSSvtre_7V49gGsg7P'\n",
        "# download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "# # 下载并保存为 'downloaded_file.csv'\n",
        "# output_file = 'downloaded_file.csv'\n",
        "# gdown.download(download_url, output_file, quiet=False)\n",
        "\n",
        "# 读取 CSV 数据\n",
        "schaaf_data = pd.read_csv(output_file)\n",
        "schaaf_data\n",
        "\n",
        "df1 = schaaf_data[['pp', 'trial1', 'response1', 'outcome1']].copy()\n",
        "df1.columns = ['pp', 'trial_in_session', 'response', 'reward']\n",
        "df1['session'] = 1\n",
        "\n",
        "df2 = schaaf_data[['pp', 'trial2', 'response2', 'outcome2']].copy()\n",
        "df2.columns = ['pp', 'trial_in_session', 'response', 'reward']\n",
        "df2['session'] = 2\n",
        "\n",
        "# —— 上面拆分、concat、sort 的部分保持不变 —— #\n",
        "\n",
        "# 合并成长表\n",
        "df_long = pd.concat([df1, df2], ignore_index=True)\n",
        "df_long = df_long.sort_values(['pp', 'session', 'trial_in_session']).reset_index(drop=True)\n",
        "\n",
        "# —— 映射 outcome，再填 missing response —— #\n",
        "# 把原来的 response1/response2 改名后是 `response`\n",
        "# 把 outcome1/outcome2 改名后是 `reward`\n",
        "# 把 outcome 映射成 1/0，然后把原本缺失的 reward 补成 -1\n",
        "df_long['reward'] = (\n",
        "    df_long['reward']\n",
        "    .map({1: 1, -1: 0})\n",
        "    .fillna(-1)\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "# 把缺失的 response 一样补成 -1\n",
        "df_long['response'] = (\n",
        "    df_long['response']\n",
        "    .fillna(-1)\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "\n",
        "# 接下来再做 session_id、shift action_n、以及 stack xs/ys 的流程……\n",
        "df_long['session_id'] = (df_long['pp'] - 1) * 2 + df_long['session']\n",
        "df_long['action_n'] = df_long.groupby('session_id')['response'].shift(-1)\n",
        "last_idx = df_long.groupby('session_id').tail(1).index\n",
        "df_long.loc[last_idx, 'action_n'] = -1\n",
        "\n",
        "# 生成 xs, ys\n",
        "session_ids = df_long['session_id'].unique()\n",
        "xs_list, ys_list = [], []\n",
        "for sid in session_ids:\n",
        "    sd = df_long[df_long['session_id'] == sid].sort_values('trial_in_session')\n",
        "    x = sd[['response', 'reward']].to_numpy().astype(float)\n",
        "    y = sd[['action_n']].to_numpy().astype(int)\n",
        "    xs_list.append(x)\n",
        "    ys_list.append(y)\n",
        "\n",
        "xs = np.stack(xs_list, axis=0)   # (n_sessions, n_trials, 2)\n",
        "ys = np.stack(ys_list, axis=0)   # (n_sessions, n_trials, 1)\n",
        "\n",
        "\n",
        "# 或者第 0 维是 trials，第 1 维是 sessions：\n",
        "xs_sch = np.stack(xs_list, axis=1)  # (n_trials, n_sessions, 2)\n",
        "ys_sch = np.stack(ys_list, axis=1)  # (n_trials, n_sessions, 1)\n",
        "\n",
        "print(\"xs.shape =\", xs_sch.shape)\n",
        "print(\"ys.shape =\", ys_sch.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8KQplq3tHC7"
      },
      "source": [
        "### Maria Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDBAI_AwD1PF",
        "outputId": "a54a9388-a125-4948-b27e-e97bc0ba8ab5"
      },
      "outputs": [
        {
          "ename": "FileURLRetrievalError",
          "evalue": "Failed to retrieve file url:\n\n\tCannot retrieve the public link of the file. You may need to change\n\tthe permission to 'Anyone with the link', or have had many accesses.\n\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=1N_zAy-qrbfjvF8Kbb504IH2JNhR5KI-P\n\nbut Gdown can't. Please check connections and permissions.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileURLRetrievalError\u001b[0m                     Traceback (most recent call last)",
            "File \u001b[0;32m~/anaconda3/envs/CogModel/lib/python3.10/site-packages/gdown/download.py:267\u001b[0m, in \u001b[0;36mdownload\u001b[0;34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 267\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[43mget_url_from_gdrive_confirmation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m FileURLRetrievalError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m~/anaconda3/envs/CogModel/lib/python3.10/site-packages/gdown/download.py:55\u001b[0m, in \u001b[0;36mget_url_from_gdrive_confirmation\u001b[0;34m(contents)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m url:\n\u001b[0;32m---> 55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FileURLRetrievalError(\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot retrieve the public link of the file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou may need to change the permission to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnyone with the link\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, or have had many accesses. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     60\u001b[0m     )\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m url\n",
            "\u001b[0;31mFileURLRetrievalError\u001b[0m: Cannot retrieve the public link of the file. You may need to change the permission to 'Anyone with the link', or have had many accesses. Check FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileURLRetrievalError\u001b[0m                     Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m file_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1N_zAy-qrbfjvF8Kbb504IH2JNhR5KI-P\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m url     \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://drive.google.com/uc?id=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mgdown\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdownloaded_file.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m eck_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdownloaded_file.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# —— 2) 筛选＆重命名列 —— #\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/CogModel/lib/python3.10/site-packages/gdown/download.py:278\u001b[0m, in \u001b[0;36mdownload\u001b[0;34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m FileURLRetrievalError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    269\u001b[0m         message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    270\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to retrieve file url:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou may still be able to access the file from the browser:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m             url_origin,\n\u001b[1;32m    277\u001b[0m         )\n\u001b[0;32m--> 278\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FileURLRetrievalError(message)\n\u001b[1;32m    280\u001b[0m filename_from_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    281\u001b[0m last_modified_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[0;31mFileURLRetrievalError\u001b[0m: Failed to retrieve file url:\n\n\tCannot retrieve the public link of the file. You may need to change\n\tthe permission to 'Anyone with the link', or have had many accesses.\n\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=1N_zAy-qrbfjvF8Kbb504IH2JNhR5KI-P\n\nbut Gdown can't. Please check connections and permissions."
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gdown\n",
        "\n",
        "# —— 1) 下载并读入原始数据 —— #\n",
        "file_id = '1N_zAy-qrbfjvF8Kbb504IH2JNhR5KI-P'\n",
        "url     = f'https://drive.google.com/uc?id={file_id}'\n",
        "gdown.download(url, 'downloaded_file.csv', quiet=False)\n",
        "eck_data = pd.read_csv('downloaded_file.csv')\n",
        "\n",
        "# —— 2) 筛选＆重命名列 —— #\n",
        "sel = ['sID','TrialID','selected_box','reward']\n",
        "eck_sorted = eck_data[sel].copy()\n",
        "eck_sorted['participant'] = eck_sorted.groupby('sID').ngroup()+1\n",
        "eck_sorted['action']      = eck_sorted['selected_box']\n",
        "\n",
        "# 只保留至少做够 120 试次的被试\n",
        "max_trial = eck_sorted.groupby('participant')['TrialID'].transform('max')\n",
        "eck_sorted = eck_sorted[max_trial>=120]\n",
        "\n",
        "# 只用前 120 试次\n",
        "eck_sorted = eck_sorted[eck_sorted['TrialID']<=120].reset_index(drop=True)\n",
        "\n",
        "# —— 3) 生成下一步动作 action_n —— #\n",
        "def generate_action_n(group):\n",
        "    group = group.sort_values('TrialID')\n",
        "    group['action_n'] = group['action'].shift(-1).fillna(-1).astype(int)\n",
        "    return group\n",
        "\n",
        "eck_sorted = (\n",
        "    eck_sorted\n",
        "    .groupby('participant', group_keys=False)\n",
        "    .apply(generate_action_n)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# —— 4) 按 participant 构造 xs_list/ys_list —— #\n",
        "xs_list, ys_list = [], []\n",
        "for pid, grp in eck_sorted.groupby('participant'):\n",
        "    grp = grp.sort_values('TrialID').iloc[:120]\n",
        "    x = grp[['action','reward']].to_numpy().astype(float)  # (120,2)\n",
        "    y = grp[['action_n']].to_numpy().astype(int)           # (120,1)\n",
        "    xs_list.append(x)\n",
        "    ys_list.append(y)\n",
        "\n",
        "# —— 5) stack 成 (n_sessions, n_trials, feat_dim) —— #\n",
        "xs_ma = np.stack(xs_list, axis=1)  # (305,120,2)\n",
        "ys_ma = np.stack(ys_list, axis=1)  # (305,120,1)\n",
        "\n",
        "print(\"xs_ma.shape =\", xs_ma.shape)\n",
        "print(\"ys_ma.shape =\", ys_ma.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbm2178XrKVd"
      },
      "source": [
        "### Generate a big human dataset with all experiments, session length is 130 trial per session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ9SycOgtbsK",
        "outputId": "9650c98e-c381-4581-eda2-0f4849543525"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total segments: 394\n",
            "First xs segment shape: (130, 2)\n",
            "First ys segment shape: (130, 1)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def segment_and_pad(x: np.ndarray,\n",
        "                    y: np.ndarray,\n",
        "                    seg_len: int = 130,\n",
        "                    pad_x: float = 0.,\n",
        "                    pad_y: int = -1):\n",
        "    T, D = x.shape\n",
        "    n_segs = int(np.ceil(T / seg_len))\n",
        "    x_segs, y_segs = [], []\n",
        "    for i in range(n_segs):\n",
        "        start = i * seg_len\n",
        "        end = start + seg_len\n",
        "        x_part = x[start : min(end, T)]\n",
        "        y_part = y[start : min(end, T)]\n",
        "        pad = end - min(end, T)\n",
        "        if pad > 0:\n",
        "            x_part = np.pad(x_part,\n",
        "                            pad_width=((0, pad), (0, 0)),\n",
        "                            constant_values=pad_x)\n",
        "            y_part = np.pad(y_part,\n",
        "                            pad_width=((0, pad), (0, 0)),\n",
        "                            constant_values=pad_y)\n",
        "        x_segs.append(x_part)\n",
        "        y_segs.append(y_part)\n",
        "    return x_segs, y_segs\n",
        "\n",
        "# —— 假设你已经有这几组 (xs, ys) —— #\n",
        "# xs_qa   (60,  206, 2), ys_qa  (60,206, 1)\n",
        "# xs_sida (800, 40,  2), ys_sida(800,40, 1)\n",
        "# xs_sch  (249, 44,  2), ys_sch (249,44, 1)\n",
        "# xs_ma   (120,305, 2), ys_ma  (120,305,1)\n",
        "\n",
        "all_xs, all_ys = [], []\n",
        "\n",
        "for xs, ys in [(xs_qa, ys_qa),\n",
        "               (xs_sida, ys_sida),\n",
        "               (xs_sch, ys_sch),\n",
        "               (xs_ma, ys_ma)\n",
        "               ]:\n",
        "\n",
        "    # 如果是 (n_trials, n_sessions, feat) 维度，就直接：\n",
        "    # T, N, D = xs.shape\n",
        "    # 否则若是 (n_sessions, n_trials, feat)，先转：\n",
        "    # xs = xs.transpose(1,0,2)\n",
        "    # ys = ys.transpose(1,0,2)\n",
        "\n",
        "    T, N, D = xs.shape\n",
        "    for sess in range(N):\n",
        "        x_seq = xs[:, sess, :]  # (T, D)\n",
        "        y_seq = ys[:, sess, :]  # (T, 1)\n",
        "        x_segs, y_segs = segment_and_pad(\n",
        "            x_seq, y_seq,\n",
        "            seg_len=130,\n",
        "            pad_x=0., pad_y=-1\n",
        "        )\n",
        "        all_xs.extend(x_segs)\n",
        "        all_ys.extend(y_segs)\n",
        "\n",
        "# —— 修改在这里：不要 np.stack，直接输出列表 —— #\n",
        "# all_xs 是一个 Python list，长度 = 总片段数，每个元素 shape=(130, D)\n",
        "# all_ys 是一个 Python list，长度 = 总片段数，每个元素 shape=(130, 1)\n",
        "\n",
        "print(f\"Total segments: {len(all_xs)}\")\n",
        "print(f\"First xs segment shape: {all_xs[0].shape}\")\n",
        "print(f\"First ys segment shape: {all_ys[0].shape}\")\n",
        "\n",
        "# 如果你需要把它们返回成变量：\n",
        "xs_segment_list = all_xs\n",
        "ys_segment_list = all_ys\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Y-JHKuQv48Ur"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def format_into_datasets_multi_source(\n",
        "    xs_list: list[np.ndarray],\n",
        "    ys_list: list[np.ndarray],\n",
        "    dataset_constructor,\n",
        "    n_train_sessions: int,\n",
        "    n_test_sessions: int,\n",
        "    n_validate_sessions: int,\n",
        "    batch_size: int = None,\n",
        "    random_seed: int = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    按照 QA、SIDA、SCH、MA 这 4 个来源的数据源比例，\n",
        "    在它们各自内部抽取 train/test/val session，\n",
        "    最后拼成全局的 DatasetRNN。\n",
        "\n",
        "    xs_list, ys_list:\n",
        "      长度 4 的 list，每个元素形状是 (timesteps, n_sessions_i, feat)\n",
        "    n_*_sessions:\n",
        "      全局希望 train/test/val 一共要多少 session\n",
        "    \"\"\"\n",
        "    if random_seed is not None:\n",
        "        rng = np.random.RandomState(random_seed)\n",
        "    else:\n",
        "        rng = np.random\n",
        "\n",
        "    # 1) 计算每个来源各有多少 session\n",
        "    sess_counts = np.array([xs.shape[1] for xs in xs_list])  # e.g. [206, 40, 44, 305]\n",
        "    total_sessions = sess_counts.sum()\n",
        "\n",
        "    # 2) 按比例分配到每个来源的 train/test/val 数目\n",
        "    def proportional_alloc(total, counts):\n",
        "        floats = counts / counts.sum() * total\n",
        "        floors = np.floor(floats).astype(int)\n",
        "        rem = total - floors.sum()\n",
        "        # 剩余的按余数最大的那些来源补齐\n",
        "        remainders = floats - floors\n",
        "        for idx in np.argsort(remainders)[-rem:]:\n",
        "            floors[idx] += 1\n",
        "        return floors\n",
        "\n",
        "    n_train_per = proportional_alloc(n_train_sessions,    sess_counts)\n",
        "    n_test_per  = proportional_alloc(n_test_sessions,     sess_counts)\n",
        "    n_val_per   = proportional_alloc(n_validate_sessions, sess_counts)\n",
        "\n",
        "    # 3) 在每个来源内部随机打乱并切分\n",
        "    train_idx_list, test_idx_list, val_idx_list = [], [], []\n",
        "    for cnt, n_tr, n_te, n_va in zip(sess_counts,\n",
        "                                     n_train_per,\n",
        "                                     n_test_per,\n",
        "                                     n_val_per):\n",
        "        all_idx = np.arange(cnt)\n",
        "        rng.shuffle(all_idx)\n",
        "        train_idx_list.append(all_idx[:n_tr])\n",
        "        test_idx_list.append( all_idx[n_tr:n_tr+n_te] )\n",
        "        val_idx_list.append(  all_idx[n_tr+n_te:n_tr+n_te+n_va] )\n",
        "\n",
        "    # 4) 汇总抽到的 sessions：concat 出全局 xs/ys\n",
        "    def gather(xs_list, ys_list, idx_lists):\n",
        "        parts_x, parts_y = [], []\n",
        "        for xs, ys, idx in zip(xs_list, ys_list, idx_lists):\n",
        "            # xs: (timesteps, n_sessions_i, feat)\n",
        "            parts_x.append(xs[:, idx, :])\n",
        "            parts_y.append(ys[:, idx, :])\n",
        "        return np.concatenate(parts_x, axis=1), np.concatenate(parts_y, axis=1)\n",
        "\n",
        "    xs_train, ys_train = gather(xs_list, ys_list, train_idx_list)\n",
        "    xs_test,  ys_test  = gather(xs_list, ys_list, test_idx_list)\n",
        "    xs_val,   ys_val   = gather(xs_list, ys_list, val_idx_list)\n",
        "\n",
        "    # 5) 构造 DatasetRNN\n",
        "    ds_train = dataset_constructor(xs_train, ys_train, batch_size=batch_size)\n",
        "    ds_test  = dataset_constructor(xs_test,  ys_test,  batch_size=batch_size)\n",
        "    ds_val   = dataset_constructor(xs_val,   ys_val,   batch_size=batch_size)\n",
        "\n",
        "    return ds_train, ds_test, ds_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VnupWYiP5vW6"
      },
      "outputs": [],
      "source": [
        "# 假设你已经有原始的：\n",
        "#   xs_qa   (T_qa,   N_qa,   D),   ys_qa   (T_qa,   N_qa,   1)\n",
        "#   xs_sida (T_sida, N_sida, D),   ys_sida (T_sida, N_sida, 1)\n",
        "#   xs_sch  (T_sch,  N_sch,  D),   ys_sch  (T_sch,  N_sch,  1)\n",
        "#   xs_ma   (T_ma,   N_ma,   D),   ys_ma   (T_ma,   N_ma,   1)\n",
        "\n",
        "def make_segmented_array(xs, ys, seg_len=130, pad_x=0., pad_y=-1):\n",
        "    all_xs, all_ys = [], []\n",
        "    T, N, D = xs.shape\n",
        "    for sess in range(N):\n",
        "        x_seq = xs[:, sess, :]    # (T, D)\n",
        "        y_seq = ys[:, sess, :]    # (T, 1)\n",
        "        x_segs, y_segs = segment_and_pad(x_seq, y_seq, seg_len, pad_x, pad_y)\n",
        "        all_xs.extend(x_segs)     # list of (130, D)\n",
        "        all_ys.extend(y_segs)     # list of (130, 1)\n",
        "    # 把 list 再拼成一个三维 array (130, n_segments, D)\n",
        "    xs_seg = np.stack(all_xs, axis=1)\n",
        "    ys_seg = np.stack(all_ys, axis=1)\n",
        "    return xs_seg, ys_seg\n",
        "\n",
        "# 针对四个源分别做一次\n",
        "xs_qa_seg,   ys_qa_seg   = make_segmented_array(xs_qa,   ys_qa)\n",
        "# xs_sida_seg,ys_sida_seg = make_segmented_array(xs_sida, ys_sida)\n",
        "xs_sch_seg, ys_sch_seg  = make_segmented_array(xs_sch,  ys_sch)\n",
        "# xs_ma_seg,  ys_ma_seg   = make_segmented_array(xs_ma,   ys_ma)\n",
        "\n",
        "# 然后再把它们送入多源拼分函数\n",
        "dataset_train, dataset_test, dataset_validate = format_into_datasets_multi_source(\n",
        "    xs_list   = [\n",
        "        xs_qa_seg,   \n",
        "        # xs_sida_seg,   \n",
        "        xs_sch_seg,   \n",
        "        # xs_ma_seg\n",
        "        ],\n",
        "    ys_list   = [\n",
        "        ys_qa_seg,   \n",
        "        # ys_sida_seg,   \n",
        "        ys_sch_seg,   \n",
        "        # ys_ma_seg\n",
        "        ],\n",
        "    dataset_constructor = rnn_utils.DatasetRNN,\n",
        "    n_train_sessions   = 783,\n",
        "    n_test_sessions    = 98,\n",
        "    n_validate_sessions= 98,\n",
        "    batch_size=64,\n",
        "    random_seed=42,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "otkhuOQyx7hB"
      },
      "outputs": [],
      "source": [
        "def compute_log_likelihood(dataset, model_fun, params):\n",
        "    xs, actual_choices = next(dataset)\n",
        "    n_trials_per_session, n_sessions = actual_choices.shape[:2]\n",
        "    model_outputs, model_states = rnn_utils.eval_model(model_fun, params, xs)\n",
        "\n",
        "    # predicted log-probs for the first two actions\n",
        "    predicted_log_choice_probabilities = np.array(\n",
        "        jax.nn.log_softmax(model_outputs[:, :, :2], axis=-1)\n",
        "    )\n",
        "\n",
        "    n_actions = predicted_log_choice_probabilities.shape[2]\n",
        "    log_likelihoods = []\n",
        "\n",
        "    for sess_i in range(n_sessions):\n",
        "        log_likelihood = 0.0\n",
        "        n = 0\n",
        "        for trial_i in range(n_trials_per_session):\n",
        "            actual_choice = int(actual_choices[trial_i, sess_i])\n",
        "            # ignore invalid trials (<0 or ≥n_actions)\n",
        "            if 0 <= actual_choice < n_actions:\n",
        "                log_likelihood += predicted_log_choice_probabilities[\n",
        "                    trial_i, sess_i, actual_choice\n",
        "                ]\n",
        "                n += 1\n",
        "\n",
        "        if n > 0:\n",
        "            normalized_likelihood = np.exp(log_likelihood / n)\n",
        "            log_likelihoods.append(normalized_likelihood)\n",
        "\n",
        "    mean_likelihood = np.mean(log_likelihoods)\n",
        "    std_likelihood  = np.std(log_likelihoods)\n",
        "\n",
        "    print(f'Average Normalized Likelihood: {100 * mean_likelihood:.1f}%')\n",
        "    return mean_likelihood, std_likelihood\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCzihu0NJO6R",
        "outputId": "94f1db8b-0a3f-436a-d233-470f384a6c11"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 挂载 Google Drive\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# import pickle\n",
        "\n",
        "# # 挂载 Google Drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8sai1OhWYy3"
      },
      "source": [
        "# Fitting different models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmgO7HywNl5-"
      },
      "source": [
        "# Fit Vanilla RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZQyXrnBlvJaZ",
        "outputId": "527bb074-2a42-4519-bac2-c88a6f866cee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 200 of 200; Loss: 2.7035e+03; Test Loss: -0.0000e+00. (Time: 7.3s)\n",
            "first loss: inf test loss new: 0.0\n",
            "updating best model ..\n",
            "Step 200 of 100000; Loss: 0.0000e+00. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.1163e+03; Test Loss: -0.0000e+00. (Time: 5.5s)\n",
            "first loss: 0.0 test loss new: 0.0\n",
            "\n",
            "Stopping early as the loss at step 200 did not improve over step 1.\n"
          ]
        }
      ],
      "source": [
        "# 这个相当于是最好的认知模型，在rescolar里面加了一些参数\n",
        "rl_pc_params, _ = rnn_utils.fit_model(\n",
        "    model_fun=bandits.Hk_PreserveConAgentQ,\n",
        "    dataset_train = dataset_train,\n",
        "    dataset_test = dataset_validate,\n",
        "    loss_fun='categorical',\n",
        "    optimizer= optax.chain(\n",
        "        optax.add_decayed_weights(1e-5),  # L2 正则化\n",
        "        optax.adam(learning_rate=1e-4)  # Adam 优化器\n",
        "    ),\n",
        "    n_steps_per_call=200,\n",
        "    n_steps_max=100000,\n",
        "    #return_all_losses=True,\n",
        "    early_stop_step=200,\n",
        "    if_early_stop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJIf1uLXwYLM",
        "outputId": "613e2379-28cc-4a12-8402-5b82183e9405"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Normalized Likelihood: 48.5%\n"
          ]
        }
      ],
      "source": [
        "mean,std = compute_log_likelihood(dataset_test, bandits.Hk_PreserveConAgentQ, rl_pc_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKUz7-bxNvub"
      },
      "outputs": [],
      "source": [
        "#@title Set up the RNN (Vanilla RNN) Model\n",
        "n_hidden = 8\n",
        "def make_vanilla_rnn():\n",
        "    model = hk.DeepRNN(\n",
        "        [hk.VanillaRNN(n_hidden), hk.Linear(output_size=2)]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "#@title Set up the RNN (Vanilla RNN) Model\n",
        "n_hidden = 8\n",
        "def make_lstm():\n",
        "    model = hk.DeepRNN(\n",
        "        [hk.LSTM(n_hidden), hk.Linear(output_size=2)]\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "khylH5F1OKHK",
        "outputId": "4eaafe01-db93-4ae3-bdc1-06e810217d6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 200 of 200; Loss: 2.6275e+03; Test Loss: 2.5221e+03. (Time: 4.7s)updating best model ..\n",
            "Step 200 of 100000; Loss: 3.0262e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1956e+03; Test Loss: 2.5076e+03. (Time: 3.9s)updating best model ..\n",
            "Step 400 of 100000; Loss: 3.0114e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.0060e+03; Test Loss: 2.4942e+03. (Time: 4.3s)updating best model ..\n",
            "Step 600 of 100000; Loss: 2.9995e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.0375e+03; Test Loss: 2.4818e+03. (Time: 4.8s)updating best model ..\n",
            "Step 800 of 100000; Loss: 2.9901e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2896e+03; Test Loss: 2.4699e+03. (Time: 3.8s)updating best model ..\n",
            "Step 1000 of 100000; Loss: 2.9821e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.0236e+03; Test Loss: 2.4578e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1200 of 100000; Loss: 2.9748e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.1469e+03; Test Loss: 2.4449e+03. (Time: 4.8s)updating best model ..\n",
            "Step 1400 of 100000; Loss: 2.9673e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.9505e+03; Test Loss: 2.4300e+03. (Time: 3.8s)updating best model ..\n",
            "Step 1600 of 100000; Loss: 2.9588e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6482e+03; Test Loss: 2.4133e+03. (Time: 3.8s)updating best model ..\n",
            "Step 1800 of 100000; Loss: 2.9494e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6579e+03; Test Loss: 2.3955e+03. (Time: 4.6s)updating best model ..\n",
            "Step 2000 of 100000; Loss: 2.9394e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6355e+03; Test Loss: 2.3780e+03. (Time: 3.8s)updating best model ..\n",
            "Step 2200 of 100000; Loss: 2.9297e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9643e+03; Test Loss: 2.3620e+03. (Time: 3.6s)updating best model ..\n",
            "Step 2400 of 100000; Loss: 2.9206e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6343e+03; Test Loss: 2.3475e+03. (Time: 4.5s)updating best model ..\n",
            "Step 2600 of 100000; Loss: 2.9125e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2995e+03; Test Loss: 2.3353e+03. (Time: 3.6s)updating best model ..\n",
            "Step 2800 of 100000; Loss: 2.9054e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1132e+03; Test Loss: 2.3244e+03. (Time: 3.6s)updating best model ..\n",
            "Step 3000 of 100000; Loss: 2.8993e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.0844e+03; Test Loss: 2.3150e+03. (Time: 4.1s)updating best model ..\n",
            "Step 3200 of 100000; Loss: 2.8938e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.6416e+03; Test Loss: 2.3065e+03. (Time: 3.6s)updating best model ..\n",
            "Step 3400 of 100000; Loss: 2.8890e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3279e+03; Test Loss: 2.2987e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3600 of 100000; Loss: 2.8845e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.6887e+03; Test Loss: 2.2914e+03. (Time: 4.0s)updating best model ..\n",
            "Step 3800 of 100000; Loss: 2.8803e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.1835e+03; Test Loss: 2.2847e+03. (Time: 3.6s)updating best model ..\n",
            "Step 4000 of 100000; Loss: 2.8765e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.6189e+03; Test Loss: 2.2784e+03. (Time: 3.6s)updating best model ..\n",
            "Step 4200 of 100000; Loss: 2.8728e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6547e+03; Test Loss: 2.2721e+03. (Time: 4.0s)updating best model ..\n",
            "Step 4400 of 100000; Loss: 2.8693e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.4394e+03; Test Loss: 2.2666e+03. (Time: 3.6s)updating best model ..\n",
            "Step 4600 of 100000; Loss: 2.8660e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6320e+03; Test Loss: 2.2607e+03. (Time: 3.6s)updating best model ..\n",
            "Step 4800 of 100000; Loss: 2.8629e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8360e+03; Test Loss: 2.2559e+03. (Time: 3.8s)updating best model ..\n",
            "Step 5000 of 100000; Loss: 2.8599e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6120e+03; Test Loss: 2.2506e+03. (Time: 4.0s)updating best model ..\n",
            "Step 5200 of 100000; Loss: 2.8572e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2125e+03; Test Loss: 2.2464e+03. (Time: 3.7s)updating best model ..\n",
            "Step 5400 of 100000; Loss: 2.8546e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0758e+03; Test Loss: 2.2418e+03. (Time: 3.7s)updating best model ..\n",
            "Step 5600 of 100000; Loss: 2.8522e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.1448e+03; Test Loss: 2.2379e+03. (Time: 4.5s)updating best model ..\n",
            "Step 5800 of 100000; Loss: 2.8498e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4851e+03; Test Loss: 2.2340e+03. (Time: 3.6s)updating best model ..\n",
            "Step 6000 of 100000; Loss: 2.8477e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3665e+03; Test Loss: 2.2303e+03. (Time: 3.6s)updating best model ..\n",
            "Step 6200 of 100000; Loss: 2.8457e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5532e+03; Test Loss: 2.2268e+03. (Time: 4.5s)updating best model ..\n",
            "Step 6400 of 100000; Loss: 2.8438e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2168e+03; Test Loss: 2.2234e+03. (Time: 3.6s)updating best model ..\n",
            "Step 6600 of 100000; Loss: 2.8421e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5063e+03; Test Loss: 2.2204e+03. (Time: 3.7s)updating best model ..\n",
            "Step 6800 of 100000; Loss: 2.8404e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6546e+03; Test Loss: 2.2172e+03. (Time: 4.2s)updating best model ..\n",
            "Step 7000 of 100000; Loss: 2.8389e+03. (Time: 0.0s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _xla_gc_callback at 0x7d103f43cfe0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "    \n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 200 of 200; Loss: 3.3487e+03; Test Loss: 2.2146e+03. (Time: 4.0s)updating best model ..\n",
            "Step 7200 of 100000; Loss: 2.8375e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6303e+03; Test Loss: 2.2115e+03. (Time: 3.7s)updating best model ..\n",
            "Step 7400 of 100000; Loss: 2.8362e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7915e+03; Test Loss: 2.2091e+03. (Time: 4.1s)updating best model ..\n",
            "Step 7600 of 100000; Loss: 2.8350e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6021e+03; Test Loss: 2.2060e+03. (Time: 3.7s)updating best model ..\n",
            "Step 7800 of 100000; Loss: 2.8340e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1777e+03; Test Loss: 2.2033e+03. (Time: 3.7s)updating best model ..\n",
            "Step 8000 of 100000; Loss: 2.8327e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0539e+03; Test Loss: 2.1991e+03. (Time: 3.9s)updating best model ..\n",
            "Step 8200 of 100000; Loss: 2.8297e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.1424e+03; Test Loss: 2.1942e+03. (Time: 3.6s)updating best model ..\n",
            "Step 8400 of 100000; Loss: 2.8250e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4035e+03; Test Loss: 2.1899e+03. (Time: 3.6s)updating best model ..\n",
            "Step 8600 of 100000; Loss: 2.8194e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3696e+03; Test Loss: 2.1861e+03. (Time: 3.9s)updating best model ..\n",
            "Step 8800 of 100000; Loss: 2.8164e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4715e+03; Test Loss: 2.1830e+03. (Time: 3.8s)updating best model ..\n",
            "Step 9000 of 100000; Loss: 2.8092e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2051e+03; Test Loss: 2.1794e+03. (Time: 3.7s)updating best model ..\n",
            "Step 9200 of 100000; Loss: 2.8057e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4275e+03; Test Loss: 2.1765e+03. (Time: 3.7s)updating best model ..\n",
            "Step 9400 of 100000; Loss: 2.8032e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6944e+03; Test Loss: 2.1738e+03. (Time: 4.3s)updating best model ..\n",
            "Step 9600 of 100000; Loss: 2.8014e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2116e+03; Test Loss: 2.1717e+03. (Time: 3.6s)updating best model ..\n",
            "Step 9800 of 100000; Loss: 2.7998e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6479e+03; Test Loss: 2.1694e+03. (Time: 3.7s)updating best model ..\n",
            "Step 10000 of 100000; Loss: 2.7984e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6075e+03; Test Loss: 2.1676e+03. (Time: 4.5s)updating best model ..\n",
            "Step 10200 of 100000; Loss: 2.7970e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6094e+03; Test Loss: 2.1656e+03. (Time: 3.7s)updating best model ..\n",
            "Step 10400 of 100000; Loss: 2.7958e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9776e+03; Test Loss: 2.1641e+03. (Time: 3.7s)updating best model ..\n",
            "Step 10600 of 100000; Loss: 2.7945e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0370e+03; Test Loss: 2.1623e+03. (Time: 4.5s)updating best model ..\n",
            "Step 10800 of 100000; Loss: 2.7933e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2220e+03; Test Loss: 2.1603e+03. (Time: 4.1s)updating best model ..\n",
            "Step 11000 of 100000; Loss: 2.7919e+03. (Time: 0.0s)\n",
            "Step 30 of 200; Loss: 4.4276e+03; Test Loss: 2.1604e+03. (Time: 1.6s)"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3664668.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#n_steps_max = 1000000 #@param\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m gru_params, _, all_losses = rnn_utils.fit_model(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_fun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_lstm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CogModelingRNNsTutorial/rnn_utils.py\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model_fun, dataset_train, dataset_test, optimizer, loss_fun, random_key, n_steps_per_call, n_steps_max, return_all_losses, early_stop_step, if_early_stop)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m     params, opt_state, losses = train_model(\n\u001b[0m\u001b[1;32m    549\u001b[0m         \u001b[0mmodel_fun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CogModelingRNNsTutorial/rnn_utils.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_fun, dataset_train, dataset_test, optimizer, random_key, opt_state, params, n_steps, penalty_scale, loss_fun, do_plot, truncate_seq_length)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;31m# Log every 10th step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m       print((f'\\rStep {step + 1} of {n_steps}; '\n\u001b[1;32m    282\u001b[0m              \u001b[0;34mf'Loss: {loss:.4e}; Test Loss: {test_loss:.4e}. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/numpy/array_methods.py\u001b[0m in \u001b[0;36mdeferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mswap\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_accepted_binop_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m     \u001b[0;31m# Note: don't use isinstance here, because we don't want to raise for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;31m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title Fit the RNN (GRU) model\n",
        "#n_steps_max = 1000000 #@param\n",
        "\n",
        "gru_params, _, all_losses = rnn_utils.fit_model(\n",
        "    model_fun=make_lstm,\n",
        "    dataset_train = dataset_train,\n",
        "    dataset_test = dataset_validate,\n",
        "    loss_fun='categorical',\n",
        "    optimizer= optax.chain(\n",
        "        optax.add_decayed_weights(1e-5),  # L2 正则化\n",
        "        optax.adam(learning_rate=1e-4)  # Adam 优化器\n",
        "    ),\n",
        "    n_steps_per_call=200,\n",
        "    n_steps_max=100000,\n",
        "    return_all_losses=True,\n",
        "    early_stop_step=200,\n",
        "    if_early_stop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOpDYABeOaEM"
      },
      "outputs": [],
      "source": [
        "# #@title Compute log-likelihood\n",
        "# def compute_log_likelihood(dataset, model_fun, params):\n",
        "\n",
        "#   xs, actual_choices = next(dataset)\n",
        "#   n_trials_per_session, n_sessions = actual_choices.shape[:2]\n",
        "#   model_outputs, model_states = rnn_utils.eval_model(model_fun, params, xs)\n",
        "\n",
        "#   predicted_log_choice_probabilities = np.array(jax.nn.log_softmax(model_outputs[:, :, :2]))\n",
        "\n",
        "#   log_likelihood = 0\n",
        "#   n = 0  # Total number of trials across sessions.\n",
        "#   for sess_i in range(n_sessions):\n",
        "#     for trial_i in range(n_trials_per_session):\n",
        "#       actual_choice = int(actual_choices[trial_i, sess_i])\n",
        "#       if actual_choice >= 0:  # values < 0 are invalid trials which we ignore.\n",
        "#         log_likelihood += predicted_log_choice_probabilities[trial_i, sess_i, actual_choice]\n",
        "#         n += 1\n",
        "\n",
        "#   normalized_likelihood = np.exp(log_likelihood / n)\n",
        "\n",
        "#   print(f'Normalized Likelihood: {100 * normalized_likelihood:.1f}%')\n",
        "\n",
        "#   return normalized_likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ute8Yj_xOMd8",
        "outputId": "ca85be09-e42c-41a5-dd3e-13c9a6bc0d28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalized Likelihoods for GRU\n",
            "Training Dataset\n",
            "Average Normalized Likelihood: 61.3%\n",
            "Held-Out Dataset\n",
            "Average Normalized Likelihood: 62.0%\n"
          ]
        }
      ],
      "source": [
        "#@title Compute quality-of-fit: Held-out Normalized Likelihood\n",
        "# Compute log-likelihood\n",
        "print('Normalized Likelihoods for GRU')\n",
        "print('Training Dataset')\n",
        "training_likelihood = compute_log_likelihood(dataset_train, make_lstm, gru_params)\n",
        "print('Held-Out Dataset')\n",
        "testing_likelihood = compute_log_likelihood(dataset_validate, make_lstm, gru_params)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "i7x3Ff2xWTdb",
        "YOGiNjm4FUAx",
        "WmgO7HywNl5-",
        "GvGdnab8CdCN",
        "mrmkwDvr500Q",
        "9WygdkV_tSUA",
        "EYkrrFeXOqG2"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (CogModel)",
      "language": "python",
      "name": "cogmodel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
