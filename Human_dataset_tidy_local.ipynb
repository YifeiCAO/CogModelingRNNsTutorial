{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7x3Ff2xWTdb"
      },
      "source": [
        "# Import and preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mYuMvSuvyGuR"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "import gdown\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CPp79Z477SU8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotnine as gg\n",
        "gg.theme_set(gg.theme_classic)  # for nicer-looking plots\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "import optax\n",
        "import scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CudaDevice(id=0)]\n"
          ]
        }
      ],
      "source": [
        "devices = jax.devices()\n",
        "print(devices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWQY_uJK7Uyx",
        "outputId": "983c0b42-ee55-48ed-e507-ccba22a8289f"
      },
      "outputs": [],
      "source": [
        "# !pip install -U dm-haiku\n",
        "import haiku as hk\n",
        "rng_seq = hk.PRNGSequence(np.random.randint(2**32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX2twY1NNT1r",
        "outputId": "c73fd2f4-31d7-470b-caf5-2d9dc712511d"
      },
      "outputs": [],
      "source": [
        "# #@title Install required packages.\n",
        "# try:\n",
        "#     from google.colab import files\n",
        "#     _ON_COLAB = True\n",
        "# except:\n",
        "#     _ON_COLAB = False\n",
        "\n",
        "# if _ON_COLAB:\n",
        "#   !rm -rf CogModelingRNNsTutorial\n",
        "#   !git clone https://github.com/YifeiCAO/CogModelingRNNsTutorial\n",
        "#   !pip install -e CogModelingRNNsTutorial/CogModelingRNNsTutorial\n",
        "#   !cp CogModelingRNNsTutorial/CogModelingRNNsTutorial/*py CogModelingRNNsTutorial\n",
        "# else:\n",
        "#   !pip install CogModelingRNNsTutorial/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_2jivtclNVjS"
      },
      "outputs": [],
      "source": [
        "#@title Imports + defaults settings.\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "# for reload\n",
        "# %reload_ext autoreload\n",
        "\n",
        "# import haiku as hk\n",
        "# import jax\n",
        "# import jax.numpy as jnp\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optax\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# try:\n",
        "#     from google.colab import files\n",
        "#     _ON_COLAB = True\n",
        "# except:\n",
        "#     _ON_COLAB = False\n",
        "\n",
        "from CogModelingRNNsTutorial import bandits\n",
        "from CogModelingRNNsTutorial import disrnn\n",
        "from CogModelingRNNsTutorial import hybrnn\n",
        "from CogModelingRNNsTutorial import hybconrnn\n",
        "from CogModelingRNNsTutorial import hybrnn_direct_con\n",
        "from CogModelingRNNsTutorial import plotting\n",
        "from CogModelingRNNsTutorial import rat_data\n",
        "from CogModelingRNNsTutorial import rnn_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyjWR7YmyPQB",
        "outputId": "0101e98d-62cc-42d5-a6d9-80fd8c89a044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File downloaded and read successfully!\n",
            "xs.shape = (60, 206, 2)\n",
            "ys.shape = (60, 206, 1)\n"
          ]
        }
      ],
      "source": [
        "osf_url = 'https://osf.io/xe6yu/download?direct=1'\n",
        "response = requests.get(osf_url)\n",
        "\n",
        "# Check the request\n",
        "if response.status_code == 200:\n",
        "    # Read as pandas dataframe\n",
        "    qasim_data = pd.read_csv(StringIO(response.text))\n",
        "    print('File downloaded and read successfully!')\n",
        "else:\n",
        "    print('Failed to download file. Status code:', response.status_code)\n",
        "\n",
        "qasim_data.head()\n",
        "\n",
        "# # read data\n",
        "selected_columns = ['participant', 'trials_gamble', 'gamble', 'prob', 'reward']\n",
        "\n",
        "qasim = qasim_data[selected_columns]\n",
        "qasim_filtered = qasim[qasim['trials_gamble'].notna()]\n",
        "qasim_sorted = qasim_filtered.groupby('participant', group_keys=False).apply(lambda x: x.sort_values('trials_gamble'))\n",
        "qasim_sorted = qasim_sorted.reset_index(drop=True)\n",
        "qasim_sorted['participant'] = qasim_sorted.groupby(['participant']).ngroup() + 1\n",
        "qasim_sorted['action'] = qasim_sorted['gamble']\n",
        "qasim_sorted\n",
        "\n",
        "# —— 2) 把缺失的 action 填成 -1 —— #\n",
        "qasim_sorted['action'] = qasim_sorted['action'].fillna(-1).astype(int)\n",
        "\n",
        "# —— 3) 如果需要，把 reward 映射到 0/1 —— #\n",
        "# （如果已经是 0/1 可跳过；否则取消下面注释并调整映射字典）\n",
        "# qasim_sorted['reward'] = (\n",
        "#     qasim_sorted['reward']\n",
        "#     .map({-1: 0, 1: 1})\n",
        "#     .fillna(-1)\n",
        "#     .astype(int)\n",
        "# )\n",
        "\n",
        "# —— 4) 排序，确保 trial 顺序 —— #\n",
        "qasim_sorted = qasim_sorted.sort_values(\n",
        "    ['participant', 'trials_gamble']\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# —— 5) 生成下一步动作 action_n —— #\n",
        "qasim_sorted['action_n'] = (\n",
        "    qasim_sorted\n",
        "    .groupby('participant')['action']\n",
        "    .shift(-1)\n",
        ")\n",
        "# 每个 participant 最后一 trial 的 action_n 设为 -1\n",
        "last_idxs = qasim_sorted.groupby('participant').tail(1).index\n",
        "qasim_sorted.loc[last_idxs, 'action_n'] = -1\n",
        "\n",
        "# —— 6) 按 participant 构造 xs_list, ys_list —— #\n",
        "xs_list, ys_list = [], []\n",
        "for pid, grp in qasim_sorted.groupby('participant'):\n",
        "    grp = grp.sort_values('trials_gamble')\n",
        "    x = grp[['prob', 'reward']].to_numpy().astype(float)    # 输入特征\n",
        "    y = grp[['action_n']].to_numpy().astype(int)             # 下一步动作\n",
        "    xs_list.append(x)\n",
        "    ys_list.append(y)\n",
        "\n",
        "# —— 7) 堆成三维数组 —— #\n",
        "xs_qa = np.stack(xs_list, axis=1)  # (n_sessions, n_trials, 2)\n",
        "ys_qa = np.stack(ys_list, axis=1)  # (n_sessions, n_trials, 1)\n",
        "\n",
        "print(\"xs.shape =\", xs_qa.shape)\n",
        "print(\"ys.shape =\", ys_qa.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNCQ2haXljHL"
      },
      "source": [
        "### Sidarus dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zetxo6c_lgFd",
        "outputId": "f8f0f5e1-0f46-452d-ce7e-87a314963918"
      },
      "outputs": [],
      "source": [
        "# 修改后的 file_id\n",
        "file_id = '1TSV6CdyClKz831qD2ln4z3WjLLcOgG1n'\n",
        "download_url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "\n",
        "# 下载并保存为 'downloaded_file.csv'\n",
        "output_file = 'sidarus_data.csv'\n",
        "gdown.download(download_url, output_file, quiet=False, fuzzy=True)\n",
        "\n",
        "# 读取 CSV 数据\n",
        "sida_data = pd.read_csv(output_file)\n",
        "sida_data\n",
        "\n",
        "# 1) action 从 {1,2} → {0,1}，并把所有缺失值填成 -1\n",
        "sida_data['action'] = (\n",
        "    sida_data['action']\n",
        "    .map({1: 0, 2: 1})    # 映射\n",
        "    .fillna(-1)           # 缺失值设为 -1\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "# 2) outcome 从 {-1,1} → {0,1}，缺失也填成 -1\n",
        "sida_data['reward'] = (\n",
        "    sida_data['outcome']\n",
        "    .map({-1: 0, 1: 1})\n",
        "    .fillna(-1)\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "# 3) 给每个被试×session 分配一个连续的 session_id\n",
        "sida_data['session_id'] = (\n",
        "    sida_data\n",
        "    .groupby(['subj', 'session'], sort=False)\n",
        "    .ngroup()\n",
        "    + 1\n",
        ")\n",
        "\n",
        "# 4) 排序，保证 trial 顺序\n",
        "sida_data = sida_data.sort_values(\n",
        "    ['session_id', 'epN', 'epTrialN']\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# 5) 生成下一步动作 action_n；每个 session 末尾设为 -1\n",
        "sida_data['action_n'] = (\n",
        "    sida_data\n",
        "    .groupby('session_id')['action']\n",
        "    .shift(-1)\n",
        ")\n",
        "last_idx = sida_data.groupby('session_id').tail(1).index\n",
        "sida_data.loc[last_idx, 'action_n'] = -1\n",
        "\n",
        "# 6) 按 session_id 抽取序列，并堆成 xs, ys\n",
        "xs_list, ys_list = [], []\n",
        "for sid, grp in sida_data.groupby('session_id'):\n",
        "    grp = grp.sort_values(['epN', 'epTrialN'])\n",
        "    x = grp[['hiRewAct', 'reward']].to_numpy().astype(float)  # 特征\n",
        "    y = grp[['action_n']].to_numpy().astype(int)              # Label\n",
        "    xs_list.append(x)\n",
        "    ys_list.append(y)\n",
        "\n",
        "# 最终结果：xs.shape == (n_sessions, n_trials, 2)，ys.shape == (n_sessions, n_trials, 1)\n",
        "xs_sida = np.stack(xs_list, axis=1)\n",
        "ys_sida = np.stack(ys_list, axis=1)\n",
        "\n",
        "print(\"xs.shape =\", xs_sida.shape)\n",
        "print(\"ys.shape =\", ys_sida.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "147k59EcluMe"
      },
      "source": [
        "### Schaaf dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRUXDOuEltlO",
        "outputId": "4d607a65-5003-4439-f314-daa193744c6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rJFmDhCE3fdXtSHSSvtre_7V49gGsg7P\n",
            "To: /home/liyuan/CogModelingRNNsTutorial/downloaded_file.csv\n",
            "100%|██████████| 374k/374k [00:00<00:00, 7.21MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "xs.shape = (249, 94, 2)\n",
            "ys.shape = (249, 94, 1)\n"
          ]
        }
      ],
      "source": [
        "# 修改后的 file_id\n",
        "file_id = '1rJFmDhCE3fdXtSHSSvtre_7V49gGsg7P'\n",
        "download_url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "\n",
        "# 下载并保存为 'downloaded_file.csv'\n",
        "output_file = 'schaaf_data.csv'\n",
        "gdown.download(download_url, output_file, quiet=False, fuzzy=True)\n",
        "\n",
        "# 读取 CSV 数据\n",
        "schaaf_data = pd.read_csv(output_file)\n",
        "schaaf_data\n",
        "\n",
        "df1 = schaaf_data[['pp', 'trial1', 'response1', 'outcome1']].copy()\n",
        "df1.columns = ['pp', 'trial_in_session', 'response', 'reward']\n",
        "df1['session'] = 1\n",
        "\n",
        "df2 = schaaf_data[['pp', 'trial2', 'response2', 'outcome2']].copy()\n",
        "df2.columns = ['pp', 'trial_in_session', 'response', 'reward']\n",
        "df2['session'] = 2\n",
        "\n",
        "# —— 上面拆分、concat、sort 的部分保持不变 —— #\n",
        "\n",
        "# 合并成长表\n",
        "df_long = pd.concat([df1, df2], ignore_index=True)\n",
        "df_long = df_long.sort_values(['pp', 'session', 'trial_in_session']).reset_index(drop=True)\n",
        "\n",
        "# —— 映射 outcome，再填 missing response —— #\n",
        "# 把原来的 response1/response2 改名后是 `response`\n",
        "# 把 outcome1/outcome2 改名后是 `reward`\n",
        "# 把 outcome 映射成 1/0，然后把原本缺失的 reward 补成 -1\n",
        "df_long['reward'] = (\n",
        "    df_long['reward']\n",
        "    .map({1: 1, -1: 0})\n",
        "    .fillna(-1)\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "# 把缺失的 response 一样补成 -1\n",
        "df_long['response'] = (\n",
        "    df_long['response']\n",
        "    .fillna(-1)\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "\n",
        "# 接下来再做 session_id、shift action_n、以及 stack xs/ys 的流程……\n",
        "df_long['session_id'] = (df_long['pp'] - 1) * 2 + df_long['session']\n",
        "df_long['action_n'] = df_long.groupby('session_id')['response'].shift(-1)\n",
        "last_idx = df_long.groupby('session_id').tail(1).index\n",
        "df_long.loc[last_idx, 'action_n'] = -1\n",
        "\n",
        "# 生成 xs, ys\n",
        "session_ids = df_long['session_id'].unique()\n",
        "xs_list, ys_list = [], []\n",
        "for sid in session_ids:\n",
        "    sd = df_long[df_long['session_id'] == sid].sort_values('trial_in_session')\n",
        "    x = sd[['response', 'reward']].to_numpy().astype(float)\n",
        "    y = sd[['action_n']].to_numpy().astype(int)\n",
        "    xs_list.append(x)\n",
        "    ys_list.append(y)\n",
        "\n",
        "xs = np.stack(xs_list, axis=0)   # (n_sessions, n_trials, 2)\n",
        "ys = np.stack(ys_list, axis=0)   # (n_sessions, n_trials, 1)\n",
        "\n",
        "\n",
        "# 或者第 0 维是 trials，第 1 维是 sessions：\n",
        "xs_sch = np.stack(xs_list, axis=1)  # (n_trials, n_sessions, 2)\n",
        "ys_sch = np.stack(ys_list, axis=1)  # (n_trials, n_sessions, 1)\n",
        "\n",
        "print(\"xs.shape =\", xs_sch.shape)\n",
        "print(\"ys.shape =\", ys_sch.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8KQplq3tHC7"
      },
      "source": [
        "### Maria Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDBAI_AwD1PF",
        "outputId": "a54a9388-a125-4948-b27e-e97bc0ba8ab5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1N_zAy-qrbfjvF8Kbb504IH2JNhR5KI-P\n",
            "To: /home/liyuan/CogModelingRNNsTutorial/downloaded_file.csv\n",
            "100%|██████████| 2.57M/2.57M [00:00<00:00, 23.5MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "xs_ma.shape = (120, 305, 2)\n",
            "ys_ma.shape = (120, 305, 1)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gdown\n",
        "\n",
        "# —— 1) 下载并读入原始数据 —— #\n",
        "file_id = '1N_zAy-qrbfjvF8Kbb504IH2JNhR5KI-P'\n",
        "url     = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "gdown.download(url, 'maria_data.csv', quiet=False, fuzzy=True)\n",
        "eck_data = pd.read_csv('maria_data.csv')\n",
        "\n",
        "# —— 2) 筛选＆重命名列 —— #\n",
        "sel = ['sID','TrialID','selected_box','reward']\n",
        "eck_sorted = eck_data[sel].copy()\n",
        "eck_sorted['participant'] = eck_sorted.groupby('sID').ngroup()+1\n",
        "eck_sorted['action']      = eck_sorted['selected_box']\n",
        "\n",
        "# 只保留至少做够 120 试次的被试\n",
        "max_trial = eck_sorted.groupby('participant')['TrialID'].transform('max')\n",
        "eck_sorted = eck_sorted[max_trial>=120]\n",
        "\n",
        "# 只用前 120 试次\n",
        "eck_sorted = eck_sorted[eck_sorted['TrialID']<=120].reset_index(drop=True)\n",
        "\n",
        "# —— 3) 生成下一步动作 action_n —— #\n",
        "def generate_action_n(group):\n",
        "    group = group.sort_values('TrialID')\n",
        "    group['action_n'] = group['action'].shift(-1).fillna(-1).astype(int)\n",
        "    return group\n",
        "\n",
        "eck_sorted = (\n",
        "    eck_sorted\n",
        "    .groupby('participant', group_keys=False)\n",
        "    .apply(generate_action_n)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# —— 4) 按 participant 构造 xs_list/ys_list —— #\n",
        "xs_list, ys_list = [], []\n",
        "for pid, grp in eck_sorted.groupby('participant'):\n",
        "    grp = grp.sort_values('TrialID').iloc[:120]\n",
        "    x = grp[['action','reward']].to_numpy().astype(float)  # (120,2)\n",
        "    y = grp[['action_n']].to_numpy().astype(int)           # (120,1)\n",
        "    xs_list.append(x)\n",
        "    ys_list.append(y)\n",
        "\n",
        "# —— 5) stack 成 (n_sessions, n_trials, feat_dim) —— #\n",
        "xs_ma = np.stack(xs_list, axis=1)  # (305,120,2)\n",
        "ys_ma = np.stack(ys_list, axis=1)  # (305,120,1)\n",
        "\n",
        "print(\"xs_ma.shape =\", xs_ma.shape)\n",
        "print(\"ys_ma.shape =\", ys_ma.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbm2178XrKVd"
      },
      "source": [
        "### Generate a big human dataset with all experiments, session length is 130 trial per session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZ9SycOgtbsK",
        "outputId": "9650c98e-c381-4581-eda2-0f4849543525"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total segments: 979\n",
            "First xs segment shape: (130, 2)\n",
            "First ys segment shape: (130, 1)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def segment_and_pad(x: np.ndarray,\n",
        "                    y: np.ndarray,\n",
        "                    seg_len: int = 130,\n",
        "                    pad_x: float = 0.,\n",
        "                    pad_y: int = -1):\n",
        "    T, D = x.shape\n",
        "    n_segs = int(np.ceil(T / seg_len))\n",
        "    x_segs, y_segs = [], []\n",
        "    for i in range(n_segs):\n",
        "        start = i * seg_len\n",
        "        end = start + seg_len\n",
        "        x_part = x[start : min(end, T)]\n",
        "        y_part = y[start : min(end, T)]\n",
        "        pad = end - min(end, T)\n",
        "        if pad > 0:\n",
        "            x_part = np.pad(x_part,\n",
        "                            pad_width=((0, pad), (0, 0)),\n",
        "                            constant_values=pad_x)\n",
        "            y_part = np.pad(y_part,\n",
        "                            pad_width=((0, pad), (0, 0)),\n",
        "                            constant_values=pad_y)\n",
        "        x_segs.append(x_part)\n",
        "        y_segs.append(y_part)\n",
        "    return x_segs, y_segs\n",
        "\n",
        "# —— 假设你已经有这几组 (xs, ys) —— #\n",
        "# xs_qa   (60,  206, 2), ys_qa  (60,206, 1)\n",
        "# xs_sida (800, 40,  2), ys_sida(800,40, 1)\n",
        "# xs_sch  (249, 44,  2), ys_sch (249,44, 1)\n",
        "# xs_ma   (120,305, 2), ys_ma  (120,305,1)\n",
        "\n",
        "all_xs, all_ys = [], []\n",
        "\n",
        "for xs, ys in [(xs_qa, ys_qa),\n",
        "               (xs_sida, ys_sida),\n",
        "               (xs_sch, ys_sch),\n",
        "               (xs_ma, ys_ma)\n",
        "               ]:\n",
        "\n",
        "    # 如果是 (n_trials, n_sessions, feat) 维度，就直接：\n",
        "    # T, N, D = xs.shape\n",
        "    # 否则若是 (n_sessions, n_trials, feat)，先转：\n",
        "    # xs = xs.transpose(1,0,2)\n",
        "    # ys = ys.transpose(1,0,2)\n",
        "\n",
        "    T, N, D = xs.shape\n",
        "    for sess in range(N):\n",
        "        x_seq = xs[:, sess, :]  # (T, D)\n",
        "        y_seq = ys[:, sess, :]  # (T, 1)\n",
        "        x_segs, y_segs = segment_and_pad(\n",
        "            x_seq, y_seq,\n",
        "            seg_len=130,\n",
        "            pad_x=0., pad_y=-1\n",
        "        )\n",
        "        all_xs.extend(x_segs)\n",
        "        all_ys.extend(y_segs)\n",
        "\n",
        "# —— 修改在这里：不要 np.stack，直接输出列表 —— #\n",
        "# all_xs 是一个 Python list，长度 = 总片段数，每个元素 shape=(130, D)\n",
        "# all_ys 是一个 Python list，长度 = 总片段数，每个元素 shape=(130, 1)\n",
        "\n",
        "print(f\"Total segments: {len(all_xs)}\")\n",
        "print(f\"First xs segment shape: {all_xs[0].shape}\")\n",
        "print(f\"First ys segment shape: {all_ys[0].shape}\")\n",
        "\n",
        "# 如果你需要把它们返回成变量：\n",
        "xs_segment_list = all_xs\n",
        "ys_segment_list = all_ys\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-JHKuQv48Ur"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def format_into_datasets_multi_source(\n",
        "    xs_list: list[np.ndarray],\n",
        "    ys_list: list[np.ndarray],\n",
        "    dataset_constructor,\n",
        "    n_train_sessions: int,\n",
        "    n_test_sessions: int,\n",
        "    n_validate_sessions: int,\n",
        "    batch_size: int = None,\n",
        "    random_seed: int = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    按照 QA、SIDA、SCH、MA 这 4 个来源的数据源比例，\n",
        "    在它们各自内部抽取 train/test/val session，\n",
        "    最后拼成全局的 DatasetRNN。\n",
        "\n",
        "    xs_list, ys_list:\n",
        "      长度 4 的 list，每个元素形状是 (timesteps, n_sessions_i, feat)\n",
        "    n_*_sessions:\n",
        "      全局希望 train/test/val 一共要多少 session\n",
        "    \"\"\"\n",
        "    if random_seed is not None:\n",
        "        rng = np.random.RandomState(random_seed)\n",
        "    else:\n",
        "        rng = np.random\n",
        "\n",
        "    # 1) 计算每个来源各有多少 session\n",
        "    sess_counts = np.array([xs.shape[1] for xs in xs_list])  # e.g. [206, 40, 44, 305]\n",
        "    total_sessions = sess_counts.sum()\n",
        "\n",
        "    # 2) 按比例分配到每个来源的 train/test/val 数目\n",
        "    def proportional_alloc(total, counts):\n",
        "        floats = counts / counts.sum() * total\n",
        "        floors = np.floor(floats).astype(int)\n",
        "        rem = total - floors.sum()\n",
        "        # 剩余的按余数最大的那些来源补齐\n",
        "        remainders = floats - floors\n",
        "        for idx in np.argsort(remainders)[-rem:]:\n",
        "            floors[idx] += 1\n",
        "        return floors\n",
        "\n",
        "    n_train_per = proportional_alloc(n_train_sessions,    sess_counts)\n",
        "    n_test_per  = proportional_alloc(n_test_sessions,     sess_counts)\n",
        "    n_val_per   = proportional_alloc(n_validate_sessions, sess_counts)\n",
        "\n",
        "    # 3) 在每个来源内部随机打乱并切分\n",
        "    train_idx_list, test_idx_list, val_idx_list = [], [], []\n",
        "    for cnt, n_tr, n_te, n_va in zip(sess_counts,\n",
        "                                     n_train_per,\n",
        "                                     n_test_per,\n",
        "                                     n_val_per):\n",
        "        all_idx = np.arange(cnt)\n",
        "        rng.shuffle(all_idx)\n",
        "        train_idx_list.append(all_idx[:n_tr])\n",
        "        test_idx_list.append( all_idx[n_tr:n_tr+n_te] )\n",
        "        val_idx_list.append(  all_idx[n_tr+n_te:n_tr+n_te+n_va] )\n",
        "\n",
        "    # 4) 汇总抽到的 sessions：concat 出全局 xs/ys\n",
        "    def gather(xs_list, ys_list, idx_lists):\n",
        "        parts_x, parts_y = [], []\n",
        "        for xs, ys, idx in zip(xs_list, ys_list, idx_lists):\n",
        "            # xs: (timesteps, n_sessions_i, feat)\n",
        "            parts_x.append(xs[:, idx, :])\n",
        "            parts_y.append(ys[:, idx, :])\n",
        "        return np.concatenate(parts_x, axis=1), np.concatenate(parts_y, axis=1)\n",
        "\n",
        "    xs_train, ys_train = gather(xs_list, ys_list, train_idx_list)\n",
        "    xs_test,  ys_test  = gather(xs_list, ys_list, test_idx_list)\n",
        "    xs_val,   ys_val   = gather(xs_list, ys_list, val_idx_list)\n",
        "\n",
        "    # 5) 构造 DatasetRNN\n",
        "    ds_train = dataset_constructor(xs_train, ys_train, batch_size=batch_size)\n",
        "    ds_test  = dataset_constructor(xs_test,  ys_test,  batch_size=batch_size)\n",
        "    ds_val   = dataset_constructor(xs_val,   ys_val,   batch_size=batch_size)\n",
        "\n",
        "    return ds_train, ds_test, ds_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnupWYiP5vW6"
      },
      "outputs": [],
      "source": [
        "# 假设你已经有原始的：\n",
        "#   xs_qa   (T_qa,   N_qa,   D),   ys_qa   (T_qa,   N_qa,   1)\n",
        "#   xs_sida (T_sida, N_sida, D),   ys_sida (T_sida, N_sida, 1)\n",
        "#   xs_sch  (T_sch,  N_sch,  D),   ys_sch  (T_sch,  N_sch,  1)\n",
        "#   xs_ma   (T_ma,   N_ma,   D),   ys_ma   (T_ma,   N_ma,   1)\n",
        "\n",
        "def make_segmented_array(xs, ys, seg_len=130, pad_x=0., pad_y=-1):\n",
        "    all_xs, all_ys = [], []\n",
        "    T, N, D = xs.shape\n",
        "    for sess in range(N):\n",
        "        x_seq = xs[:, sess, :]    # (T, D)\n",
        "        y_seq = ys[:, sess, :]    # (T, 1)\n",
        "        x_segs, y_segs = segment_and_pad(x_seq, y_seq, seg_len, pad_x, pad_y)\n",
        "        all_xs.extend(x_segs)     # list of (130, D)\n",
        "        all_ys.extend(y_segs)     # list of (130, 1)\n",
        "    # 把 list 再拼成一个三维 array (130, n_segments, D)\n",
        "    xs_seg = np.stack(all_xs, axis=1)\n",
        "    ys_seg = np.stack(all_ys, axis=1)\n",
        "    return xs_seg, ys_seg\n",
        "\n",
        "# 针对四个源分别做一次\n",
        "xs_qa_seg,   ys_qa_seg   = make_segmented_array(xs_qa,   ys_qa)\n",
        "xs_sida_seg,ys_sida_seg = make_segmented_array(xs_sida, ys_sida)\n",
        "xs_sch_seg, ys_sch_seg  = make_segmented_array(xs_sch,  ys_sch)\n",
        "xs_ma_seg,  ys_ma_seg   = make_segmented_array(xs_ma,   ys_ma)\n",
        "\n",
        "# 然后再把它们送入多源拼分函数\n",
        "dataset_train, dataset_test, dataset_validate = format_into_datasets_multi_source(\n",
        "    xs_list   = [\n",
        "        xs_qa_seg,   \n",
        "        xs_sida_seg,   \n",
        "        xs_sch_seg,   \n",
        "        xs_ma_seg\n",
        "        ],\n",
        "    ys_list   = [\n",
        "        ys_qa_seg,   \n",
        "        ys_sida_seg,   \n",
        "        ys_sch_seg,   \n",
        "        ys_ma_seg\n",
        "        ],\n",
        "    dataset_constructor = rnn_utils.DatasetRNN,\n",
        "    n_train_sessions   = 783,\n",
        "    n_test_sessions    = 98,\n",
        "    n_validate_sessions= 98,\n",
        "    batch_size=64,\n",
        "    random_seed=42,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otkhuOQyx7hB"
      },
      "outputs": [],
      "source": [
        "def compute_log_likelihood(dataset, model_fun, params):\n",
        "    xs, actual_choices = next(dataset)\n",
        "    n_trials_per_session, n_sessions = actual_choices.shape[:2]\n",
        "    model_outputs, model_states = rnn_utils.eval_model(model_fun, params, xs)\n",
        "\n",
        "    # predicted log-probs for the first two actions\n",
        "    predicted_log_choice_probabilities = np.array(\n",
        "        jax.nn.log_softmax(model_outputs[:, :, :2], axis=-1)\n",
        "    )\n",
        "\n",
        "    n_actions = predicted_log_choice_probabilities.shape[2]\n",
        "    log_likelihoods = []\n",
        "\n",
        "    for sess_i in range(n_sessions):\n",
        "        log_likelihood = 0.0\n",
        "        n = 0\n",
        "        for trial_i in range(n_trials_per_session):\n",
        "            actual_choice = int(actual_choices[trial_i, sess_i])\n",
        "            # ignore invalid trials (<0 or ≥n_actions)\n",
        "            if 0 <= actual_choice < n_actions:\n",
        "                log_likelihood += predicted_log_choice_probabilities[\n",
        "                    trial_i, sess_i, actual_choice\n",
        "                ]\n",
        "                n += 1\n",
        "\n",
        "        if n > 0:\n",
        "            normalized_likelihood = np.exp(log_likelihood / n)\n",
        "            log_likelihoods.append(normalized_likelihood)\n",
        "\n",
        "    mean_likelihood = np.mean(log_likelihoods)\n",
        "    std_likelihood  = np.std(log_likelihoods)\n",
        "\n",
        "    print(f'Average Normalized Likelihood: {100 * mean_likelihood:.1f}%')\n",
        "    return mean_likelihood, std_likelihood\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCzihu0NJO6R",
        "outputId": "94f1db8b-0a3f-436a-d233-470f384a6c11"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 挂载 Google Drive\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# import pickle\n",
        "\n",
        "# # 挂载 Google Drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8sai1OhWYy3"
      },
      "source": [
        "# Fitting different models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmgO7HywNl5-"
      },
      "source": [
        "# Fit Vanilla RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZQyXrnBlvJaZ",
        "outputId": "527bb074-2a42-4519-bac2-c88a6f866cee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/nThe mean of testing losses in one call: nan\n",
            "Step 200 of 200; Loss: 5.2712e+03; Test Loss: 4.2356e+03. (Time: 7.0s)/nThe mean of testing losses in one call: 3112.6062048339845\n",
            "\n",
            "first loss: inf test loss new: 3113.8359326171876\n",
            "updating best model ..\n",
            "Step 200 of 100000; Loss: 3.1138e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9892e+03; Test Loss: 4.2306e+03. (Time: 8.8s)/nThe mean of testing losses in one call: 3109.3778729248047\n",
            "\n",
            "first loss: 3113.8359326171876 test loss new: 3110.5805810546876\n",
            "updating best model ..\n",
            "Step 400 of 100000; Loss: 3.1106e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5653e+03; Test Loss: 4.2258e+03. (Time: 7.4s)/nThe mean of testing losses in one call: 3106.2028973388674\n",
            "\n",
            "first loss: 3110.5805810546876 test loss new: 3107.375754394531\n",
            "updating best model ..\n",
            "Step 600 of 100000; Loss: 3.1074e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9406e+03; Test Loss: 4.2210e+03. (Time: 7.0s)/nThe mean of testing losses in one call: 3103.116608276367\n",
            "\n",
            "first loss: 3107.375754394531 test loss new: 3104.258229980469\n",
            "updating best model ..\n",
            "Step 800 of 100000; Loss: 3.1043e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3409e+03; Test Loss: 4.2163e+03. (Time: 9.7s)/nThe mean of testing losses in one call: 3100.116303100586\n",
            "\n",
            "first loss: 3104.258229980469 test loss new: 3101.226467285156\n",
            "updating best model ..\n",
            "Step 1000 of 100000; Loss: 3.1012e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.0035e+03; Test Loss: 4.2117e+03. (Time: 9.8s)/nThe mean of testing losses in one call: 3097.190202636719\n",
            "\n",
            "first loss: 3101.226467285156 test loss new: 3098.273347167969\n",
            "updating best model ..\n",
            "Step 1200 of 100000; Loss: 3.0983e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.3736e+03; Test Loss: 4.2071e+03. (Time: 7.6s)/nThe mean of testing losses in one call: 3094.3406896972656\n",
            "\n",
            "first loss: 3098.273347167969 test loss new: 3095.3940576171876\n",
            "updating best model ..\n",
            "Step 1400 of 100000; Loss: 3.0954e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.3172e+03; Test Loss: 4.2026e+03. (Time: 7.3s)/nThe mean of testing losses in one call: 3091.5650103759767\n",
            "\n",
            "first loss: 3095.3940576171876 test loss new: 3092.5923974609377\n",
            "updating best model ..\n",
            "Step 1600 of 100000; Loss: 3.0926e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6384e+03; Test Loss: 4.1982e+03. (Time: 9.4s)/nThe mean of testing losses in one call: 3088.863791503906\n",
            "\n",
            "first loss: 3092.5923974609377 test loss new: 3089.8630297851564\n",
            "updating best model ..\n",
            "Step 1800 of 100000; Loss: 3.0899e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6973e+03; Test Loss: 4.1939e+03. (Time: 8.2s)/nThe mean of testing losses in one call: 3086.245689697266\n",
            "\n",
            "first loss: 3089.8630297851564 test loss new: 3087.2191186523437\n",
            "updating best model ..\n",
            "Step 2000 of 100000; Loss: 3.0872e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6397e+03; Test Loss: 4.1896e+03. (Time: 8.5s)/nThe mean of testing losses in one call: 3083.6882946777346\n",
            "\n",
            "first loss: 3087.2191186523437 test loss new: 3084.636623535156\n",
            "updating best model ..\n",
            "Step 2200 of 100000; Loss: 3.0846e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.9340e+03; Test Loss: 4.1855e+03. (Time: 6.3s)/nThe mean of testing losses in one call: 3081.210499267578\n",
            "\n",
            "first loss: 3084.636623535156 test loss new: 3082.134033203125\n",
            "updating best model ..\n",
            "Step 2400 of 100000; Loss: 3.0821e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3292e+02; Test Loss: 4.1814e+03. (Time: 8.9s)/nThe mean of testing losses in one call: 3078.798260498047\n",
            "\n",
            "first loss: 3082.134033203125 test loss new: 3079.697658691406\n",
            "updating best model ..\n",
            "Step 2600 of 100000; Loss: 3.0797e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2732e+03; Test Loss: 4.1774e+03. (Time: 7.9s)/nThe mean of testing losses in one call: 3076.445746459961\n",
            "\n",
            "first loss: 3079.697658691406 test loss new: 3077.3229907226564\n",
            "updating best model ..\n",
            "Step 2800 of 100000; Loss: 3.0773e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9423e+03; Test Loss: 4.1734e+03. (Time: 7.6s)/nThe mean of testing losses in one call: 3074.14978515625\n",
            "\n",
            "first loss: 3077.3229907226564 test loss new: 3075.005029296875\n",
            "updating best model ..\n",
            "Step 3000 of 100000; Loss: 3.0750e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5719e+03; Test Loss: 4.1695e+03. (Time: 6.5s)/nThe mean of testing losses in one call: 3071.8877239990234\n",
            "\n",
            "first loss: 3075.005029296875 test loss new: 3072.7234692382813\n",
            "updating best model ..\n",
            "Step 3200 of 100000; Loss: 3.0727e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9110e+03; Test Loss: 4.1656e+03. (Time: 8.3s)/nThe mean of testing losses in one call: 3069.6861444091796\n",
            "\n",
            "first loss: 3072.7234692382813 test loss new: 3070.5015380859377\n",
            "updating best model ..\n",
            "Step 3400 of 100000; Loss: 3.0705e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3483e+03; Test Loss: 4.1617e+03. (Time: 7.5s)/nThe mean of testing losses in one call: 3067.5373522949217\n",
            "\n",
            "first loss: 3070.5015380859377 test loss new: 3068.3311596679687\n",
            "updating best model ..\n",
            "Step 3600 of 100000; Loss: 3.0683e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9699e+03; Test Loss: 4.1580e+03. (Time: 6.9s)/nThe mean of testing losses in one call: 3065.444471435547\n",
            "\n",
            "first loss: 3068.3311596679687 test loss new: 3066.2201123046875\n",
            "updating best model ..\n",
            "Step 3800 of 100000; Loss: 3.0662e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.3366e+03; Test Loss: 4.1542e+03. (Time: 7.9s)/nThe mean of testing losses in one call: 3063.3973876953123\n",
            "\n",
            "first loss: 3066.2201123046875 test loss new: 3064.15298828125\n",
            "updating best model ..\n",
            "Step 4000 of 100000; Loss: 3.0642e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2614e+03; Test Loss: 4.1505e+03. (Time: 8.1s)/nThe mean of testing losses in one call: 3061.4059942626955\n",
            "\n",
            "first loss: 3064.15298828125 test loss new: 3062.143876953125\n",
            "updating best model ..\n",
            "Step 4200 of 100000; Loss: 3.0621e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5808e+03; Test Loss: 4.1469e+03. (Time: 7.8s)/nThe mean of testing losses in one call: 3059.4597692871093\n",
            "\n",
            "first loss: 3062.143876953125 test loss new: 3060.178708496094\n",
            "updating best model ..\n",
            "Step 4400 of 100000; Loss: 3.0602e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5888e+03; Test Loss: 4.1434e+03. (Time: 7.5s)/nThe mean of testing losses in one call: 3057.5798443603517\n",
            "\n",
            "first loss: 3060.178708496094 test loss new: 3058.2803564453125\n",
            "updating best model ..\n",
            "Step 4600 of 100000; Loss: 3.0583e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5804e+03; Test Loss: 4.1399e+03. (Time: 6.0s)/nThe mean of testing losses in one call: 3055.733508911133\n",
            "\n",
            "first loss: 3058.2803564453125 test loss new: 3056.4176733398435\n",
            "updating best model ..\n",
            "Step 4800 of 100000; Loss: 3.0564e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.8403e+03; Test Loss: 4.1365e+03. (Time: 9.1s)/nThe mean of testing losses in one call: 3053.9515338134765\n",
            "\n",
            "first loss: 3056.4176733398435 test loss new: 3054.61796875\n",
            "updating best model ..\n",
            "Step 5000 of 100000; Loss: 3.0546e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3011e+02; Test Loss: 4.1331e+03. (Time: 9.3s)/nThe mean of testing losses in one call: 3052.207545776367\n",
            "\n",
            "first loss: 3054.61796875 test loss new: 3052.8582275390627\n",
            "updating best model ..\n",
            "Step 5200 of 100000; Loss: 3.0529e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2783e+03; Test Loss: 4.1298e+03. (Time: 6.0s)/nThe mean of testing losses in one call: 3050.5108068847658\n",
            "\n",
            "first loss: 3052.8582275390627 test loss new: 3051.1454809570314\n",
            "updating best model ..\n",
            "Step 5400 of 100000; Loss: 3.0511e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9187e+03; Test Loss: 4.1265e+03. (Time: 7.6s)/nThe mean of testing losses in one call: 3048.845510864258\n",
            "\n",
            "first loss: 3051.1454809570314 test loss new: 3049.4663159179686\n",
            "updating best model ..\n",
            "Step 5600 of 100000; Loss: 3.0495e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5795e+03; Test Loss: 4.1232e+03. (Time: 8.4s)/nThe mean of testing losses in one call: 3047.2014343261717\n",
            "\n",
            "first loss: 3049.4663159179686 test loss new: 3047.8086254882815\n",
            "updating best model ..\n",
            "Step 5800 of 100000; Loss: 3.0478e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9008e+03; Test Loss: 4.1200e+03. (Time: 7.8s)/nThe mean of testing losses in one call: 3045.6010290527342\n",
            "\n",
            "first loss: 3047.8086254882815 test loss new: 3046.1947436523437\n",
            "updating best model ..\n",
            "Step 6000 of 100000; Loss: 3.0462e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3570e+03; Test Loss: 4.1167e+03. (Time: 6.0s)/nThe mean of testing losses in one call: 3044.031167602539\n",
            "\n",
            "first loss: 3046.1947436523437 test loss new: 3044.609931640625\n",
            "updating best model ..\n",
            "Step 6200 of 100000; Loss: 3.0446e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9561e+03; Test Loss: 4.1136e+03. (Time: 7.0s)/nThe mean of testing losses in one call: 3042.505852661133\n",
            "\n",
            "first loss: 3044.609931640625 test loss new: 3043.0721020507813\n",
            "updating best model ..\n",
            "Step 6400 of 100000; Loss: 3.0431e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.3032e+03; Test Loss: 4.1105e+03. (Time: 7.9s)/nThe mean of testing losses in one call: 3041.0053521728514\n",
            "\n",
            "first loss: 3043.0721020507813 test loss new: 3041.5581372070315\n",
            "updating best model ..\n",
            "Step 6600 of 100000; Loss: 3.0416e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2263e+03; Test Loss: 4.1074e+03. (Time: 7.5s)/nThe mean of testing losses in one call: 3039.5489953613283\n",
            "\n",
            "first loss: 3041.5581372070315 test loss new: 3040.0895703125\n",
            "updating best model ..\n",
            "Step 6800 of 100000; Loss: 3.0401e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5266e+03; Test Loss: 4.1043e+03. (Time: 7.4s)/nThe mean of testing losses in one call: 3038.118023681641\n",
            "\n",
            "first loss: 3040.0895703125 test loss new: 3038.645754394531\n",
            "updating best model ..\n",
            "Step 7000 of 100000; Loss: 3.0386e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.5061e+03; Test Loss: 4.1014e+03. (Time: 5.6s)/nThe mean of testing losses in one call: 3036.741463623047\n",
            "\n",
            "first loss: 3038.645754394531 test loss new: 3037.2559399414063\n",
            "updating best model ..\n",
            "Step 7200 of 100000; Loss: 3.0373e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.5250e+03; Test Loss: 4.0984e+03. (Time: 8.2s)/nThe mean of testing losses in one call: 3035.3798992919924\n",
            "\n",
            "first loss: 3037.2559399414063 test loss new: 3035.883937988281\n",
            "updating best model ..\n",
            "Step 7400 of 100000; Loss: 3.0359e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.7697e+03; Test Loss: 4.0956e+03. (Time: 7.6s)/nThe mean of testing losses in one call: 3034.072646484375\n",
            "\n",
            "first loss: 3035.883937988281 test loss new: 3034.5633447265627\n",
            "updating best model ..\n",
            "Step 7600 of 100000; Loss: 3.0346e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 9.3039e+02; Test Loss: 4.0927e+03. (Time: 7.6s)/nThe mean of testing losses in one call: 3032.783991699219\n",
            "\n",
            "first loss: 3034.5633447265627 test loss new: 3033.2649755859375\n",
            "updating best model ..\n",
            "Step 7800 of 100000; Loss: 3.0333e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2838e+03; Test Loss: 4.0900e+03. (Time: 6.3s)/nThe mean of testing losses in one call: 3031.534741821289\n",
            "\n",
            "first loss: 3033.2649755859375 test loss new: 3032.0037646484375\n",
            "updating best model ..\n",
            "Step 8000 of 100000; Loss: 3.0320e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9104e+03; Test Loss: 4.0872e+03. (Time: 8.1s)/nThe mean of testing losses in one call: 3030.299525756836\n",
            "\n",
            "first loss: 3032.0037646484375 test loss new: 3030.7601953125\n",
            "updating best model ..\n",
            "Step 8200 of 100000; Loss: 3.0308e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.5860e+03; Test Loss: 4.0844e+03. (Time: 7.2s)/nThe mean of testing losses in one call: 3029.077921142578\n",
            "\n",
            "first loss: 3030.7601953125 test loss new: 3029.5288403320315\n",
            "updating best model ..\n",
            "Step 8400 of 100000; Loss: 3.0295e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9027e+03; Test Loss: 4.0817e+03. (Time: 7.5s)/nThe mean of testing losses in one call: 3027.8892779541015\n",
            "\n",
            "first loss: 3029.5288403320315 test loss new: 3028.3312866210936\n",
            "updating best model ..\n",
            "Step 8600 of 100000; Loss: 3.0283e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3649e+03; Test Loss: 4.0789e+03. (Time: 6.8s)/nThe mean of testing losses in one call: 3026.7153112792967\n",
            "\n",
            "first loss: 3028.3312866210936 test loss new: 3027.1470166015624\n",
            "updating best model ..\n",
            "Step 8800 of 100000; Loss: 3.0271e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.9549e+03; Test Loss: 4.0763e+03. (Time: 9.1s)/nThe mean of testing losses in one call: 3025.5786145019533\n",
            "\n",
            "first loss: 3027.1470166015624 test loss new: 3026.001650390625\n",
            "updating best model ..\n",
            "Step 9000 of 100000; Loss: 3.0260e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.2726e+03; Test Loss: 4.0736e+03. (Time: 9.6s)/nThe mean of testing losses in one call: 3024.4525036621094\n",
            "\n",
            "first loss: 3026.001650390625 test loss new: 3024.8663818359373\n",
            "updating best model ..\n",
            "Step 9200 of 100000; Loss: 3.0249e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2056e+03; Test Loss: 4.0710e+03. (Time: 7.6s)/nThe mean of testing losses in one call: 3023.3627239990233\n",
            "\n",
            "first loss: 3024.8663818359373 test loss new: 3023.76802734375\n",
            "updating best model ..\n",
            "Step 9400 of 100000; Loss: 3.0238e+03. (Time: 0.0s)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 这个相当于是最好的认知模型，在rescolar里面加了一些参数\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m rl_pc_params, _ \u001b[38;5;241m=\u001b[39m \u001b[43mrnn_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_fun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbandits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHk_PreserveConAgentQ\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataset_validate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategorical\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_decayed_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# L2 正则化\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adam 优化器\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_steps_per_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_steps_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#return_all_losses=True,\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_early_stop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/CogModelingRNNsTutorial/CogModelingRNNsTutorial/rnn_utils.py:550\u001b[0m, in \u001b[0;36mfit_model\u001b[0;34m(model_fun, dataset_train, dataset_test, optimizer, loss_fun, random_key, n_steps_per_call, n_steps_max, return_all_losses, early_stop_step, if_early_stop)\u001b[0m\n\u001b[1;32m    547\u001b[0m   early_stop_triggered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m continue_training:\n\u001b[0;32m--> 550\u001b[0m   params, opt_state, losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel_fun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdataset_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdataset_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m      \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m      \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m      \u001b[49m\u001b[43mloss_fun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdo_plot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m      \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps_per_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m   n_calls_to_train_model \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    562\u001b[0m   t_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
            "File \u001b[0;32m~/CogModelingRNNsTutorial/CogModelingRNNsTutorial/rnn_utils.py:276\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model_fun, dataset_train, dataset_test, optimizer, random_key, opt_state, params, n_steps, penalty_scale, loss_fun, do_plot, truncate_seq_length)\u001b[0m\n\u001b[1;32m    273\u001b[0m   xs_test \u001b[38;5;241m=\u001b[39m xs_test[:truncate_seq_length]\n\u001b[1;32m    274\u001b[0m   ys_test \u001b[38;5;241m=\u001b[39m ys_test[:truncate_seq_length]\n\u001b[0;32m--> 276\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mys_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m testing_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(test_loss))\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# Log every 10th step\u001b[39;00m\n",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "File \u001b[0;32m~/anaconda3/envs/CogModel/lib/python3.10/site-packages/jax/_src/pjit.py:292\u001b[0m, in \u001b[0;36m_cpp_pjit.<locals>.cache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mno_tracing\u001b[38;5;241m.\u001b[39mvalue:\n\u001b[1;32m    288\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mre-tracing function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjit_info\u001b[38;5;241m.\u001b[39mfun_sourceinfo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    289\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`jit`, but \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_tracing\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    291\u001b[0m (outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked, box_data,\n\u001b[0;32m--> 292\u001b[0m  executable, pgle_profiler) \u001b[38;5;241m=\u001b[39m \u001b[43m_python_pjit_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m maybe_fastpath_data \u001b[38;5;241m=\u001b[39m _get_fastpath_data(\n\u001b[1;32m    295\u001b[0m     executable, out_tree, args_flat, out_flat, attrs_tracked, box_data,\n\u001b[1;32m    296\u001b[0m     jaxpr\u001b[38;5;241m.\u001b[39meffects, jaxpr\u001b[38;5;241m.\u001b[39mconsts, jit_info\u001b[38;5;241m.\u001b[39mabstracted_axes, pgle_profiler)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outs, maybe_fastpath_data, _need_to_rebuild_with_fdo(pgle_profiler)\n",
            "File \u001b[0;32m~/anaconda3/envs/CogModel/lib/python3.10/site-packages/jax/_src/pjit.py:153\u001b[0m, in \u001b[0;36m_python_pjit_helper\u001b[0;34m(fun, jit_info, *args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m   args_flat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(core\u001b[38;5;241m.\u001b[39mfull_lower, args_flat)\n\u001b[1;32m    152\u001b[0m   core\u001b[38;5;241m.\u001b[39mcheck_eval_args(args_flat)\n\u001b[0;32m--> 153\u001b[0m   out_flat, compiled, profiler \u001b[38;5;241m=\u001b[39m \u001b[43m_pjit_call_impl_python\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   out_flat \u001b[38;5;241m=\u001b[39m pjit_p\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs_flat, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mp\u001b[38;5;241m.\u001b[39mparams)\n",
            "File \u001b[0;32m~/anaconda3/envs/CogModel/lib/python3.10/site-packages/jax/_src/pjit.py:1855\u001b[0m, in \u001b[0;36m_pjit_call_impl_python\u001b[0;34m(jaxpr, in_shardings, out_shardings, in_layouts, out_layouts, donated_invars, ctx_mesh, name, keep_unused, inline, compiler_options_kvs, *args)\u001b[0m\n\u001b[1;32m   1843\u001b[0m compiler_options_kvs \u001b[38;5;241m=\u001b[39m compiler_options_kvs \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mtuple\u001b[39m(pgle_compile_options\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m   1844\u001b[0m \u001b[38;5;66;03m# Passing mutable PGLE profile here since it should be extracted by JAXPR to\u001b[39;00m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;66;03m# initialize the fdo_profile compile option.\u001b[39;00m\n\u001b[1;32m   1846\u001b[0m compiled \u001b[38;5;241m=\u001b[39m \u001b[43m_resolve_and_lower\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1847\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjaxpr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjaxpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_shardings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_shardings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_shardings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_layouts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_layouts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1849\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_layouts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_layouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdonated_invars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdonated_invars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctx_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx_mesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1851\u001b[0m \u001b[43m    \u001b[49m\u001b[43minline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowering_platforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlowering_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoweringParameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 1855\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;66;03m# This check is expensive so only do it if enable_checks is on.\u001b[39;00m\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compiled\u001b[38;5;241m.\u001b[39m_auto_spmd_lowering \u001b[38;5;129;01mand\u001b[39;00m config\u001b[38;5;241m.\u001b[39menable_checks\u001b[38;5;241m.\u001b[39mvalue:\n",
            "File \u001b[0;32m~/anaconda3/envs/CogModel/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:2410\u001b[0m, in \u001b[0;36mMeshComputation.compile\u001b[0;34m(self, compiler_options)\u001b[0m\n\u001b[1;32m   2408\u001b[0m compiler_options_kvs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiler_options_kvs \u001b[38;5;241m+\u001b[39m t_compiler_options\n\u001b[1;32m   2409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m compiler_options_kvs:\n\u001b[0;32m-> 2410\u001b[0m   executable \u001b[38;5;241m=\u001b[39m \u001b[43mUnloadedMeshExecutable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_hlo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2411\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2412\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2413\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m compiler_options_kvs:\n\u001b[1;32m   2414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_executable \u001b[38;5;241m=\u001b[39m executable\n",
            "File \u001b[0;32m~/anaconda3/envs/CogModel/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:2952\u001b[0m, in \u001b[0;36mUnloadedMeshExecutable.from_hlo\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   2949\u001b[0m       \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   2951\u001b[0m util\u001b[38;5;241m.\u001b[39mtest_event(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpxla_cached_compilation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2952\u001b[0m xla_executable \u001b[38;5;241m=\u001b[39m \u001b[43m_cached_compilation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2953\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspmd_lowering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtuple_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_spmd_lowering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_prop_to_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_prop_to_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpmap_nreps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiler_options_kvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auto_spmd_lowering:\n\u001b[1;32m   2959\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m mesh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/CogModel/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py:2743\u001b[0m, in \u001b[0;36m_cached_compilation\u001b[0;34m(computation, name, mesh, spmd_lowering, tuple_args, auto_spmd_lowering, allow_prop_to_inputs, allow_prop_to_outputs, host_callbacks, backend, da, pmap_nreps, compiler_options_kvs, pgle_profiler)\u001b[0m\n\u001b[1;32m   2735\u001b[0m compile_options \u001b[38;5;241m=\u001b[39m create_compile_options(\n\u001b[1;32m   2736\u001b[0m     computation, mesh, spmd_lowering, tuple_args, auto_spmd_lowering,\n\u001b[1;32m   2737\u001b[0m     allow_prop_to_inputs, allow_prop_to_outputs, backend,\n\u001b[1;32m   2738\u001b[0m     dev, pmap_nreps, compiler_options)\n\u001b[1;32m   2740\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dispatch\u001b[38;5;241m.\u001b[39mlog_elapsed_time(\n\u001b[1;32m   2741\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished XLA compilation of \u001b[39m\u001b[38;5;132;01m{fun_name}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{elapsed_time:.9f}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2742\u001b[0m     fun_name\u001b[38;5;241m=\u001b[39mname, event\u001b[38;5;241m=\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mBACKEND_COMPILE_EVENT):\n\u001b[0;32m-> 2743\u001b[0m   xla_executable \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_or_get_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2744\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2745\u001b[0m \u001b[43m      \u001b[49m\u001b[43mda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpgle_profiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2746\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xla_executable\n",
            "File \u001b[0;32m~/anaconda3/envs/CogModel/lib/python3.10/site-packages/jax/_src/compiler.py:500\u001b[0m, in \u001b[0;36mcompile_or_get_cached\u001b[0;34m(backend, computation, devices, compile_options, host_callbacks, executable_devices, pgle_profiler)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    499\u001b[0m   log_persistent_cache_miss(module_name, cache_key)\n\u001b[0;32m--> 500\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_and_write_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m      \u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mhost_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/CogModel/lib/python3.10/site-packages/jax/_src/compiler.py:768\u001b[0m, in \u001b[0;36m_compile_and_write_cache\u001b[0;34m(backend, computation, executable_devices, compile_options, host_callbacks, module_name, cache_key)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_compile_and_write_cache\u001b[39m(\n\u001b[1;32m    759\u001b[0m     backend: xc\u001b[38;5;241m.\u001b[39mClient,\n\u001b[1;32m    760\u001b[0m     computation: ir\u001b[38;5;241m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    765\u001b[0m     cache_key: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    766\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m xc\u001b[38;5;241m.\u001b[39mLoadedExecutable:\n\u001b[1;32m    767\u001b[0m   start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 768\u001b[0m   executable \u001b[38;5;241m=\u001b[39m \u001b[43mbackend_compile_and_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomputation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost_callbacks\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    771\u001b[0m   compile_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    772\u001b[0m   _cache_write(\n\u001b[1;32m    773\u001b[0m       cache_key, compile_time, module_name, backend, executable, host_callbacks\n\u001b[1;32m    774\u001b[0m   )\n",
            "File \u001b[0;32m~/anaconda3/envs/CogModel/lib/python3.10/site-packages/jax/_src/profiler.py:354\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    353\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m TraceAnnotation(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/CogModel/lib/python3.10/site-packages/jax/_src/compiler.py:385\u001b[0m, in \u001b[0;36mbackend_compile_and_load\u001b[0;34m(backend, module, executable_devices, options, host_callbacks)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile_and_load(\n\u001b[1;32m    377\u001b[0m           built_c,\n\u001b[1;32m    378\u001b[0m           executable_devices\u001b[38;5;241m=\u001b[39mexecutable_devices,\n\u001b[1;32m    379\u001b[0m           compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    380\u001b[0m           host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks,\n\u001b[1;32m    381\u001b[0m       )\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecutable_devices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutable_devices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m xc\u001b[38;5;241m.\u001b[39mXlaRuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    391\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m error_handler \u001b[38;5;129;01min\u001b[39;00m _XLA_RUNTIME_ERROR_HANDLERS:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 这个相当于是最好的认知模型，在rescolar里面加了一些参数\n",
        "rl_pc_params, _ = rnn_utils.fit_model(\n",
        "    model_fun=bandits.Hk_PreserveConAgentQ,\n",
        "    dataset_train = dataset_train,\n",
        "    dataset_test = dataset_validate,\n",
        "    loss_fun='categorical',\n",
        "    optimizer= optax.chain(\n",
        "        optax.add_decayed_weights(1e-5),  # L2 正则化\n",
        "        optax.adam(learning_rate=1e-4)  # Adam 优化器\n",
        "    ),\n",
        "    n_steps_per_call=200,\n",
        "    n_steps_max=100000,\n",
        "    #return_all_losses=True,\n",
        "    early_stop_step=200,\n",
        "    if_early_stop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJIf1uLXwYLM",
        "outputId": "613e2379-28cc-4a12-8402-5b82183e9405"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Normalized Likelihood: 48.5%\n"
          ]
        }
      ],
      "source": [
        "mean,std = compute_log_likelihood(dataset_test, bandits.Hk_PreserveConAgentQ, rl_pc_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKUz7-bxNvub"
      },
      "outputs": [],
      "source": [
        "#@title Set up the RNN (Vanilla RNN) Model\n",
        "n_hidden = 8\n",
        "def make_vanilla_rnn():\n",
        "    model = hk.DeepRNN(\n",
        "        [hk.VanillaRNN(n_hidden), hk.Linear(output_size=2)]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "#@title Set up the RNN (Vanilla RNN) Model\n",
        "n_hidden = 8\n",
        "def make_lstm():\n",
        "    model = hk.DeepRNN(\n",
        "        [hk.LSTM(n_hidden), hk.Linear(output_size=2)]\n",
        "    )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "khylH5F1OKHK",
        "outputId": "4eaafe01-db93-4ae3-bdc1-06e810217d6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 200 of 200; Loss: 2.6275e+03; Test Loss: 2.5221e+03. (Time: 4.7s)updating best model ..\n",
            "Step 200 of 100000; Loss: 3.0262e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1956e+03; Test Loss: 2.5076e+03. (Time: 3.9s)updating best model ..\n",
            "Step 400 of 100000; Loss: 3.0114e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.0060e+03; Test Loss: 2.4942e+03. (Time: 4.3s)updating best model ..\n",
            "Step 600 of 100000; Loss: 2.9995e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.0375e+03; Test Loss: 2.4818e+03. (Time: 4.8s)updating best model ..\n",
            "Step 800 of 100000; Loss: 2.9901e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2896e+03; Test Loss: 2.4699e+03. (Time: 3.8s)updating best model ..\n",
            "Step 1000 of 100000; Loss: 2.9821e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.0236e+03; Test Loss: 2.4578e+03. (Time: 3.9s)updating best model ..\n",
            "Step 1200 of 100000; Loss: 2.9748e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.1469e+03; Test Loss: 2.4449e+03. (Time: 4.8s)updating best model ..\n",
            "Step 1400 of 100000; Loss: 2.9673e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.9505e+03; Test Loss: 2.4300e+03. (Time: 3.8s)updating best model ..\n",
            "Step 1600 of 100000; Loss: 2.9588e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6482e+03; Test Loss: 2.4133e+03. (Time: 3.8s)updating best model ..\n",
            "Step 1800 of 100000; Loss: 2.9494e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6579e+03; Test Loss: 2.3955e+03. (Time: 4.6s)updating best model ..\n",
            "Step 2000 of 100000; Loss: 2.9394e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6355e+03; Test Loss: 2.3780e+03. (Time: 3.8s)updating best model ..\n",
            "Step 2200 of 100000; Loss: 2.9297e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9643e+03; Test Loss: 2.3620e+03. (Time: 3.6s)updating best model ..\n",
            "Step 2400 of 100000; Loss: 2.9206e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6343e+03; Test Loss: 2.3475e+03. (Time: 4.5s)updating best model ..\n",
            "Step 2600 of 100000; Loss: 2.9125e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2995e+03; Test Loss: 2.3353e+03. (Time: 3.6s)updating best model ..\n",
            "Step 2800 of 100000; Loss: 2.9054e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.1132e+03; Test Loss: 2.3244e+03. (Time: 3.6s)updating best model ..\n",
            "Step 3000 of 100000; Loss: 2.8993e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.0844e+03; Test Loss: 2.3150e+03. (Time: 4.1s)updating best model ..\n",
            "Step 3200 of 100000; Loss: 2.8938e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.6416e+03; Test Loss: 2.3065e+03. (Time: 3.6s)updating best model ..\n",
            "Step 3400 of 100000; Loss: 2.8890e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3279e+03; Test Loss: 2.2987e+03. (Time: 3.9s)updating best model ..\n",
            "Step 3600 of 100000; Loss: 2.8845e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.6887e+03; Test Loss: 2.2914e+03. (Time: 4.0s)updating best model ..\n",
            "Step 3800 of 100000; Loss: 2.8803e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.1835e+03; Test Loss: 2.2847e+03. (Time: 3.6s)updating best model ..\n",
            "Step 4000 of 100000; Loss: 2.8765e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.6189e+03; Test Loss: 2.2784e+03. (Time: 3.6s)updating best model ..\n",
            "Step 4200 of 100000; Loss: 2.8728e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6547e+03; Test Loss: 2.2721e+03. (Time: 4.0s)updating best model ..\n",
            "Step 4400 of 100000; Loss: 2.8693e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.4394e+03; Test Loss: 2.2666e+03. (Time: 3.6s)updating best model ..\n",
            "Step 4600 of 100000; Loss: 2.8660e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6320e+03; Test Loss: 2.2607e+03. (Time: 3.6s)updating best model ..\n",
            "Step 4800 of 100000; Loss: 2.8629e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.8360e+03; Test Loss: 2.2559e+03. (Time: 3.8s)updating best model ..\n",
            "Step 5000 of 100000; Loss: 2.8599e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6120e+03; Test Loss: 2.2506e+03. (Time: 4.0s)updating best model ..\n",
            "Step 5200 of 100000; Loss: 2.8572e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.2125e+03; Test Loss: 2.2464e+03. (Time: 3.7s)updating best model ..\n",
            "Step 5400 of 100000; Loss: 2.8546e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0758e+03; Test Loss: 2.2418e+03. (Time: 3.7s)updating best model ..\n",
            "Step 5600 of 100000; Loss: 2.8522e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.1448e+03; Test Loss: 2.2379e+03. (Time: 4.5s)updating best model ..\n",
            "Step 5800 of 100000; Loss: 2.8498e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4851e+03; Test Loss: 2.2340e+03. (Time: 3.6s)updating best model ..\n",
            "Step 6000 of 100000; Loss: 2.8477e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3665e+03; Test Loss: 2.2303e+03. (Time: 3.6s)updating best model ..\n",
            "Step 6200 of 100000; Loss: 2.8457e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5532e+03; Test Loss: 2.2268e+03. (Time: 4.5s)updating best model ..\n",
            "Step 6400 of 100000; Loss: 2.8438e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2168e+03; Test Loss: 2.2234e+03. (Time: 3.6s)updating best model ..\n",
            "Step 6600 of 100000; Loss: 2.8421e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.5063e+03; Test Loss: 2.2204e+03. (Time: 3.7s)updating best model ..\n",
            "Step 6800 of 100000; Loss: 2.8404e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6546e+03; Test Loss: 2.2172e+03. (Time: 4.2s)updating best model ..\n",
            "Step 7000 of 100000; Loss: 2.8389e+03. (Time: 0.0s)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _xla_gc_callback at 0x7d103f43cfe0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "    \n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 200 of 200; Loss: 3.3487e+03; Test Loss: 2.2146e+03. (Time: 4.0s)updating best model ..\n",
            "Step 7200 of 100000; Loss: 2.8375e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6303e+03; Test Loss: 2.2115e+03. (Time: 3.7s)updating best model ..\n",
            "Step 7400 of 100000; Loss: 2.8362e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.7915e+03; Test Loss: 2.2091e+03. (Time: 4.1s)updating best model ..\n",
            "Step 7600 of 100000; Loss: 2.8350e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6021e+03; Test Loss: 2.2060e+03. (Time: 3.7s)updating best model ..\n",
            "Step 7800 of 100000; Loss: 2.8340e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.1777e+03; Test Loss: 2.2033e+03. (Time: 3.7s)updating best model ..\n",
            "Step 8000 of 100000; Loss: 2.8327e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0539e+03; Test Loss: 2.1991e+03. (Time: 3.9s)updating best model ..\n",
            "Step 8200 of 100000; Loss: 2.8297e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.1424e+03; Test Loss: 2.1942e+03. (Time: 3.6s)updating best model ..\n",
            "Step 8400 of 100000; Loss: 2.8250e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4035e+03; Test Loss: 2.1899e+03. (Time: 3.6s)updating best model ..\n",
            "Step 8600 of 100000; Loss: 2.8194e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.3696e+03; Test Loss: 2.1861e+03. (Time: 3.9s)updating best model ..\n",
            "Step 8800 of 100000; Loss: 2.8164e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4715e+03; Test Loss: 2.1830e+03. (Time: 3.8s)updating best model ..\n",
            "Step 9000 of 100000; Loss: 2.8092e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2051e+03; Test Loss: 2.1794e+03. (Time: 3.7s)updating best model ..\n",
            "Step 9200 of 100000; Loss: 2.8057e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 4.4275e+03; Test Loss: 2.1765e+03. (Time: 3.7s)updating best model ..\n",
            "Step 9400 of 100000; Loss: 2.8032e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.6944e+03; Test Loss: 2.1738e+03. (Time: 4.3s)updating best model ..\n",
            "Step 9600 of 100000; Loss: 2.8014e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 3.2116e+03; Test Loss: 2.1717e+03. (Time: 3.6s)updating best model ..\n",
            "Step 9800 of 100000; Loss: 2.7998e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6479e+03; Test Loss: 2.1694e+03. (Time: 3.7s)updating best model ..\n",
            "Step 10000 of 100000; Loss: 2.7984e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.6075e+03; Test Loss: 2.1676e+03. (Time: 4.5s)updating best model ..\n",
            "Step 10200 of 100000; Loss: 2.7970e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 2.6094e+03; Test Loss: 2.1656e+03. (Time: 3.7s)updating best model ..\n",
            "Step 10400 of 100000; Loss: 2.7958e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.9776e+03; Test Loss: 2.1641e+03. (Time: 3.7s)updating best model ..\n",
            "Step 10600 of 100000; Loss: 2.7945e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 1.0370e+03; Test Loss: 2.1623e+03. (Time: 4.5s)updating best model ..\n",
            "Step 10800 of 100000; Loss: 2.7933e+03. (Time: 0.0s)\n",
            "Step 200 of 200; Loss: 5.2220e+03; Test Loss: 2.1603e+03. (Time: 4.1s)updating best model ..\n",
            "Step 11000 of 100000; Loss: 2.7919e+03. (Time: 0.0s)\n",
            "Step 30 of 200; Loss: 4.4276e+03; Test Loss: 2.1604e+03. (Time: 1.6s)"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3664668.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#n_steps_max = 1000000 #@param\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m gru_params, _, all_losses = rnn_utils.fit_model(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_fun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_lstm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CogModelingRNNsTutorial/rnn_utils.py\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model_fun, dataset_train, dataset_test, optimizer, loss_fun, random_key, n_steps_per_call, n_steps_max, return_all_losses, early_stop_step, if_early_stop)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m     params, opt_state, losses = train_model(\n\u001b[0m\u001b[1;32m    549\u001b[0m         \u001b[0mmodel_fun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CogModelingRNNsTutorial/rnn_utils.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_fun, dataset_train, dataset_test, optimizer, random_key, opt_state, params, n_steps, penalty_scale, loss_fun, do_plot, truncate_seq_length)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;31m# Log every 10th step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m       print((f'\\rStep {step + 1} of {n_steps}; '\n\u001b[1;32m    282\u001b[0m              \u001b[0;34mf'Loss: {loss:.4e}; Test Loss: {test_loss:.4e}. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/numpy/array_methods.py\u001b[0m in \u001b[0;36mdeferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mswap\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_accepted_binop_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m     \u001b[0;31m# Note: don't use isinstance here, because we don't want to raise for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;31m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title Fit the RNN (GRU) model\n",
        "#n_steps_max = 1000000 #@param\n",
        "\n",
        "gru_params, _, all_losses = rnn_utils.fit_model(\n",
        "    model_fun=make_lstm,\n",
        "    dataset_train = dataset_train,\n",
        "    dataset_test = dataset_validate,\n",
        "    loss_fun='categorical',\n",
        "    optimizer= optax.chain(\n",
        "        optax.add_decayed_weights(1e-5),  # L2 正则化\n",
        "        optax.adam(learning_rate=1e-4)  # Adam 优化器\n",
        "    ),\n",
        "    n_steps_per_call=200,\n",
        "    n_steps_max=100000,\n",
        "    return_all_losses=True,\n",
        "    early_stop_step=200,\n",
        "    if_early_stop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOpDYABeOaEM"
      },
      "outputs": [],
      "source": [
        "# #@title Compute log-likelihood\n",
        "# def compute_log_likelihood(dataset, model_fun, params):\n",
        "\n",
        "#   xs, actual_choices = next(dataset)\n",
        "#   n_trials_per_session, n_sessions = actual_choices.shape[:2]\n",
        "#   model_outputs, model_states = rnn_utils.eval_model(model_fun, params, xs)\n",
        "\n",
        "#   predicted_log_choice_probabilities = np.array(jax.nn.log_softmax(model_outputs[:, :, :2]))\n",
        "\n",
        "#   log_likelihood = 0\n",
        "#   n = 0  # Total number of trials across sessions.\n",
        "#   for sess_i in range(n_sessions):\n",
        "#     for trial_i in range(n_trials_per_session):\n",
        "#       actual_choice = int(actual_choices[trial_i, sess_i])\n",
        "#       if actual_choice >= 0:  # values < 0 are invalid trials which we ignore.\n",
        "#         log_likelihood += predicted_log_choice_probabilities[trial_i, sess_i, actual_choice]\n",
        "#         n += 1\n",
        "\n",
        "#   normalized_likelihood = np.exp(log_likelihood / n)\n",
        "\n",
        "#   print(f'Normalized Likelihood: {100 * normalized_likelihood:.1f}%')\n",
        "\n",
        "#   return normalized_likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ute8Yj_xOMd8",
        "outputId": "ca85be09-e42c-41a5-dd3e-13c9a6bc0d28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalized Likelihoods for GRU\n",
            "Training Dataset\n",
            "Average Normalized Likelihood: 61.3%\n",
            "Held-Out Dataset\n",
            "Average Normalized Likelihood: 62.0%\n"
          ]
        }
      ],
      "source": [
        "#@title Compute quality-of-fit: Held-out Normalized Likelihood\n",
        "# Compute log-likelihood\n",
        "print('Normalized Likelihoods for GRU')\n",
        "print('Training Dataset')\n",
        "training_likelihood = compute_log_likelihood(dataset_train, make_lstm, gru_params)\n",
        "print('Held-Out Dataset')\n",
        "testing_likelihood = compute_log_likelihood(dataset_validate, make_lstm, gru_params)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "i7x3Ff2xWTdb",
        "YOGiNjm4FUAx",
        "WmgO7HywNl5-",
        "GvGdnab8CdCN",
        "mrmkwDvr500Q",
        "9WygdkV_tSUA",
        "EYkrrFeXOqG2"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (CogModel)",
      "language": "python",
      "name": "cogmodel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
