{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b713c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define hybRNNs.\"\"\"\n",
    "from typing import Optional\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "RNNState = jnp.array\n",
    "\n",
    "\n",
    "class BiConRNN(hk.RNNCore):\n",
    "  \"\"\"A hybrid RNN: \"habit\" processes action choices; \"value\" processes rewards.\"\"\"\n",
    "\n",
    "  def __init__(self, rl_params, network_params, init_value=0.5):\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "    self._hs = rl_params['s']\n",
    "    self._vs = rl_params['s']\n",
    "    self._ho = rl_params['o']\n",
    "    self._vo = rl_params['o']\n",
    "\n",
    "    self.w_h = rl_params['w_h']\n",
    "    self.w_v = rl_params['w_v']\n",
    "    self.init_value = init_value\n",
    "\n",
    "    self._n_actions = network_params['n_actions']\n",
    "    self._hidden_size = network_params['hidden_size']\n",
    "\n",
    "    if rl_params['fit_forget']:\n",
    "      init = hk.initializers.RandomNormal(stddev=1, mean=0)\n",
    "      self.forget = jax.nn.sigmoid(  # 0 < forget < 1\n",
    "          hk.get_parameter('unsigmoid_forget', (1,), init=init)\n",
    "      )\n",
    "    else:\n",
    "      self.forget = rl_params['forget']\n",
    "\n",
    "  def _value_rnn(self, state, value, action, reward):\n",
    "\n",
    "    pre_act_val = jnp.sum(value * action, axis=1)  # (batch_s, 1)\n",
    "\n",
    "    inputs = jnp.concatenate(\n",
    "        [pre_act_val[:, jnp.newaxis], reward[:, jnp.newaxis]], axis=-1)\n",
    "    if self._vo:  # \"o\" = output -> feed previous output back in\n",
    "      inputs = jnp.concatenate([inputs, value], axis=-1)\n",
    "    if self._vs:  # \"s\" = state -> feed previous hidden state back in\n",
    "      inputs = jnp.concatenate([inputs, state], axis=-1)\n",
    "\n",
    "    next_state = jax.nn.tanh(hk.Linear(self._hidden_size)(inputs))\n",
    "\n",
    "    update = hk.Linear(1)(next_state)\n",
    "    value = (1 - self.forget) * value + self.forget * self.init_value\n",
    "    next_value = value + action * update\n",
    "\n",
    "    return next_value, next_state\n",
    "\n",
    "  def _value_con_rnn(self, state, value, action, reward):\n",
    "\n",
    "    pre_act_val = jnp.sum(value * (np.ones(1,2) - action), axis=1)  # (batch_s, 1)\n",
    "\n",
    "    inputs = jnp.concatenate(\n",
    "        [pre_act_val[:, jnp.newaxis], reward[:, jnp.newaxis]], axis=-1)\n",
    "    if self._vo:  # \"o\" = output -> feed previous output back in\n",
    "      inputs = jnp.concatenate([inputs, value], axis=-1)\n",
    "    if self._vs:  # \"s\" = state -> feed previous hidden state back in\n",
    "      inputs = jnp.concatenate([inputs, state], axis=-1)\n",
    "\n",
    "    next_state = jax.nn.tanh(hk.Linear(self._hidden_size)(inputs))\n",
    "\n",
    "    update = hk.Linear(1)(next_state)\n",
    "    value = (1 - self.forget) * value + self.forget * self.init_value\n",
    "    next_value = value + (np.ones(1,2) - action) * update\n",
    "\n",
    "    return next_con_value, next_state\n",
    "\n",
    "  def _habit_rnn(self, state, habit, action):\n",
    "\n",
    "    inputs = action\n",
    "    if self._ho:  # \"o\" = output -> feed previous output back in\n",
    "      inputs = jnp.concatenate([inputs, habit], axis=-1)\n",
    "    if self._hs:  # \"s\" = state -> feed previous hidden state back in\n",
    "      inputs = jnp.concatenate([inputs, state], axis=-1)\n",
    "\n",
    "    next_state = jax.nn.tanh(hk.Linear(self._hidden_size)(inputs))\n",
    "    next_habit = hk.Linear(self._n_actions)(next_state)\n",
    "\n",
    "    return next_habit, next_state\n",
    "\n",
    "  def __call__(self, inputs: jnp.ndarray, prev_state: jnp.ndarray):\n",
    "    h_state, v_state, c_state, habit, value, c_value = prev_state\n",
    "    action = inputs[:, 0]  # shape: (batch_size, )\n",
    "    reward = inputs[:, -1]  # shape: (batch_size,)\n",
    "    action_onehot = jax.nn.one_hot(action,2)\n",
    "    \n",
    "    # Value module: update/create new values\n",
    "    next_value, next_v_state = self._value_rnn(v_state, value, action_onehot, reward)\n",
    "    \n",
    "    next_c_value, next_c_state = self._value_con_rnn(c_state, value, action_onehot, reward)\n",
    "\n",
    "    # Habit module: update/create new habit\n",
    "    next_habit, next_h_state = self._habit_rnn(h_state, habit, action_onehot)\n",
    "\n",
    "    # Combine value and habit\n",
    "    logits = self.w_v * next_value + self.w_h * next_habit + self.w_c * next_c_value  # (bs, n_a)\n",
    "\n",
    "    return logits, (next_h_state, next_v_state, next_c_state, next_habit, next_value, next_c_value)\n",
    "\n",
    "  def initial_state(self, batch_size: Optional[int]):\n",
    "\n",
    "    return (\n",
    "        0 * jnp.ones([batch_size, self._hidden_size]),  # h_state\n",
    "        0 * jnp.ones([batch_size, self._hidden_size]),  # v_state\n",
    "        0 * jnp.ones([batch_size, self._hidden_size]),  # c_state\n",
    "        0 * jnp.ones([batch_size, self._n_actions]),  # habit\n",
    "        self.init_value * jnp.ones([batch_size, self._n_actions]),  # value\n",
    "        self.init_value * jnp.ones([batch_size, self._n_actions]),  # c_value\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
